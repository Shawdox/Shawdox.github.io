{"meta":{"title":"Shaw","subtitle":"@hust","description":"积沙成塔","author":"Shaw","url":"http://example.com","root":"/"},"pages":[{"title":"About","date":"2023-02-20T05:05:09.968Z","updated":"2023-02-20T05:05:09.968Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"邮箱：asdiop123321@qq.com； @华中科技大学 网络空间安全学院 Email: asdiop123321@qq.com ； @ School of Cyberspace Security, Huazhong University of science and technology"},{"title":"All archives","date":"2021-09-03T03:39:45.000Z","updated":"2021-09-03T03:40:04.450Z","comments":true,"path":"archives/index.html","permalink":"http://example.com/archives/index.html","excerpt":"","text":""},{"title":"All categories","date":"2021-09-03T03:22:06.577Z","updated":"2021-09-03T03:22:06.577Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"友链","date":"2021-06-14T09:40:37.000Z","updated":"2021-06-14T09:55:02.256Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"All tags","date":"2021-06-14T09:39:05.000Z","updated":"2021-09-03T03:22:24.928Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"contact","date":"2021-06-14T09:40:01.000Z","updated":"2021-06-14T09:40:14.806Z","comments":true,"path":"contact/index.html","permalink":"http://example.com/contact/index.html","excerpt":"","text":""},{"title":"","date":"2022-07-15T07:10:17.486Z","updated":"2022-07-15T07:10:17.486Z","comments":true,"path":"book/home.html","permalink":"http://example.com/book/home.html","excerpt":"","text":"More is different，宏观与微观的审视哲学，量变产生质变。 ​ ----Shaw的，一些记录，一些随手写。"},{"title":"","date":"2022-07-15T07:39:37.754Z","updated":"2022-07-15T07:39:37.754Z","comments":true,"path":"book/menu.html","permalink":"http://example.com/book/menu.html","excerpt":"","text":"Home Changelog h Categories Elements Excerpts Gallery Post Hello World Images Untitled Link Post Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam justo turpis, tincidunt ac convallis id. Untitled Tag Plugins Tags Videos 中文測試 日本語テスト"}],"posts":[{"title":"(技术总结)The Kernel Address Sanitizer(KASAN)","slug":"【技术总结】The Kernel Address Sanitizer(KASAN)","date":"2023-05-04T06:33:12.791Z","updated":"2023-05-04T06:36:52.861Z","comments":true,"path":"2023/05/04/【技术总结】The Kernel Address Sanitizer(KASAN)/","link":"","permalink":"http://example.com/2023/05/04/%E3%80%90%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%E3%80%91The%20Kernel%20Address%20Sanitizer(KASAN)/","excerpt":"The Kernel Address Sanitizer(KASAN) 1. 兼容性 ​ KASAN是一个动态内存安全错误检测器，旨在发现内核out-of-bounds和UAF错误。KASAN有三个模块：Generic KASAN、Software Tag-Based KASAN、Hardware Tag-Based KASAN。Generic KASAN兼容许多CPU架构，但性能开销很大；Software Tag-Based KASAN和Hardware Tag-Based KASAN都只兼容arm64架构的CPU，性能开销会更小。 ​ 一般的，称Generic KASAN和Software Tag-Based KASAN为software KASAN； ​ 称Software Tag-Based KASAN和Hardware Tag-Based KASAN为tag-based KASAN。 ​ Generic KASAN支持的指令集架构有：x86_64, arm, arm64, powerpc, riscv, s390, 和 xtensa；tag-based KASAN只支持arm64。","text":"The Kernel Address Sanitizer(KASAN) 1. 兼容性 ​ KASAN是一个动态内存安全错误检测器，旨在发现内核out-of-bounds和UAF错误。KASAN有三个模块：Generic KASAN、Software Tag-Based KASAN、Hardware Tag-Based KASAN。Generic KASAN兼容许多CPU架构，但性能开销很大；Software Tag-Based KASAN和Hardware Tag-Based KASAN都只兼容arm64架构的CPU，性能开销会更小。 ​ 一般的，称Generic KASAN和Software Tag-Based KASAN为software KASAN； ​ 称Software Tag-Based KASAN和Hardware Tag-Based KASAN为tag-based KASAN。 ​ Generic KASAN支持的指令集架构有：x86_64, arm, arm64, powerpc, riscv, s390, 和 xtensa；tag-based KASAN只支持arm64。 ### 2. 编译器要求 ​ KASAN对内存检查的逻辑是在编译的时候在内存访问操作之前插入相关检查指令，故需要相应的编译器支持该操作。 ​ Generic KASAN需要GCC 8.3.0+/任何版本的Clang(只要内核支持)； ​ Software Tag-Based KASAN需要GCC 11+/任何版本的Clang(只要内核支持)； ​ Hardware Tag-Based KASAN需要GCC10+/Clang12+。 3. 使用方法 ​ 开启KASAN需要在内核配置文件中开启对应配置： CONFIG_KASAN=y ​ 然后根据选择的KASAN类型选择标签：CONFIG_KASAN_GENERIC、CONFIG_KASAN_SW_TAGS、CONFIG_KASAN_HW_TAGS。 ​ 对于software KASAN，其需要指定其编译插装类型：CONFIG_KASAN_OUTLINE、CONFIG_KASAN_INLINE，outline类型生成体积更小的二进制代码，而inline类型速度比outline快两倍。 ​ 为了在报告中包括受影响的slab对象的stack traces，指定：CONFIG_STACKTRACE;包括受影响的物理页的stack traces，指定CONFIG_PAGE_OWNER并page_owner=on。 4. Report ​ 默认情况下，KASAN只对第一个无效的内存访问打印错误报告。使用 kasan_multi_shot，KASAN对每一个无效的访问都打印一份报告。这会禁用 了KASAN报告的 panic_on_warn。 ​ 典型的KASAN报告如下所示: ================================================================== BUG: KASAN: slab-out-of-bounds in kmalloc_oob_right+0xa8/0xbc [test_kasan] Write of size 1 at addr ffff8801f44ec37b by task insmod/2760 CPU: 1 PID: 2760 Comm: insmod Not tainted 4.19.0-rc3+ #698 Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-1 04/01/2014 Call Trace: dump_stack+0x94/0xd8 print_address_description+0x73/0x280 kasan_report+0x144/0x187 __asan_report_store1_noabort+0x17/0x20 kmalloc_oob_right+0xa8/0xbc [test_kasan] kmalloc_tests_init+0x16/0x700 [test_kasan] do_one_initcall+0xa5/0x3ae do_init_module+0x1b6/0x547 load_module+0x75df/0x8070 __do_sys_init_module+0x1c6/0x200 __x64_sys_init_module+0x6e/0xb0 do_syscall_64+0x9f/0x2c0 entry_SYSCALL_64_after_hwframe+0x44/0xa9 RIP: 0033:0x7f96443109da RSP: 002b:00007ffcf0b51b08 EFLAGS: 00000202 ORIG_RAX: 00000000000000af RAX: ffffffffffffffda RBX: 000055dc3ee521a0 RCX: 00007f96443109da RDX: 00007f96445cff88 RSI: 0000000000057a50 RDI: 00007f9644992000 RBP: 000055dc3ee510b0 R08: 0000000000000003 R09: 0000000000000000 R10: 00007f964430cd0a R11: 0000000000000202 R12: 00007f96445cff88 R13: 000055dc3ee51090 R14: 0000000000000000 R15: 0000000000000000 Allocated by task 2760: save_stack+0x43/0xd0 kasan_kmalloc+0xa7/0xd0 kmem_cache_alloc_trace+0xe1/0x1b0 kmalloc_oob_right+0x56/0xbc [test_kasan] kmalloc_tests_init+0x16/0x700 [test_kasan] do_one_initcall+0xa5/0x3ae do_init_module+0x1b6/0x547 load_module+0x75df/0x8070 __do_sys_init_module+0x1c6/0x200 __x64_sys_init_module+0x6e/0xb0 do_syscall_64+0x9f/0x2c0 entry_SYSCALL_64_after_hwframe+0x44/0xa9 Freed by task 815: save_stack+0x43/0xd0 __kasan_slab_free+0x135/0x190 kasan_slab_free+0xe/0x10 kfree+0x93/0x1a0 umh_complete+0x6a/0xa0 call_usermodehelper_exec_async+0x4c3/0x640 ret_from_fork+0x35/0x40 The buggy address belongs to the object at ffff8801f44ec300 which belongs to the cache kmalloc-128 of size 128 The buggy address is located 123 bytes inside of 128-byte region [ffff8801f44ec300, ffff8801f44ec380) The buggy address belongs to the page: page:ffffea0007d13b00 count:1 mapcount:0 mapping:ffff8801f7001640 index:0x0 flags: 0x200000000000100(slab) raw: 0200000000000100 ffffea0007d11dc0 0000001a0000001a ffff8801f7001640 raw: 0000000000000000 0000000080150015 00000001ffffffff 0000000000000000 page dumped because: kasan: bad access detected Memory state around the buggy address: ffff8801f44ec200: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb ffff8801f44ec280: fb fb fb fb fb fb fb fb fc fc fc fc fc fc fc fc >ffff8801f44ec300: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 03 ^ ffff8801f44ec380: fc fc fc fc fc fc fc fc fb fb fb fb fb fb fb fb ffff8801f44ec400: fb fb fb fb fb fb fb fb fc fc fc fc fc fc fc fc ================================================================== ​ BUG: KASAN: 第一行报告了错误类型，slab-out-of-bounds； ​ Call Trace:表明了当前error的堆栈跟踪； ​ Allocated by task 2760:表明了所访问内存分配位置的堆栈跟踪（“where the accessed memory was allocated ”）； ​ Freed by task 815:表明了对象被释放的位置的堆栈跟踪（“where the object was freed”）； ​ The buggy address belongs to ……:表明了访问的slab对象的相关描述； ​ Memory state around the buggy address:表明了访问slab对象周围的内存情况。 5. 影子内存 ​ KASAN的原理是利用“额外”的内存来标记那些可以被使用的内存的状态。这些做标记的区域被称为影子区域（shadow region）。了解 Linux 内存管理的读者知道，内存中的每个物理页在内存中都会有一个 struct page 这样的结构体来表示，即每 4KB 的页需要 40B 的结构体，大约 1% 的内存用来表示内存本身。Kasan 与其类似但“浪费”更为严重，影子区域的比例是 1:8，即总内存的九分之一会被“浪费”。用官方文档中的例子，如果有 128TB 的可用内存，需要有额外 16TB 的内存用来做标记。 Shadow Memory of KASAN ​ 做标记的方法比较简单，将可用内存按照 8 子节的大小分组，如果每组中所有 8 个字节都可以访问，则影子内存中相应的地方用全零（0x00）表示；如果可用内存的前 N（1 到 7 范围之间）个字节可用，则影子内存中响应的位置用 N 表示；其它情况影子内存用负数表示该内存不可用（KASAN使用不同的负值来区分不同类型的不可访问内存，如redzones 或已释放的内存，参见 mm/kasan/kasan.h）。 ​ 上述Report中，箭头指向的影子字节03，表示访问的地址是部分可访问的。报告中可访问的内存是“00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 03”，一共15个00和一个03，其表明了一共有15×8+3=123个字节可以访问，当试图访问这123个字节之外的内容时，就会报错out-of-bounds。 ​ KASAN错误标题（如 slab-out-of-bounds 或 use-after-free ） 是尽量接近的KASAN根据其拥有的有限信息打印出最可能的错误类型。错误的实际类型 可能会有所不同。 Reference 文档： The Kernel Address Sanitizer (KASAN) — The Linux Kernel documentation 内核地址消毒剂(KASAN) — The Linux Kernel documentation Kasan - Linux 内核的内存检测工具 - 腾讯云开发者社区-腾讯云 (tencent.com) 内存分配: Linux 内核 | 内存管理——slab 分配器 - 知乎 (zhihu.com) (60条消息) Linux内存子系统——分配物理页面（alloc_pages）_绍兴小贵宁的博客-CSDN博客","categories":[{"name":"Code","slug":"Code","permalink":"http://example.com/categories/Code/"}],"tags":[{"name":"Vulnerability","slug":"Vulnerability","permalink":"http://example.com/tags/Vulnerability/"},{"name":"AEG","slug":"AEG","permalink":"http://example.com/tags/AEG/"}],"author":"Shaw"},{"title":"(LLVM入门)编写LLVM PASS","slug":"(LLVM入门)编写LLVM PASS","date":"2023-05-01T06:40:01.508Z","updated":"2023-05-01T06:42:38.683Z","comments":true,"path":"2023/05/01/(LLVM入门)编写LLVM PASS/","link":"","permalink":"http://example.com/2023/05/01/(LLVM%E5%85%A5%E9%97%A8)%E7%BC%96%E5%86%99LLVM%20PASS/","excerpt":"(LLVM入门)编写LLVM PASS ​ 前置知识： LLVM IR结构； C++ 面向对象； CMake；","text":"(LLVM入门)编写LLVM PASS ​ 前置知识： LLVM IR结构； C++ 面向对象； CMake； 一、理论 LLVM IR LLVM IR 文件的基本单位称为 module，LLVM中的Module，代表了一块代码。它是一个比较完整独立的代码块，是一个最小的编译单元。； 一个 module 中可以拥有多个顶层实体，比如 function 和 global variavle； 一个 function define 中至少有一个 basicblock； 每个 basicblock 中有若干 instruction，并且都以 terminator instruction 结尾。 (详见：PowerPoint Presentation (llvm.org)) What is a PASS? pass 对LLVM IR的一些单元进行操作，例如module或者function； pass有两类，Analysis pass和Transformation pass。Transformation pass会对单元进行修改操作，Analysis pass不修改，只观察并生成一些高层信息； ​ Pass Manager：调度Pass在各IR层级按顺序运行。 （什么是各IR层级？） opt ​ opt是LLVM的代码优化器，其输入ll源文件，其可以对源文件进行优化（输出优化后的ll源文件）或者分析（输出分析结果）。 ​ 在使用opt命令时，-{passname}提供了以任何顺序运行任何LLVM优化或分析Pass的能力。 二、代码实践 常见llvm编译命令： #.c -> .ll： clang -emit-llvm -S a.c -o a.ll #.c -> .bc: clang -emit-llvm -c a.c -o a.bc #.ll -> .bc: llvm-as a.ll -o a.bc #.bc -> .ll: llvm-dis a.bc -o a.ll #.bc -> .s: lc a.bc -o a.s 2. Hellow World PASS 目标：打印出编译程序中存在的非外部函数的名称，该 Pass 只是检查程序，不修改原程序。 ​ llvm安装编译过程略，详见github官方项目。 2.1 old manager ​ 旧版pass manager采用继承对应PASS类，覆写父类虚函数的方式定义用户自己的pass类，编写old_hello.cpp如下： //old_Hello.cpp #include \"llvm/Pass.h\" #include \"llvm/IR/Function.h\" #include \"llvm/Support/raw_ostream.h\" using namespace llvm; //使用匿名命名空间，使得其中定义的函数对其它文件不可见； //防止命名空间污染 namespace&#123; //FunctionPass每次操作一个函数,Hellp继承它 struct Hello:public FunctionPass&#123; static char ID; Hello() : FunctionPass(ID)&#123;&#125; //覆写父类的虚函数，override是覆写标志，使用它时当没有正确 //进行覆写操作时编译器会报error而不是warnning，且子类覆写 //的虚函数也不用再加virtual关键字； //如果要禁止子类覆写虚函数，可以使用final关键字。 bool runOnFunction(Function &amp;F) override &#123; //打印\"Hello: 函数名称\" errs() &lt;&lt; \"Hello: \"; errs().write_escaped(F.getName()) &lt;&lt; '\\n'; return false; &#125; &#125;; &#125; //初始化ID char Hello::ID = 0; //Register for opt static RegisterPass&lt;Hello> X(\"hello\",\"Hello World Pass\", false/* Only looks at CFG */, false/* Only looks at CFG */); ​ 编写一个待测试文件test.cpp： #include&lt;stdio.h> void fun1()&#123; printf(\"111\\n\"); &#125; void fun2()&#123; printf(\"222\\n\"); &#125; void fun3()&#123; printf(\"333\\n\"); &#125; int main(void)&#123; printf(\"%d\\n\",fun1()); printf(\"%d\\n\",fun2()); //printf(\"%d\\n\",fun3()); return 0; &#125; ​ 编译pass和test文件： #编译pass，也可以自行定义CMakeLists文件，手动编译; #llvm-config提供了CXXFLAGS与LDFLAGS参数方便查找LLVM的头文件与库文件; #如果链接有问题，还可以用llvm-config --libs提供动态链接的LLVM库; #具体llvm-config打印了什么，请自行尝试或查找官方文档; #-fPIC -shared 是编译动态库的必要参数。 #因为LLVM没用到RTTI，所以用-fno-rtti 来让我们的Pass与之一致; #-Wl,-znodelete是为了应对LLVM 5.0+中加载ModulePass引起segmentation fault的bug; #若Pass继承了ModulePass，务必加上。 clang `llvm-config --cxxflags` -Wl,-znodelete -fno-rtti -fPIC -shared old_hello.cpp -o LLVMHello.so `llvm-config --ldflags` #将待测试文件编译为bitcode clang -emit-llvm -c test.cpp -o test.bc ​ 使用opt命令加载运行，运行结果： #-loda 选项表明要加载进程序的pass #-hello 是注册时规定的参数 #由于此pass并不修改程序，故将opt的输出结果放入/dev/null(丢弃) sudo opt -load ./LLVMHello.so -hello &lt; test.bc > /dev/null Hello: _Z4fun1v Hello: _Z4fun2v Hello: _Z4fun3v Hello: main ​ 在命令中加入-time-passes参数，可以获得运行时间相关结果： $sudo opt -load ./LLVMHello.so -hello -time-passes &lt; test.bc > /dev/null Hello: _Z4fun1v Hello: _Z4fun2v Hello: _Z4fun3v Hello: main ==-------------------------------------------------------------------------== ... Pass execution timing report ... ==-------------------------------------------------------------------------== Total Execution Time: 0.0006 seconds (0.0006 wall clock) ---User Time--- --System Time-- --User+System-- ---Wall Time--- ---Name --- 0.0001 ( 61.1%) 0.0002 ( 62.0%) 0.0003 ( 61.7%) 0.0003 ( 62.0%) Bitcode Writer 0.0001 ( 33.7%) 0.0001 ( 33.2%) 0.0002 ( 33.3%) 0.0002 ( 33.0%) Hello World Pass 0.0000 ( 5.3%) 0.0000 ( 4.8%) 0.0000 ( 5.0%) 0.0000 ( 5.0%) Module Verifier 0.0002 (100.0%) 0.0004 (100.0%) 0.0006 (100.0%) 0.0006 (100.0%) Total ==-------------------------------------------------------------------------== LLVM IR Parsing ===-------------------------------------------------------------------------== Total Execution Time: 0.0005 seconds (0.0005 wall clock) ---User Time--- --System Time-- --User+System-- ---Wall Time--- --- Name --- 0.0002 (100.0%) 0.0003 (100.0%) 0.0005 (100.0%) 0.0005 (100.0%) Parse IR 0.0002 (100.0%) 0.0003 (100.0%) 0.0005 (100.0%) 0.0005 (100.0%) Total ​ 如果想直接使用clang集成这个过程，需要在pass注册时添加： #include \"llvm/IR/LegacyPassManager.h\" #include \"llvm/Transforms/IPO/PassManagerBuilder.h\" // Register for clang static RegisterStandardPasses Y(PassManagerBuilder::EP_EarlyAsPossible, [](const PassManagerBuilder &amp;Builder, legacy::PassManagerBase &amp;PM) &#123; PM.add(new Hello()); &#125;); ​ 接下来无需提前编译bc文件再调用opt，直接使用clang即可： clang -Xclang -load -Xclang ./LLVMHello.so test.cpp -o clang_test Hello: _Z4fun1v Hello: _Z4fun2v Hello: _Z4fun3v Hello: main 2.2 new manager ​ llvm新版本的pass manager定义依赖于多态，意味着并不存在显示的接口，所有的 Pass 是继承自 CRTP 模板PassInfoMixin&lt;PassT&gt;，其中需要有一个run()方法，接收一些 IR 单元和一个分析管理器，返回类型为 PreservedAnalyses。 &#x2F;&#x2F;new_hellp.cpp #include &quot;llvm&#x2F;IR&#x2F;LegacyPassManager.h&quot; #include &quot;llvm&#x2F;Passes&#x2F;PassBuilder.h&quot; #include &quot;llvm&#x2F;Passes&#x2F;PassPlugin.h&quot; #include &quot;llvm&#x2F;Support&#x2F;raw_ostream.h&quot; using namespace llvm; namespace&#123; void visitor(Function &amp;F)&#123; errs() &lt;&lt; &quot;(New Hello)FunctionName: &quot; &lt;&lt; F.getName() &lt;&lt; &quot;\\n&quot;; errs() &lt;&lt; &quot;(New Hello)ArgSize: &quot; &lt;&lt; F.arg_size() &lt;&lt; &quot;\\n&quot;; &#125; struct HelloWorld:PassInfoMixin&lt;HelloWorld&gt;&#123; PreservedAnalyses run(Function &amp;F,FunctionAnalysisManager &amp;)&#123; visitor(F); return PreservedAnalyses::all(); &#125; static bool isRequired()&#123;return true;&#125; &#125;; &#125;&#x2F;&#x2F;namespace llvm::PassPluginLibraryInfo getHelloWorldPluginInfo()&#123; return&#123; LLVM_PLUGIN_API_VERSION,&quot;HelloWorld&quot;,LLVM_VERSION_STRING, [](PassBuilder &amp;PB)&#123; PB.registerPipelineParsingCallback( [](StringRef Name, FunctionPassManager &amp;FPM, ArrayRef&lt;PassBuilder::PipelineElement&gt;)&#123; if(Name &#x3D;&#x3D; &quot;hello-world&quot;)&#123; FPM.addPass(HelloWorld()); return true; &#125; return true; &#125;); &#125; &#125;; &#125; extern &quot;C&quot; LLVM_ATTRIBUTE_WEAK ::llvm::PassPluginLibraryInfo llvmGetPassPluginInfo()&#123; return getHelloWorldPluginInfo(); &#125; ​ 以上代码分为两部分，Pass本体和Pass注册，具体含义和对应的CMakeLists.txt可参照llvm-tutor，这里不做赘述。尝试运行： #编译Pass export LLVM_DIR=/usr/local/include/llvm/ cmake -DLT_LLVM_INSTALL_DIR=$LLVM_DIR ../ make #编译输入文件 clang -O1 -S -emit-llvm ../../data/test.cpp -o ../../data/test.ll #使用new manager加载pass opt -load-pass-plugin ./libHelloWorld.so -passes=hello-world -disable-output ../../data/test.ll Reference LLVM IR 的第二个 Pass：上手官方文档 New Pass Manager HelloWorld Pass - 知乎 (zhihu.com) LLVM中的pass及其管理机制 - 知乎 (zhihu.com) Using the New Pass Manager — LLVM 17.0.0git documentation 2019 LLVM Developers’ Meeting: A. Warzynski “Writing an LLVM Pass: 101” - YouTube banach-space/llvm-tutor: A collection of out-of-tree LLVM passes for teaching and learning (github.com)","categories":[{"name":"Code","slug":"Code","permalink":"http://example.com/categories/Code/"}],"tags":[{"name":"LLVM","slug":"LLVM","permalink":"http://example.com/tags/LLVM/"}],"author":"Shaw"},{"title":"GREBE-Unveiling Exploitation Potential for Linux Kernel Bugs","slug":"【论文阅读】GREBE-Unveiling-Exploitation-Potential-for-Linux-Kernel-Bugs","date":"2023-04-10T07:24:25.618Z","updated":"2023-04-10T07:28:59.767Z","comments":true,"path":"2023/04/10/【论文阅读】GREBE-Unveiling-Exploitation-Potential-for-Linux-Kernel-Bugs/","link":"","permalink":"http://example.com/2023/04/10/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91GREBE-Unveiling-Exploitation-Potential-for-Linux-Kernel-Bugs/","excerpt":"GREBE: Unveiling Exploitation Potential for Linux Kernel Bugs 时间：2022 作者：Zhenpeng Lin,Yueqi Chen,Yuhang Wu 会议：SP ABSTRACT ​ 最近，动态测试工具显著提升了Linux内核漏洞的发掘速度，这些工具会在挖掘漏洞时自动地生成报告，具体说明Linux系统的error。报告中的error暗示了相应的内核错误的可能的exploitability，因此，许多安全分析员使用(报告中)表现出来的error来推断一个错误的可利用性，从而考虑其exploit开发的优先级。然而，使用报告中的error可能会低估一个错误的可利用性。报告中表现出的error可能取决于该error是如何被触发的。通过不同的路径或在不同的背景下，一个error可能表现出各种错误行为，意味着非常不同的利用潜力。 ​ 此文提出了一个新的内核fuzz技术来找到所有可能的内核漏洞的error的表现。与传统的专注于内核代码覆盖率的内核fuzzing技术不同，我们的fuzzing技术更多的是针对有漏洞的代码片段。为了探索不同触发同一个bug的上下文/路径，文章引入了object-driven内核fuzzing技术。通过新探索的error，安全研究人员可以更好地推断出一个error的可利用性。 ​ 为了评估我们提出的技术的有效性、效率和影响，我们将我们的fuzzing技术作为一个工具GREBE来实现，并将其应用于60个真实世界的Linux内核漏洞。平均来说，GREBE可以为每个内核漏洞表现出2个以上的额外错误行为。对于26个内核错误，GREBE发现了更高的开发潜力。我们向内核供应商报告了其中的一些错误--这些错误的可利用性被错误地评估了，相应的补丁还没有被仔细地应用--导致他们迅速地采用补丁。","text":"GREBE: Unveiling Exploitation Potential for Linux Kernel Bugs 时间：2022 作者：Zhenpeng Lin,Yueqi Chen,Yuhang Wu 会议：SP ABSTRACT ​ 最近，动态测试工具显著提升了Linux内核漏洞的发掘速度，这些工具会在挖掘漏洞时自动地生成报告，具体说明Linux系统的error。报告中的error暗示了相应的内核错误的可能的exploitability，因此，许多安全分析员使用(报告中)表现出来的error来推断一个错误的可利用性，从而考虑其exploit开发的优先级。然而，使用报告中的error可能会低估一个错误的可利用性。报告中表现出的error可能取决于该error是如何被触发的。通过不同的路径或在不同的背景下，一个error可能表现出各种错误行为，意味着非常不同的利用潜力。 ​ 此文提出了一个新的内核fuzz技术来找到所有可能的内核漏洞的error的表现。与传统的专注于内核代码覆盖率的内核fuzzing技术不同，我们的fuzzing技术更多的是针对有漏洞的代码片段。为了探索不同触发同一个bug的上下文/路径，文章引入了object-driven内核fuzzing技术。通过新探索的error，安全研究人员可以更好地推断出一个error的可利用性。 ​ 为了评估我们提出的技术的有效性、效率和影响，我们将我们的fuzzing技术作为一个工具GREBE来实现，并将其应用于60个真实世界的Linux内核漏洞。平均来说，GREBE可以为每个内核漏洞表现出2个以上的额外错误行为。对于26个内核错误，GREBE发现了更高的开发潜力。我们向内核供应商报告了其中的一些错误--这些错误的可利用性被错误地评估了，相应的补丁还没有被仔细地应用--导致他们迅速地采用补丁。 问题背景 1. 根据分析结果确定漏洞优先级 ​ 为了提高Linux其安全性，研究人员和分析人员引入了自动化的内核fuzzing技术和各种调试/sanitzation功能。在他们的推动下，安全研究人员和内核开发人员变得更容易确定Linux内核中的错误。然而，要确定触发bug的条件是否足以代表安全漏洞，仍然是一个挑战。例如，一个表现out-of-bound错误行为的bug通常意味着比那些表现出空指针解除引用错误行为的bug有更高的机会被利用。因此，我们的调查结果和以前的研究[1]、[2]、[3]都表明，在确定漏洞开发工作的优先次序时，漏洞的表现出的错误行为起着关键作用。 2. 自动化漏洞扫描工具提供的错误报告不完善 ​ 在实践中，当现有的fuzzing工具识别出一个内核错误时，该错误所表现出来的错误行为可能是其众多可能的错误行为之一。它的其他可能的错误行为可能与已经暴露的错误行为相去甚远。通过遵循不同的路径或执行环境来触发内核错误，我们可以使内核错误不仅表现出不太可能被利用的GPF错误，而且表现出极可能被利用的UAF错误。因此，如果只使用单一表现的错误行为来推断该错误可能的可利用性，这可能会产生一定误导。 3. 解决思路 ​ 为了解决上述问题，一种本能的反应是把一个内核错误报告作为输入，分析该内核错误的根本原因，并推断出该错误的根本原因可能带来的所有可能的后果（例如，内存越界、空指针解引用和内存泄漏等）。然而，根源诊断通常被认为是一项费时费力的工作。因此，我们认为，解决这个问题的一个更现实的策略是，在不进行根本原因分析的情况下，揭露出一个给定的内核错误的许多可能的后触发错误行为。然后，从公布的错误行为中，安全分析师可以更好地以更准确的方式推断其可能的可利用性。 4. 已有的方法 ​ 我们可以借用内核fuzzing的概念。然而，现有的内核fuzzing方法主要是为了最大化代码覆盖率而设计的（例如，Syzkaller[4]、KAFL[5]和Trinity[6]等）。在我们的任务中使用这些方法不可避免地存在效率低下和效果差的问题，这只是因为代码覆盖率驱动的内核fuzzing法没有被在该问题下定制，也没有为寻找与同一错误代码片段相关的各种路径或上下文进行优化。 ​ 为此，我们提出了一种定制的内核fuzzing处理机制，它将fuzzing处理的能量集中在有缺陷的代码区域，同时，将内核执行路径和上下文分散到目标有缺陷的代码片段。 问题引入 ​ 我们提出的内核fuzzing技术可以被看作是一种定向fuzzing技术。它首先将一个内核错误报告作为输入，并提取与报告的内核错误相关的内核结构/对象。然后，该方法进行fuzzing测试，并利用已识别的内核结构/对象的点击率作为对fuzzer的反馈。由于确定的内核结构/对象对成功触发错误至关重要，利用它们来指导fuzzing可以缩小内核fuzzer的范围，使fuzzer主要关注与报告错误有关的路径和背景。在这项工作中，我们将这种方法作为一个内核对象驱动的fuzzing工具来实现，并以GREBE命名，意味着 \"多行为探索fuzzing\"。 1. 例子 ​ 如上图所示，函数tun_attach是网络接口配置函数，它的参数tun是一个所有处于open状态的tun文件共享的全局变量。代码第3行表明，如果IFF_NAPI在tun-&gt;flags中被设定，内核将初始化一个定时器，并将相应的napi链接到网络设备napi_list的列表中。代码第12行表明，函数tun_detach负责清理tun_file中包含的数据以及关闭该文件。如果IFF_NAPI被设置，内核将取消定时器并从设备的napi_list中删除napi。在第24行，函数free_netdev将通过napi_list来删除列表中的napi。 ​ 这里的bug是由于tun_attach和tun_detach中的标志tun-&gt;flags的可能存在的不一致导致的。 ​ 以Syzkaller产生的内核错误报告为例： ​ 报告所附的PoC显示，一个系统调用在IFF_NAPI未设置的情况下调用了tun_attach。这样一来，内核既没有初始化定时器，也没有将相应的napi添加到列表中。在这个设置之后，PoC程序进一步调用系统调用ioctl函数，在调用tun_detach之前在tun-&gt;flags中设置IFF_NAPI，这导致tun_attach和tun_detach的标志不一致。然后，在第17行，内核试图停止定时器，它解除了对tun_detach中的定时器对象所包含的指针的引用。然而，如上所述，定时器在tun_attach中没有被初始化，这导致了一个一般保护故障。一般保护故障意味着访问未被指定使用的存储。因此，基于这个单一的观察，许多分析人员可能会推断出这个错误可能是不可利用的。 ​ 然而，通过改变PoC程序，修改共享变量的赋值方式，我们可以让内核表现出一个UAF错误。具体来说，我们可以在调用函数tun_attach之前用IFF_NAPI设置tun-&gt;flags。这样，在调用tun_attach后，它可以将相应的tun_file添加到设备列表napi_list。在这个设置之后，我们可以进一步调用ioctl来清除tun_flags，然后再调用tun_detach。如上图，函数tun_detach在第18∼19行没有从列表中删除相应的napi，而是在第21行free了它。因此，当遍历设备列表时，拥有KASAN功能的内核将抛出UAF的错误。与报中显示的错误相比，这种非允许的访问不是访问一个无效的内核内存地址而产生一般的保护故障，而是与一个有效的内核内存地址联系在一起，并最终可以破坏内核内存。因此，基于这个UAF错误，许多分析人员可能认为这个错误可能是可利用的。 ps: Kernel Address SANitizer(KASAN)是一种动态内存安全错误检测工具，主要功能是检查内存越界访问和使用已释放内存的问题； UAF，Use after free。 2. 设计原理 ​ 鉴于内核错误报告展示了一个特定的错误行为，探索该错误其他可能的错误行为的一个本能反应是利用directed fuzzing，即探索通往我们感兴趣的程序站点的路径。但这种方法有两个局限： 首先，为了使用directed fuzzing来暴露多种错误行为，我们需要确定有错误的代码片段（即错误的root），将其作为锚点，并将其提供给directed fuzzer。然而，要正确和自动地确定内核错误的root是很有挑战性的。如果不正确地将一个非根本原因的点视为fuzzer的锚点，甚至可能使fuzzer无法触发该错误，更不用说找到该错误的多种错误行为了; 其次，即使我们能够指出内核错误的根源，也不意味着内核可以表现出多种错误行为。除了遵循不同的路径到达有缺陷的代码外，错误行为的展示也依赖于错误触发后的环境。例如，除了遵循一个特定的路径到有缺陷的代码片段，我们还需要一个单独的内核线程来改变一个全局变量，使触发错误和展示不同错误所需的背景多样化。根据设计，定向fuzzing法在接触到其感兴趣的目标代码后不能改变上下文。 ​ 在本文，我们通过扩展现有的带有内核-对象指导的内核fuzzing方法来解决这个问题。根据我们对许多内核错误的观察，我们发现内核错误的根本原因通常来自两种： 对内核对象的不恰当使用。这进一步导致了内核错误（例如前面提到的为tun_struct类型的内核对象分配不一致的标志值的案例）； 在使用内核对象进行计算时涉及到一个不正确的值，它被进一步传播到一个关键的内核操作中，迫使内核表现出一个错误（例如，一个未消毒的整数被用作内核对象的偏移量，导致一个越界的内存访问）。 ​ 因此，在错误报告中指定的与错误相关的对象的指导下，我们可以让内核fuzzer远离那些与错误无关的路径和上下文，从而显著提高其效率。 模型方法 ​ 如上图所示，首先将一个内核错误报告作为输入，运行所附带的PoC，并追踪那些涉及内核错误的内核结构（例如，例子中的结构tun_file）。进一步检查内核源代码，找出操作这些类型的对象的代码语句。我们将这些语句视为对内核错误触发的成功至关重要的锚点。因此，我们对这些语句进行检测，以便在进行内核fuzzing时收集对象覆盖率作为反馈，然后利用覆盖率来调整相应的PoC程序。在这项工作中，我们的内核fuzzing机制将错误报告中附加的原始PoC程序作为输入。使用一种新的突变和种子生成方法，它逐步改变PoC，提高了已知错误的多种错误行为探索的效率和效果。 1. 确定关键结构 ​ 利用后向污点分析来识别基本的内核结构（即参与给定报告中错误的相关结构）。 1.1 报告分析 &amp; 确定污点源 1.1.1 Explicit Checking ​ 在上图中，函数vhost_dev_cleanup()的作用是：如果dev-&gt;work_list为空，清除连接到vhost_dev设备上的worker；否则通过WARN_ON内的宏报错。故在这里，污点分析的sources应该就是dev-&gt;work_list。在这个例子中，内核开发者明确地将预定义的条件制定为一个表达式，并将其传递给宏WARN_ON进行错误处理。然而，对于其他一些调试功能，检查是由编译器检测的，或者由硬件完成，而不是由内核开发者编写的一段源代码。对于这些功能，条件是隐含的，不能从内核的源代码中识别。 ​ Explicit Checking就是对这一类明确条件的检查。内核开发者明确地将检查制定为一个表达式，并将其传递给标准调试功能，如WARN_ON和BUG_ON。这些宏是模式化的代码块，包括条件语句和日志语句，如果条件得到满足就会执行。除了这种标准化的记录内核错误的方式外，开发者还可以建立自己的宏，将日志语句包裹在一个辅助函数中（如上图15,16行）。 ​ 为了找到触发bug日志记录语句的源头，确定污点分析的sources，需要沿着支配树（如上图右半部分）回溯，直到找到一个条件跳转基本块。然后，我们将其相应的比较语句作为触发error记录操作的条件，从其中提取相应的变量作为sources。 1.1.2 Implicit Checking ​ 对于由编译器检测或由硬件完成的相关条件检查，称之为隐式检查。 ​ 对于通过编译器工具完成的隐式检查，内核地址消毒器（KASAN）可以对每一个内存访问进行检测，这样就可以检查对一个内存地址的访问是否合法。KASAN依靠影子内存来记录内存状态。例如，如果被检测的内核触及一个已释放的内存区域，它将产生一个错误报告，指出引发使用后释放错误的指令。关于中断所做的隐性检查（例如，由MMU检测到的一般保护故障），中断处理程序负责记录相应的指令。从这些调试机制产生的错误报告中，我们可以很容易地确定执行无效内存访问的指令。有了这些信息，我们的下一步就是确定与该无效内存访问相关的变量。 ​ 一般报告中所包含的错误指令是二进制指令。为了处理这个问题，从调试信息中，需要将二进制指令与源代码中的相应语句进行映射。假设对应的源代码是一个简单语句，只有一个加载或存储操作，在这种情况下，这个语句就是导致非法内存访问的语句，并将操作数变量作为污点源处理。然而，如果被识别的指令链接到一个涉及多个内存加载和存储的复合语句（如上图中描述的walk-&gt;offset = sg-&gt;offset），将进行进一步的分析。 ​ 具体来说，首先检查错误报告，并找出捕捉到内核错误的具体指令。然后，把与捕捉错误的指令相关的内存访问操作作为我们的sources。再次以上图中的情况为例。错误报告指出错误是由语句kasan_check_read(&amp;sg-&gt;offset, sizeof(var))捕获的，它与sg-&gt;offset有关。故这里认为第2行的sg-&gt;offset是污点源。 ​ 1.2 污点传播 &amp; 确定sink ​ 从错误报告中提取调用痕迹（call trace），基于call trace构建cfg，并在该图上向后传播污点源。 如果被污染的变量是一个嵌套结构的字段或一个联合变量，则进一步污染其父结构变量，并将父结构视为一个关键结构（原因是，嵌套结构或联合变量是内存中父结构变量的一部分。如果嵌套结构或联合变量的某个字段带有一个无效的值，这很可能是由于对其父结构变量的不恰当使用造成的）； 当污点传播遇到一个循环时，如果污点源在循环内被更新，那么也将循环计数器污染（举例，一些越界操作就是由于循环计数器被修改/破坏/扩大，导致访问了无效的内存区域）。 ​ 当以下条件之一成立时终止污点传播，在进行污点向后传播的同时，也将传播扩展到被污点变量的别名上： 污点传播到了污点变量本身的定义语句； 污点传播到了系统调用的入口、中断处理程序或启动工作队列调度器的函数入口； 2. 内核结构排序 ​ 成功识别所有与报告中的错误有关的内核结构后，直接fuzzing可能导致效率低下问题，故这里做进一步缩小操作。 2.1 内核结构选择 ​ 在Linux内核中，提供了一个用来创建双向循环链表的结构 list_head。虽然linux内核是用C语言写的，但是list_head的引入，使得内核数据结构也可以拥有面向对象的特性，通过使用操作list_head 的通用接口很容易实现代码的重用。在整个内核代码库中，list_head结构被广泛地使用。如果将这种结构和相应的对象纳入内核模糊指导，内核模糊器将不可避免地探索一个大的代码空间，使模糊器偏离重心。因此，为了保持内核fuzzer探索的效率，我们需要将这些结构从我们的内核fuzzing中排除。 ​ 除了上面提到的结构，Linux内核开发者还实现了许多其他与抽象接口有关的结构。这些接口与实现层耦合在一起，以支持大量的设备和功能。例如，内核为所有从用户空间请求的网络服务创建了一个结构socket ，不管指定什么协议。因此，与struct list_head类似，它们也应该在后期的内核fuzzing中被排除。 ​ 为了精确定位并排除这些结构，这里设计了一种系统的方法，根据内核结构的流行程度对其进行排序。在更高层次上，这个方法构建了一个描述内核结构之间引用关系的图。图中的每个节点代表一个内核结构，而节点之间的有向边表示参考关系。在该图上，应用PageRank算法，给每个结构分配一个权重，排除较高权值的结构。 2.2 构建图结构 ​ 对于结构定义语句，给定一个结构，我们浏览它的所有字段成员。如果字段是一个指向另一个结构的指针，我们就把给定的结构与被引用的结构联系起来。如上图extensions是一个引用struct skb_ext的指针，就在图中把sk_buff结构链接到skb_ext结构。 ​ struct rb_node是一个匿名union中的自我引用结构。这里就跳过匿名union，只将struct sk_buff直接链接到struct rb_node，而无需进一步扩展。 ​ 对于类型转换语句，由于内核支持多态，例如上图中ip6_fraglist_init函数，skb-&gt;data从void被投射到struct frag_hdr。void* 是一个抽象的数据类型，而结构类型struct frag_hdr*则更加具体化。因此，我们在结构图中增加了一条边，将结构skb_buff连接到结构frag_hdr。 ​ 利用构建的图，使用PageRank算法对其流行程度进行排序。只使用那些等级较低的内核结构和对象来指导fuzzing。 2.3 技术性研讨 ​ 根据我们对数百个真实世界的内核错误的观察，大多数内核错误的根本原因与不太流行的结构有关。因此，删除流行的结构并不会对模糊器触发我们感兴趣的错误产生负面影响。 ​ 即使被消除的流行结构与我们感兴趣的内核错误的根源有关，让模糊器专注于这些不太流行的结构，仍然可以让我们接触到流行结构类型中的一些对象。原因是不太受欢迎的结构通常是由受欢迎的结构组成的（例如，list 1中罕见的结构struct napi_struct包含受欢迎的结构struct hrtimer）。 3. 对象驱动内核fuzzing ​ 传统的内核探索方法是利用跟踪函数来跟踪已经执行的基本块。在这项工作中，我们的探索机制保留了这种能力，并进一步引入了一个额外的工具化组件。其被设计成一个编译器插件，该插件检查基本块中的每条语句，并识别那些负责分配、删除和使用关键对象的基本块。更具体地说，组件引入了一个新的追踪功能，它将记录的基本块地址中最重要的16位替换为一个神奇的数字，以将这些基本块与其他基本块区分开来。有了这些工具，通过观察代码覆盖率反馈中最重要的16位地址，我们可以很容易地确定哪些与关键对象有关的基本块是在fuzzing程序的操作之下的。当运行一个模糊测试程序时，我们可以很容易地确定它是否接触到了一个关键对象。 3.1 种子选择 ​ 若程序接触到了一个新的涉及关键对象的基本块/至少有一个系统调用涵盖了更多的代码（允许内核fuzzer积累内核状态，从而增加未来突变的可能性，以达到涉及关键对象的未见过的基本块。），并操作了关键对象时，将变异的种子添加到库中。 3.2 种子生成 &amp; 变异 ​ 使用报告中的POC程序用作初始种子，每次在生成新的种子时，只使用已经包含在种子库中的系统调用来组装新的种子。 ​ 进一步引入了现有内核fuzzing（即Syzkaller）中使用的变异机制。这种突变机制将与种子库中已包含的系统调用相关的新系统调用引入种子中。 3.3 变异优化 ​ 在对模糊程序进行突变时，Syzkaller的突变机制利用预定义的模板来指导新种子的合成。模板规定了系统调用之间的依赖关系和相应系统调用的参数格式。例如，Syzkaller的模板规定，系统调用read需要一个resource（即文件描述符）作为其参数之一，openat（）以及socket（）将产生相应的resource。在这个模板的指导下，Syzkaller可以通过在系统调用openat（）上附加系统调用read（）或socket（）来对模糊程序种子进行变异。模板指导下的突变确保了种子程序（也就是runner的输入）的合法性，从而避免了过早产生无效输入。 ​ 但是，这种基于模板的突变方式也不一定符合我们的需求。以上图两个POC程序为例，Syzkaller在a中插入了socket()，显然增大了搜索空间，不利于fuzzing；在b中，Syzkaller根据@max变量的合法范围是[INT_MIN，INT_MAX]从而改变了@max的值，但在此Bug中，只有@max=-1时bug才会被触发，故这里的突变也是副作用的。 ​ 故为了更高效的fuzzing，这里需要进行变异优化。根据相应的系统调用所依据的资源类型对系统调用规范模板进行分组（例如将与网络套接字和设备文件有关的系统调用分别归类）。在每组中，再将系统调用分为两个子组。一个是负责资源的创建，另一个是负责资源的使用。当变异种子程序时，我们的模糊组件要么用同一组中的系统调用替换，要么插入与种子程序中显示的资源相关的系统调用。 ps： 污点分析：三元组&lt;sources, sinks, sanitizers&gt;，代表污点源，污点会聚处和消毒器。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Vulnerability","slug":"Vulnerability","permalink":"http://example.com/tags/Vulnerability/"},{"name":"AEG","slug":"AEG","permalink":"http://example.com/tags/AEG/"}],"author":"Shaw"},{"title":"Demons in the Shared Kernel--Abstract Resource Attacks Against OS-level Virtualization","slug":"【论文阅读】Demons-in-the-Shared-Kernel-Abstract-Resource-Attacks-Against-OS-level-Virtualization","date":"2023-03-15T07:29:02.891Z","updated":"2023-03-15T07:31:18.618Z","comments":true,"path":"2023/03/15/【论文阅读】Demons-in-the-Shared-Kernel-Abstract-Resource-Attacks-Against-OS-level-Virtualization/","link":"","permalink":"http://example.com/2023/03/15/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Demons-in-the-Shared-Kernel-Abstract-Resource-Attacks-Against-OS-level-Virtualization/","excerpt":"Demons in the Shared Kernel: Abstract Resource Attacks Against OS-level Virtualization 时间：2021.11 作者：Nanzi Yang（西电）、Wenbo Shen（浙大） 会议：CCS 对docker的一种Ddos攻击，实现了自动化检测。 ABSTRACT ​ 由于其更快的启动速度和更好的资源利用效率，操作系统级虚拟化(OS-level virtualization)已被广泛采用，并已成为云计算的一项基本技术。与硬件虚拟化相比，操作系统级虚拟化利用共享内核设计来达到更高的效率，并在共享内核上运行多个用户空间实例（又称容器）。然而，在本文中，我们揭示了一个新的攻击面，此漏洞是操作系统级虚拟化技术所固有的，会影响到Linux、FreeBSD和Fuchsia。 ​ 产生漏洞的根本原因是，操作系统级虚拟化中的共享内核设计导致容器直接或间接地共享成千上万的内核变量和数据结构。在不利用任何内核漏洞的情况下，非特权容器可以轻易地用尽共享的内核变量和数据结构实例，对其他容器进行DoS攻击。与物理资源相比，这些内核变量或数据结构实例（称为抽象资源）更普遍，但受到的保护不足。 ​ 为了显示限制抽象资源（Abstract Resources）的重要性，我们针对操作系统内核的不同方面进行了抽象资源攻击。结果表明，攻击抽象资源是非常实用和关键的。我们进一步进行了系统分析，以识别Linux内核中易受攻击的抽象资源，成功检测出1010个抽象资源，其中501个可以被动态地重复消耗。我们还在四大云厂商的自部署共享内核容器环境中进行了攻击实验。结果显示，所有环境都容易受到抽象资源的攻击。我们得出结论，限制抽象资源的使用是很难的，并给出了减轻此风险的多种策略。","text":"Demons in the Shared Kernel: Abstract Resource Attacks Against OS-level Virtualization 时间：2021.11 作者：Nanzi Yang（西电）、Wenbo Shen（浙大） 会议：CCS 对docker的一种Ddos攻击，实现了自动化检测。 ABSTRACT ​ 由于其更快的启动速度和更好的资源利用效率，操作系统级虚拟化(OS-level virtualization)已被广泛采用，并已成为云计算的一项基本技术。与硬件虚拟化相比，操作系统级虚拟化利用共享内核设计来达到更高的效率，并在共享内核上运行多个用户空间实例（又称容器）。然而，在本文中，我们揭示了一个新的攻击面，此漏洞是操作系统级虚拟化技术所固有的，会影响到Linux、FreeBSD和Fuchsia。 ​ 产生漏洞的根本原因是，操作系统级虚拟化中的共享内核设计导致容器直接或间接地共享成千上万的内核变量和数据结构。在不利用任何内核漏洞的情况下，非特权容器可以轻易地用尽共享的内核变量和数据结构实例，对其他容器进行DoS攻击。与物理资源相比，这些内核变量或数据结构实例（称为抽象资源）更普遍，但受到的保护不足。 ​ 为了显示限制抽象资源（Abstract Resources）的重要性，我们针对操作系统内核的不同方面进行了抽象资源攻击。结果表明，攻击抽象资源是非常实用和关键的。我们进一步进行了系统分析，以识别Linux内核中易受攻击的抽象资源，成功检测出1010个抽象资源，其中501个可以被动态地重复消耗。我们还在四大云厂商的自部署共享内核容器环境中进行了攻击实验。结果显示，所有环境都容易受到抽象资源的攻击。我们得出结论，限制抽象资源的使用是很难的，并给出了减轻此风险的多种策略。 针对容器的抽象资源攻击 ​ 操作系统级虚拟化可以在同一个内核上运行多个用户空间容器，与硬件虚拟化相比，其减轻了模拟操作系统内核的负担，故有着更快的速度和更高的资源利用率。用户空间的操作系统级虚拟化实例，在FreeBSD上叫jails、在Solaris上叫Zones、在Liunx上叫containers。 ​ 由于共享内核机制的存在，注意到，底层的操作系统内核包含数为容器提供服务的十万个变量和数据结构实例。因此，这些容器直接或间接地共享这些内核变量和数据结构实例。 ​ 这些抽象资源可以被利用来进行DoS攻击，并且系统对它们的保护措施往往不足。内核和容器开发者更注重保护物理资源而不是抽象资源。例如，Linux内核提供控制组来限制每个容器实例的资源使用。然而，在13个控制组中，有12个是针对物理资源的，限制了CPU、内存、存储和IO的使用。只有PIDs控制组是为限制抽象资源PID而设计的。因此，数百个容器共享的抽象资源没有任何限制，如global dirty ratio、open-file structs、pseudo-terminal structs等，这使得它们容易受到DoS攻击。 ​ 举个例子，下图是Linux内核中的一个全局变量nr_files及其利用函数，nr_files是系统中任意时刻文件数量的上限值，限制的文件总数。然而Linux内核并没有对nr_files变量提供任何控制隔离措施，因此，所有容器都可以直接控制nr_files的值。 ​ 在Linux世界中，所有几乎所有操作都可以看做文件操作，计时器、事件生成、运行命令等。一个容器可以在几秒内轻松消耗完nr_files的值，这样导致的结果就是同一系统内的其它容器在系统资源还很充裕的时候，一条命令、一个程序都不能运行。 自动检测可利用的抽象资源 ​ 文章的整体思路就是围绕着抽象资源的检测与利用进行。为了检测出系统中可利用的抽象资源，文章提出了： configuration-based analysis 和access-based analysis方法用于查找内核中共享在容器中的抽象资源； Syscall Reachability Analysis和三个Restriction Analysis方法用于确定抽象资源可以被容器消耗完。 1. Configuration-based Analysis &amp; Access-based Analysis 1.1 configuration-based analysis ​ Linux下的sysctl命令可以查看/修改内核参数，这些参数位于/proc/sys目录下。注意到，这些sysctl配置大多用于抽象的资源限制，比如限制文件数量fs.file-nr或内存大页面vm.nr_hugepages。因此，所有的容器都在共享由sysctl配置指定的相同的全局限制。这种sysctl配置提供了关于容器之间可共享的抽象资源的重要线索。 ​ 故这里的configuration指的就是sysctl配置参数。基于配置的分析分为三步： 首先，它使用特定的sysctl数据类型来识别所有与sysctl相关的数据结构。这些数据结构包含可配置的sysctl内核参数； 其次，sysctl数据结构通常包含在/proc/sys/文件夹中显示sysctl值的函数。因此，通过分析该函数，我们能够准确地找出该内核参数的变量； 最后，如果一个内核参数被用于限制资源消耗，其相应的变量应该出现在比较指令中。因此，我们按照使用-定义链来检查所确定的变量的使用情况，如果它在比较指令中被使用，就把它标记为抽象资源 ​ 如下图所示，Linux的proc文件系统使用数据结构ctl_table来配置sysctl内核参数。我们在LLVM中设计并实现了一个程序间分析通道，分析程序首先遍历所有内核全局变量来查找所有ctl_table数据结构，跟随.proc_handler回调指针启动程序间分析以获得确切的变量，定位到19行的nr_files关键变量。最后，检查所有已识别的关键变量的使用情况。如果一个关键变量在比较指令中被使用（即LLVM IR中的icmp），就会记录这些位置并将这个变量标记为抽象资源（25行，nr_files）。 1.2 access-based analysis ​ 除了sysctl配置，Linux内核还使用锁或原语机制来保护并发访问的资源。因此，我们使用并发访问性质作为标识一组可共享的抽象资源的标志。如果某个数据结构本身就是锁，或者在上锁/解锁之间被定量修改，我们就将其定位为抽象资源。 ​ 同时，分析方法还考虑了atomic和percpu计数器，其分析方法都集成在LLVM中。 2. Syscall Reachability Analysis &amp; Restriction Analysis 2.1 syscall reachability analysis ​ 为了确定筛选出来的抽象资源可以被容器消耗，我们根据内核控制流图进行传统的后向控制流分析，其中间接调用是根据结构类型来解决的[42, 70]。如果没有从系统调用条目到抽象资源消耗点的路径，我们就把这个抽象资源从容器中标记为不可达。 2.2 restriction analysis ​ 仅有可达性分析是不够的，我们需要进一步确保路径上没有额外的针对容器的限制。如seccomp、命名空间、控制组以及每个用户的资源限制。 2.2.1 seccomp ​ Seccomp是一种用于系统调用过滤的机制。我们对seccomp的限制分析中，使用Docker默认的seccomp配置文件[15]，它阻止了50多个系统调用。在所有从系统调用条目到资源消耗点的路径中，过滤掉源自任何被阻止的系统调用的路径。 2.2.2 per-user ​ 在实际部署中，容器通常使用不同的用户运行。因此，每个容器的资源消耗也被每个用户的资源配额所限制。例如，Linux提供了用户限制命令ulimit来限制特定用户的资源消耗。而ulimit的底层实现是使用rlimit来设置多个每个用户的资源配额。 ​ 除了ulimit，Linux还提供了一些接口，允许用户利用PAM（Pluggable Authentication Module）来部署每个用户的配额。PAM使用setup_limits函数[64]来设置每个用户的资源配额，它调用setrlimit来配置多个rlimit约束。对于由ulimit、rlimit和PAM限制的资源，攻击者容器不能消耗超过每个用户的配额。因此，它不能完全控制这些抽象的资源来发动DoS攻击。 ​ 由于ulimit和PAM都使用rlimit来设置每个用户的资源配额，我们需要分析rlimit并过滤出受其限制的抽象资源。对于rlimit分析，我们的关键观察是，rlimit值通常是在struct rlimit或struct rlimit64中指定的。因此，我们首先遍历内核IR，以确定所有从结构rlimit或结构rlimit64加载的变量。然后，我们进行数据流分析，跟踪这些变量的所有传播和使用情况，如果这些变量在任何比较指令中被使用，则标记这些函数。在这些函数中，rlimit被检查以限制某些资源。我们认为这些资源不能被攻击者容器用尽，因此我们根据这些函数过滤掉路径。我们的工具确定了40个检查rlimit的函数。 2.2.3 namespace ​ 对于一个命名空间隔离的资源，Linux内核会在每个命名空间下为其创建一个 \"副本\"，这样在一个命名空间的修改就不会影响到其他命名空间。因此，为了确认容器的可控性，我们需要确保那些抽象资源不受名字空间的保护。这里存在一个问题，即使Linux有关于命名空间的文档，也没有关于哪些抽象资源被命名空间所隔离的规范。 ​ 观察到，对于一个被命名空间隔离的资源，相应的数据结构有一个指针字段，指向它所属的命名空间。因此，我们的工具首先遍历了内核中每个数据结构类型的所有字段。如果该类型有一个命名空间指针，我们就把它标记为一个被限制隔离的资源。其次，对于识别出的隔离资源，我们的工具用它来过滤§4.1中识别的共享抽象资源。请注意，由于不同命名空间之间的映射，一些命名空间隔离的资源可能仍然容易受到抽象资源的攻击。如§3.2.2所述，idr是由pid_namespace-&gt;idr隔离的。然而，在非根PID命名空间分配的每个idr都被映射到根PID命名空间的一个新idr，这样根命名空间就可以管理它。因此，根PID命名空间被所有PID命名空间的所有容器全局共享。因此，它仍然容易受到idr耗尽的攻击。在我们的分析中，我们手动过滤掉这些资源。 ​ 如图所示，通过两次分析，程序就可以自动识别内核中可利用的抽象资源。由于本文重点关注漏洞的AEG利用，这里的容器攻击难点其实就是自动化查找抽象资源，利用并不难，故本文到这里结束。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Vulnerability","slug":"Vulnerability","permalink":"http://example.com/tags/Vulnerability/"},{"name":"AEG","slug":"AEG","permalink":"http://example.com/tags/AEG/"},{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/tags/Cloud/"}],"author":"Shaw"},{"title":"Windows of Vulnerability--A Case Study Analysis","slug":"【论文阅读】Windows of Vulnerability A Case Study Analysis","date":"2023-02-27T07:10:38.311Z","updated":"2023-02-28T10:41:26.468Z","comments":true,"path":"2023/02/27/【论文阅读】Windows of Vulnerability A Case Study Analysis/","link":"","permalink":"http://example.com/2023/02/27/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Windows%20of%20Vulnerability%20A%20Case%20Study%20Analysis/","excerpt":"【论文阅读】Windows of Vulnerability: A Case Study Analysis 时间：2000 作者：Arbaugh W A, Fithen W L, McHugh J 期刊：Computer ABSTRACT 作者为系统漏洞提出了一个“生命循环”模型，以此来分析三个样例，揭示了系统如何在安全修复后的很长一段时间内仍然存在漏洞。","text":"【论文阅读】Windows of Vulnerability: A Case Study Analysis 时间：2000 作者：Arbaugh W A, Fithen W L, McHugh J 期刊：Computer ABSTRACT 作者为系统漏洞提出了一个“生命循环”模型，以此来分析三个样例，揭示了系统如何在安全修复后的很长一段时间内仍然存在漏洞。 Model 漏洞的生命周期 1.本文提出的模型： Birth：代表漏洞的产生，漏洞通常在大型开发项目的过程中无意地被制造。若漏洞是故意被制造的，那么Birth和Discovery同时发生； Discovery：代表有人发现了漏洞的存在，许多情况下漏洞的发现者并不会向外披露他的发现； Disclosure：代表漏洞的存在被传播到更广的范围，例如被上传到了Bugtraq的邮件列表中，向公众公开； Correction：代表开发商提供了软件补丁或者更改了配置，修补了漏洞； Publicity：代表漏洞已经广为人知，例如被新闻报道或被应急事件响应中心发布报告，一旦漏洞在Disclosure阶段失控，漏洞就会被广为人知； Scripting：代表漏洞的自动化利用程序已经被编写，脚本小子也可以利用脚本来实施攻击。Scripting极大地增加了对应漏洞的被利用次数； Death：代表漏洞的湮灭，当一个漏洞可以利用的系统数量缩减到不重要的时候，这个漏洞就会死亡。理论上来说，如果所有的系统都安装了对应补丁、或者漏洞影响的系统被淘汰、或者攻击者和媒体对该漏洞失去兴趣，这个漏洞就会消亡。 ​ 一般来说，Birth、Discovery和Disclosure是按顺序进行的，Correction、Publicity、Scripting顺序不定。 2. 三个Case Studies： 2.1 Phf incident ​ phf漏洞是一个命令执行漏洞： ​ 理论上，利用这个漏洞可以在目标主机上执行任意命令，但网上第一个发布的phf漏洞的利用脚本只是尝试在目标主机上下载密码文件，并无过多动作。有趣的是，向CERT报告的大多数事件只涉及下载密码文件，可见：大多数脚本小子们在不了解脚本的情况下盲目地运行利用脚本，如果他们懂得漏洞的原理，返回一个shell明显可以做更多事情。 ​ 从图中可以看到，漏洞的利用次数从自动脚本化开始就激增。随着96年8月该漏洞的相关补丁发布，入侵行为短时间内骤减。但直到1998年该漏洞的利用行为都一直大量存在。 2.2 IMAP incident ​ IMAP邮件服务提供了通过互联网访问电子邮件的功能。其在早期的版本存在缓冲区溢出漏洞，下图显示了其两个短期内发现的相似的漏洞。 ​ 可以看到，两个漏洞的柱状图形状大体相同。 2.3 BIND incident ​ Bind是DNS协议的一种实现，是现今互联网上使用最为广泛的DNS 服务器程序。其1998年存在缓冲区溢出漏洞： ​ 漏洞数据来源 ​ CERT/CC，美国计算机紧急事件响应小组协调中心。成立于1998年，位于匹兹堡的卡内基梅隆大学内。它的主要职能是对软件中的安全漏洞提供咨询，对病毒和蠕虫的爆发提供警报，向计算机用户提供保证计算机系统安全的技巧以及在处理计算机安全事故的行动中进行协调。当有恶意程序或系统漏洞被报告时，CERT/CC会对外发布“事件”，其描述了漏洞的性质、利用的方式或工具等。 ​ 作者从CERT/CC数据库中选择样例进行分析，数据涵盖了1996~1999年的一些特有的入侵行为。 ''人''的薄弱 ​ 从上述三个例子可以看出，漏洞的自动化利用是导致相关入侵行为激增的关键，但除此之外，可以注意到的是： 即使漏洞的相关修补措施已经发布，相关的入侵行为仍在其后的很长一段时间内存在，甚至增长。在漏洞补丁发布之后，一方面，需要时间传播与安装；另一方满，一些谨慎的组织要求在改变系统之前进行测试，这是正确的，以确保补丁不会产生新的问题。有些组织在 \"有机会 \"的时候会安装补丁，而其他组织可能永远不会安装补丁，其中牵扯到的原因有很多； 即使一些漏洞修补措施仅能防御一些简单的攻击，但其仍有意义。在漏洞发布后紧跟的修补措施所防止的入侵可能只是脚本小子的骚扰性攻击，更复杂的攻击者可能仍然会成功。但即使这样，也能减少整体的入侵行为，留出更多的时间精力资源跟踪调查更复杂的攻击。","categories":[{"name":"paper","slug":"paper","permalink":"http://example.com/categories/paper/"}],"tags":[{"name":"Vulnerability","slug":"Vulnerability","permalink":"http://example.com/tags/Vulnerability/"},{"name":"Windows","slug":"Windows","permalink":"http://example.com/tags/Windows/"}],"author":"Shaw"},{"title":"Symbolic Execution----从思维上理解符号执行","slug":"Symbolic Execution-符号执行","date":"2023-02-16T07:07:49.314Z","updated":"2023-02-17T11:40:01.653Z","comments":true,"path":"2023/02/16/Symbolic Execution-符号执行/","link":"","permalink":"http://example.com/2023/02/16/Symbolic%20Execution-%E7%AC%A6%E5%8F%B7%E6%89%A7%E8%A1%8C/","excerpt":"Symbolic Execution：从思维上理解符号执行 ​ 符号执行作为软件分析、漏洞发掘领域经常出现的技术，国内已经有不少文章总结讨论。但新手直接翻阅学习这些总结性质的文章时碰到的诸如“符号路径约束”、“约束求解”、“执行状态”等专有名词会难以理解，思维逻辑与理论概念不易同轨。本文从国外的一些热门教程入手， 将符号执行从思维上捋一遍， 诠才末学，仅做抛砖引玉之用。 1. 引子： 1.0 一个样例 ​ 从一段经典的代码入手，解释符号执行的核心思路。 ​ 分析目标：对于任意可能的输入组合(a,b)，找到使得断言assert语句失败的所有输入。","text":"Symbolic Execution：从思维上理解符号执行 ​ 符号执行作为软件分析、漏洞发掘领域经常出现的技术，国内已经有不少文章总结讨论。但新手直接翻阅学习这些总结性质的文章时碰到的诸如“符号路径约束”、“约束求解”、“执行状态”等专有名词会难以理解，思维逻辑与理论概念不易同轨。本文从国外的一些热门教程入手， 将符号执行从思维上捋一遍， 诠才末学，仅做抛砖引玉之用。 1. 引子： 1.0 一个样例 ​ 从一段经典的代码入手，解释符号执行的核心思路。 ​ 分析目标：对于任意可能的输入组合(a,b)，找到使得断言assert语句失败的所有输入。 ​ 对于以上函数，如果使用随机测试的方法，共有232×232种输入组合，想要找到所有的符合条件的输入就得遍历所有输入，显然，这么做效率低下，费时费力。 ​ 符号执行的核心思想有以下几点： 每一个输入变量都被映射为了一个符号，例如int i ——&gt; αi ； 每一个符号代表所有可能的输入值的集合，例如 αi ∈[0，232-1] ； 程序中的语句都由符号来完成运算，例如i*2 + 5 ——&gt; 2· αi + 5 ； ​ 如果遇到了分支条件，需要考虑每一个分支下可能执行的路径： ​ 在开始第一次符号执行演练之前，这里定义执行状态（Execution state）的概念： \\[ Execution\\enspace state=\\{stmt,\\sigma,\\pi\\} \\] stmt是当前状态的下一个状态； σ是将程序变量映射到符号或者常数的映射，如果变量的值不确定就是映射到符号，反之映射到常量； π表示路径上的限制，即到达当前状态需要什么限制条件。例如对于上述函数，只有a^(!b)才能到达第6行，所以若程序在第6行的π = \\((\\alpha_a \\neq 0 )\\and (\\alpha_b = 0)\\)，π的初始值为true。 ​ 如果使用符号执行来分析上述函数，在程序的开头，会得到第一个执行状态。根据上述定义，其stmt是它的下个待分析状态，也就是程序的第二行：“int x = 1, y = 0;”；其σ会将第一行引入的变量a,b映射到符号，由于a,b是输入，都不是常量，故可以得到：\\(\\sigma=\\{a\\rightarrow \\alpha_a, b\\rightarrow \\alpha_b\\}\\)；1开始并没有遇到条件判断语句，故π为初始值true。如下图所示： ​ 当状态A接着执行，我们遇到了一个整型声明语句，其添加了两个新变量x，y并赋初始值。故从状态A到状态B其σ添加了新的映射，stmt变为下一条待执行语句，π不变： ​ 从状态B的stmt表明接下来的分析遇到了条件判断语句，对于if语句的两种可能，衍生出两个执行状态： ​ 当状态D继续执行就遇到了assert断言语句，在断言处我们判断当前的条件（σ、π）下是否满足断言条件： ​ 如图所示，在x = 1，y = 0，且αa = 0的条件下，x-y ≠ 0 ，断言并未触发，此条分支执行结束。 ​ 当C状态继续执行，遇到了y = 3 + x赋值语句，这时就要改变σ中的映射关系： ​ 状态E继续执行，遇到条件判断语句，同理，生成两个分支： ​ 状态G进行逻辑判断，并未触发： ​ 状态F继续执行，进行判断，触发错误条件： ​ 到此为止，所有分支执行完毕，符号执行判断出当a=2且b=0时，断言语句会被触发。如果我们的测试目标从触发断言语句改为触发漏洞行为，符号测试就是进行漏洞的排查。 1.1 机器如何判断？ ​ 上文所述的例子描述了符号执行的基本逻辑：用符号代替变量，判断最终生成的布尔表达式是否可满足。在上文中，我们当然可以轻松地用大脑分析出只要输入a = 2且b = 0，断言就会被触发。但在面对复杂且庞大的逻辑表达式时，我们不能每次都去人工分析，想要完整的利用计算机实现符号执行，就需要解决自动化判断逻辑表达式是否可满足的问题。 ​ SAT，布尔可满足性问题，它询问给定布尔公式的变量是否可以用值TRUE或FALSE一致地替换，以使该公式的值为TRUE。例如$a\\and b $就是可满足的，而 \\(a \\and !a\\) 就是不可满足的。SAT在理论上已经被证明是NP问题，虽然可以快速验证这个问题的解，但不能确定其求解速度。目前已经有很多成熟的SAT算法。 ​ SMT将SAT实例中的一些二进制变量（True or False）替换为非二进制变量，非二进制变量包括线性不等式（x+2y &gt; 4）、符号等式（\\(f(f(u,v),v) = f(u,v)\\)）。SMT的背景符合符号执行所面临的的问题，利用已有的SMT解法就可以解决上述的逻辑表达式自动化判断问题。可以看到，我们将符号执行过程中的一系列限制条件称为约束（constraint），而使用SMT对该约束进行求解的过程就叫约束求解。 ​ 使用符号执行不并一定要具体的理解SMT的技术细节，但其逻辑大体如下： ​ 对于SMT这类NP完全问题，在指定时间内SMT求解器会给出是否满足的结果，若超出时间则给出“不知道”。 ​ 综上，在使用前文定义的符号执行流程分析后，配合SMT Slover即可进行完整的符号执行分析。 1.2 如何证明没有其它样例？ ​ 如何证明我们找到的\"a=2且b=0\"就是所有的满足条件的输入了？ ​ 从理论上来说，符号执行遍历执行了程序所有可能的条件分支，分析了所有可能路径下的SMT问题，以此得出的结果必然是完整的，没有其它特例的。但在实际应用中，存在一些很难被解决的限制，执行生成的路径可能特别多，并且影响程序执行路径的因素不再是a，b几个变量这么简单，这些因素导致了在实际的符号执行中不便于做理论完整的评估。 2. 进一步深入 2.1 memory model ​ 看看如下代码： ​ 当代码中涉及到对内存的操作时，又如何设计相应符号呢？事实上我们将整个内存看成一个大数组MEM[]，对内存的操作不过是对数组的操作： ​ 这里的y1与y不完全相同，y代表地址，y1代表MEM数组中的位置。在C语言中，y = x + 10其实是y指针往后移动了10个x指针类型的位置，也就是y1 = x1 + 10*sizeof(int)。而对于malloc函数，可以将其简单看做返回未使用空间起始地址的函数，下面是一个简单的抽象： ​ 当将堆内存抽象为数组操作时，现有SMT求解器支持基于数组的逻辑操作（Theory of arrays）。对数组的操作可以抽象为： \\[ \\begin{equation} a\\{i\\rightarrow e\\}[k] = \\left\\{ \\begin{array}{cl} a[k] &amp; if &amp; k \\neq i \\\\ e &amp; if &amp; k = i \\end{array} \\right \\\\ \\end{equation} \\] ​ 经过一系列的数组操作： \\[ a\\{i\\rightarrow 5\\}\\{j\\rightarrow 7\\}···[k] = 5 \\Leftrightarrow ··· \\] ​ SMT引擎会将数组操作转化为对应的布尔操作，进而进行求解。当然，这个函数与C语言中malloc具体实现很不一样，其忽略了很多细节，例如对当前分配空间是否使用的检查。但经过这样简单的抽象，就可以使用上述的思路进行符号执行的判断。 ​ memory model，翻译为“内存建模”，从以上的例子可以看出，面对复杂的数据结构、函数操作甚至是陌生的第三方库，在使用符号执行前需要对其进行合适的抽象建模。目前已经有许多研究成果针对C语言的第三方库建模。建模的好坏也直接影响符号执行的性能与效率，例如对于上述的malloc，如果我们仅仅想检测最简单的缓冲区溢出漏洞，考虑malloc复杂的堆分配机制就是多余的。 2.2 path explosion ​ 抛开上述举例的toy code，现实中需要检测的商业软件一般都有成千上万行代码，其结构包含大量分支、循环、内存操作等等，如果还按照上文的思路进行执行，不仅会导致搜索时间的过长，还会使最终的执行状态过大。 ​ 在设计符号时，就应该避免引入无关/作用不大的符号，避免状态过大。且在路径搜索时可以使用深度优先、随机路径、广度优先、错误路路径优先等算法优化搜索方法，以求达到广度与深度的平衡。（又是经典的exploration and exploitation问题） ​ 具体的进一步深入可以查看：Baldoni R, Coppa E, D’elia D C, et al. A survey of symbolic execution techniques[J]. ACM Computing Surveys (CSUR), 2018, 51(3): 1-39. 3. 实践一下 ​ 对于源代码分析，KLEE[4]是很个很棒的框架工具；对于二进制分析，S2E[5]是个非常强有力工具，但使用稍显复杂。 ​ 这里使用angr，angr[6]是一个分析二进制的python框架，使用非常便捷。 目标1：使用angr符号执行分析本文举出的第一个例子，follbar()的二进制文件. 目标2：使用angr符号执行分析logic-bomb二进制程序. ​ 安装相应虚拟环境： #安装python虚拟环境 sudo pip3 install virtualenvwrapper #创建目录用来存放虚拟环境 mkdir $HOME/.virtualenvs #在~/.bashrc中,最后添加 export WORKON_HOME=$HOME/.virtualenvs VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 source /usr/local/bin/virtualenvwrapper.sh #创建一个名为angr的虚拟环境 mkvirtualenv angr #进入虚拟环境并安装angr workon angr pip3 install angr 3.1 foolbar ​ 使用idafree-7.0打开slide-example/example文件，找到foolbar函数： ​ 打开行前缀选项：Options–&gt;General–&gt;Disassembly–&gt;Line prefixes(graph)，可以显示汇编指令地址： ​ 想要使用angr分析函数，需要给定其： start target：代码开始运行地址； find target：代码结束运行地址； avoid targets：哪些路径（地址）被忽略，以此来避免路径爆炸问题； ​ 由于angr不知道什么是符号什么是常数，后续还需要定义输入的符号。angr分析脚本如下： import angr proj = angr.Project('example') # customize XXX, YYY, and ZZZ!!! start = XXX # addr of foobar avoid = [YYY] # point(s) that are not interesting (e.g., early exits) end = ZZZ # point that I want to reach # blank_state since exploration should start from an arbitrary point # otherwise, use entry_state() state = proj.factory.blank_state(addr=start) # arguments are inside registers in x86_64 a = state.regs.edi b = state.regs.esi sm = proj.factory.simulation_manager(state) while len(sm.active) > 0: print(sm) # get a feeling of what is happening sm.explore(avoid=avoid, find=end, n=1) if len(sm.found) > 0: # Bazinga! print(\"\\nReached the target\\n\") state = sm.found[0] print(\"%edi = \" + str(state.solver.eval_upto(a, 10))) print(\"%esi = \" + str(state.solver.eval_upto(b, 10))) break ​ 代码整体很简洁，angr.Project构建Project类，Project类是angr模块的主类，它对一个二进制文件进行初始的分析以及参数配置，并将数据存储起来进行后续进一步分析。proj.factory.blank_state()指定任意地址开始运行。在Angr中，程序每执行一步（也就是step）就会产生一个状态（state，在SM这里叫做stashe）,Simulation_Manager就是提供给我们管理这些state的接口。注意到，符号变量是使用寄存器进行存储的。 ​ 运行脚本： ​ 可以看到，脚本程序分析出，当edi(a)=2或者2147483650，esi(b)=0时会触发断言，这里2147483650显然就是整数溢出后的2。以此可见符号执行的程序实现需要考虑到计算机数字与数学数字的区别。 3.2 logic-bomb ​ logic-bomb是一个没有源码的二进制程序，你需要依次输入正确答案以此逃过爆炸： ​ 用IDA打开： ​ 可以看到程序从主函数开始分别调用phase_1()、phase_2()······phase_6()，进入phase_1()： ​ 如果你懂得逆向的知识，可以尝试人工分析去解开这个bomb，但如果使用符号执行分析，只需要确定使其成功通过的地址即可： ​ 分析脚本如下： import angr import logging import claripy import pdb import resource import time proj = angr.Project('bomb') start = XXX avoid = [YYY] end = [ZZZ] state = proj.factory.blank_state(addr=start) # a symbolic input string with a length up to 128 bytes arg = state.se.BVS(\"input_string\", 8 * 128) # an ending byte arg_end = state.se.BVS(\"end_input_string\", 8) # add a constraint on this byte to force it to be '\\0' state.se.add(arg_end == 0x0) # the constraint is added to the state. # Another way to do same is with: # arg_end = state.se.BVV(0x0, 8) # in this case arg_end is a concrete value # concat arg and arg_end arg = state.se.Concat(arg, arg_end) # an address where to store my arg bind_addr = 0x603780 # bind the symbolic string at this address state.memory.store(bind_addr, arg) # phase_one reads the string [rdi] state.regs.rdi = bind_addr # make rsi concrete state.regs.rsi = 0x0 pg = proj.factory.simulation_manager(state) start_time = time.time() while len(pg.active) > 0: print(pg) # step 1 basic block for each active path pg.explore(avoid=avoid, find=end, n=1) # Bazinga! if len(pg.found) > 0: print() print(\"Reached the target\") print(pg) state = pg.found[0] sol = state.solver.eval(arg, cast_to=bytes).decode('ascii').split('\\0')[0] print(\"Solution: \" + sol) break print() print(\"Memory usage: \" + str(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024) + \" MB\") print(\"Elapsed time: \" + str(time.time() - start_time)) ​ 执行脚本： ​ 可以看到，符号执行仅用40秒就找到了phase1()的口令：“Border relations with Canada have never been better.” ​ 对于phase_2()······phase_6()使用同样的方法即可破解，这里略。 ​ 综上，我们可以看到符号执行在面对复杂的二进制文件时卓越的能力表现，在logic-bomb中，其二进制文件并没有经过混淆处理，相应的函数名，完整字符串都可以经过IDA分析后直接看到，所以对logic-bomb人工分析其实也并不复杂。但在真实的软件测试环境里，大量的二进制文件都是经过混淆加壳处理的，人工分析无法直接定位关键函数的作用（logic-bomb直接用函数名体现了函数的作用），甚至定位函数的进出口都变得具有挑战性，这时符号执行的优越性就有更明显的体现。 ​ 参考： [1] Symbolic Execution [2] symbolic-execution-tutorial/README.md at master · ercoppa/symbolic-execution-tutorial (github.com) [3] MIT 6.858 Computer Systems Security, Fall 2014—–10. Symbolic Execution [4] KLEE [5] Overview - S²E: A Platform for In-Vivo Analysis of Software Systems (s2e.systems) [6] angr","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Vulnerability","slug":"Vulnerability","permalink":"http://example.com/tags/Vulnerability/"},{"name":"Symbolic Execution","slug":"Symbolic-Execution","permalink":"http://example.com/tags/Symbolic-Execution/"}],"author":"Shaw"},{"title":"(论文复现)How Machine Learning Is Solving the Binary Function Similarity Problem","slug":"【论文复现】How Machine Learning Is Solving the Binary Function Similarity Problem","date":"2023-02-12T02:04:09.593Z","updated":"2023-02-17T11:35:45.413Z","comments":true,"path":"2023/02/12/【论文复现】How Machine Learning Is Solving the Binary Function Similarity Problem/","link":"","permalink":"http://example.com/2023/02/12/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91How%20Machine%20Learning%20Is%20Solving%20the%20Binary%20Function%20Similarity%20Problem/","excerpt":"【论文复现】How Machine Learning Is Solving the Binary Function Similarity Problem 一、模糊哈希（Fuzzy Hashing） 1. Bytes fuzzy hashing——Catalog1 出处： ​ 1. xorpd | FCatalog ​ 2. binary_function_similarity/Models/Catalog1 at main · Cisco-Talos/binary_function_similarity (github.com) \"FCatalog allows you to keep a database of all your named functions, and find similarities from this database efficiently.\" 前置知识： binary blob：一串bytes； S(a)：字节串a的四字截取集合，例如: \\[ \\mathbf{S(a)}= \\{123x,23xy,3xy1\\},a=123xy1 \\\\ \\mathbf{S(b)}= \\{1111\\},b=111111111 \\\\ \\] Jaccard Similarity：计算两个集合相似性： \\[ J（S(a),S(b)）=\\frac {S(a)\\cap S(b)}{S(a)\\cup S(b)} \\] minhash：文本相似度比较算法，用于快速估算两个集合的相似度。 ​ Catalog1算法利用minhash，使用哈希近似替代两个S集合，以此来估算其Jaccard系数，从而达到高效率地比较两个集合。","text":"【论文复现】How Machine Learning Is Solving the Binary Function Similarity Problem 一、模糊哈希（Fuzzy Hashing） 1. Bytes fuzzy hashing——Catalog1 出处： ​ 1. xorpd | FCatalog ​ 2. binary_function_similarity/Models/Catalog1 at main · Cisco-Talos/binary_function_similarity (github.com) \"FCatalog allows you to keep a database of all your named functions, and find similarities from this database efficiently.\" 前置知识： binary blob：一串bytes； S(a)：字节串a的四字截取集合，例如: \\[ \\mathbf{S(a)}= \\{123x,23xy,3xy1\\},a=123xy1 \\\\ \\mathbf{S(b)}= \\{1111\\},b=111111111 \\\\ \\] Jaccard Similarity：计算两个集合相似性： \\[ J（S(a),S(b)）=\\frac {S(a)\\cap S(b)}{S(a)\\cup S(b)} \\] minhash：文本相似度比较算法，用于快速估算两个集合的相似度。 ​ Catalog1算法利用minhash，使用哈希近似替代两个S集合，以此来估算其Jaccard系数，从而达到高效率地比较两个集合。 代码结构分析： ​ Catalog1算法需要将二进制文件截取分割后，由于每个二进制文件的长度不定，故其分割计算出的S(a)也不定长。为了对每个二进制文件都得到一个定长的特征签名，就需要对S(a)进行特征提取。 ​ 使用哈希函数将集合中的每个DWORD映射到DWORD，这里的哈希采用类似密码学中置换的方法（permutation）： #define WORD_SIZE 32 // 32 bits #define MAX_WORD 0xffffffff // Maximum size of a dword. #define BYTE_SIZE 8 // Amount of bits in a byte. #define NUM_ITERS 4 // Amount of iterations per permutation. // There are 128 RAND_DWORDS. Don't change the amount of random dwords here. unsigned int RAND_DWORDS[] = &#123;1445200656, 3877429363, 1060188777, 4260769784, 1438562000, 2836098482, 1986405151, 4230168452, 380326093, 2859127666, 1134102609, 788546250, 3705417527, 1779868252, 1958737986, 4046915967, 1614805928, 4160312724, 3682325739, 534901034, 2287240917, 2677201636, 71025852, 1171752314, 47956297, 2265969327, 2865804126, 1364027301, 2267528752, 1998395705, 576397983, 636085149, 3876141063, 1131266725, 3949079092, 1674557074, 2566739348, 3782985982, 2164386649, 550438955, 2491039847, 2409394861, 3757073140, 3509849961, 3972853470, 1377009785, 2164834118, 820549672, 2867309379, 1454756115, 94270429, 2974978638, 2915205038, 1887247447, 3641720023, 4292314015, 702694146, 1808155309, 95993403, 1529688311, 2883286160, 1410658736, 3225014055, 1903093988, 2049895643, 476880516, 3241604078, 3709326844, 2531992854, 265580822, 2920230147, 4294230868, 408106067, 3683123785, 1782150222, 3876124798, 3400886112, 1837386661, 664033147, 3948403539, 3572529266, 4084780068, 691101764, 1191456665, 3559651142, 709364116, 3999544719, 189208547, 3851247656, 69124994, 1685591380, 1312437435, 2316872331, 1466758250, 1979107610, 2611873442, 80372344, 1251839752, 2716578101, 176193185, 2142192370, 1179562050, 1290470544, 1957198791, 1435943450, 2989992875, 3703466909, 1302678442, 3343948619, 3762772165, 1438266632, 1761719790, 3668101852, 1283600006, 671544087, 1665876818, 3645433092, 3760380605, 3802664867, 1635015896, 1060356828, 1666255066, 2953295653, 2827859377, 386702151, 3372348076, 4248620909, 2259505262&#125;; // Amount of rand dwords - 1: #define NUM_DWORDS_MASK 0x7f unsigned int ror(unsigned int x, unsigned int i) &#123; // Rotate right a dword x by i bits. return (x >> i) | (x &lt;&lt; (WORD_SIZE - i)); &#125; unsigned int perm(unsigned int num, unsigned int x) &#123; // Permutation from dword to dword. // num is the permutation number. x is the input. unsigned int ror_index; for(unsigned int i=0; i&lt;NUM_ITERS; ++i) &#123; // Addition: x += RAND_DWORDS[(i + num + x) &amp; NUM_DWORDS_MASK]; // Rotation: ror_index = (x ^ RAND_DWORDS[(i + num + 1) &amp; NUM_DWORDS_MASK]) &amp; 0x1f; x = ror(x,ror_index); // Xor: x ^= RAND_DWORDS[(i + num + x) &amp; NUM_DWORDS_MASK]; // Rotation: ror_index = (x ^ RAND_DWORDS[(i + num + 1) &amp; NUM_DWORDS_MASK]) &amp; 0x1f; x = ror(x,ror_index); &#125; return x; &#125; ​ 由上可知，perm(i，x)就是第i个哈希函数hi(x)。计算完k个哈希函数后，我们得到了k个集合： \\[ h_{1}(x),h_{2}(x),h_{3}(x)\\cdots h_{k}(x) \\] ​ 然后每个集合中最小的数，就是我们所需的定长特征： \\[ sig(T) = \\{min_{t\\in T}h_{1}(T),min_{t\\in T}h_{2}(T),min_{t\\in T}h_{3}(T)\\cdots min_{t\\in T}h_{k}(T)\\} \\] ​ 故用\\(J(sig(A),sig(B))\\)代替\\(J(A,B)\\)即可大大简化计算，k的数量可以自己选定。对于完整的二进制文件，以下代码可以生成其对应特征： int sign( unsigned char* data, unsigned int len, unsigned int *result, unsigned int num_perms) &#123; // Find entry number &lt;num> of the signature of data. // len is the length of the data. // The result is inside &lt;result>, as an array of dwords. // We need at least 4 bytes to generate a signature. // We return -1 (error) if we don't have at least 4 bytes. if(len &lt; 4) &#123; return -1; &#125; unsigned int y; // Current integer value of 4 consecutive bytes. unsigned int py; // Permutation over y. unsigned int min_py; // Minimum py ever found. for(unsigned int permi=0; permi&lt;num_perms; ++permi) &#123; // Initialize y to be the first dword from the data: y = (unsigned int)data[0] &lt;&lt; 24; y += ((unsigned int)data[1]) &lt;&lt; 16; y += ((unsigned int)data[2]) &lt;&lt; 8; y += (unsigned int)data[3]; // Calculate first permutation: py = perm(permi,y); min_py = py; for(unsigned int i=4; i&lt;len; ++i) &#123; y &lt;&lt;= 8; y += data[i]; py = perm(permi,y); if(min_py > py) &#123; min_py = py; &#125; &#125; // Save minimum perm value found to memory: result[permi] = min_py; &#125; // Everything went well. // Result should be stored at &lt;result> return 0; &#125; ​ ​ 运行测试： ​ Catalog1共分为server和client两部分，client客户端作为IDA的一个插件，server服务器可以使用官方提供的testfcatalog.xorpd.net:1337，也可以自行搭建https://github.com/xorpd/fcatalog_server。 ​ 为了计算速度更快，这里使用C语言编写catalog相关计算操作，首先编译catalog1 ： ​ 【更新中】","categories":[{"name":"Reproduce","slug":"Reproduce","permalink":"http://example.com/categories/Reproduce/"}],"tags":[{"name":"Binary","slug":"Binary","permalink":"http://example.com/tags/Binary/"},{"name":"ML","slug":"ML","permalink":"http://example.com/tags/ML/"}],"author":"Shaw"},{"title":"Comparing One with Many — Solving Binary2source Function Matching Under  Function Inlining","slug":"Comparing-One-with-Many-—-Solving-Binary2source-Function-Matching-Under-Function-Inlining","date":"2023-01-10T07:27:48.471Z","updated":"2023-02-16T11:58:32.219Z","comments":true,"path":"2023/01/10/Comparing-One-with-Many-—-Solving-Binary2source-Function-Matching-Under-Function-Inlining/","link":"","permalink":"http://example.com/2023/01/10/Comparing-One-with-Many-%E2%80%94-Solving-Binary2source-Function-Matching-Under-Function-Inlining/","excerpt":"【论文阅读】Comparing One with Many — Solving Binary2source Function Matching Under Function Inlining 一、ABSTRACT 为了在函数内联下施行b2s函数匹配，我们提出了一个叫O2NMatcher的方法，通过其生成的Source function Sets (SFSs)作为匹配结果。 我们首先提出了一个模型EOOCJ48来预测内联的位置， 为了训练这个模型，我们利用可编译的开源软件（Open Source Software）生成一个带有标记的调用点（内联或不内联）的数据集，从调用点中提取几个特征，并通过检查不同编译之间的内联相关性来设计一个基于编译器-选项的多标签分类器。 然后，我们使用这个模型来预测不可编译的开源项目，得到带标记的函数调用图。接下来，我们将SFSs的构建视为一个子树生成问题，并设计根节点选择和边缘扩展规则来自动构建SFSs。最后，这些SFSs将被添加到源函数的语料库中，并与有内联的二进制函数进行比较。 我们对OSNMatcher进行了一些实验测试，结果表明我们的方法超过所有state-of-the-art，将结果提升了6%。","text":"【论文阅读】Comparing One with Many — Solving Binary2source Function Matching Under Function Inlining 一、ABSTRACT 为了在函数内联下施行b2s函数匹配，我们提出了一个叫O2NMatcher的方法，通过其生成的Source function Sets (SFSs)作为匹配结果。 我们首先提出了一个模型EOOCJ48来预测内联的位置， 为了训练这个模型，我们利用可编译的开源软件（Open Source Software）生成一个带有标记的调用点（内联或不内联）的数据集，从调用点中提取几个特征，并通过检查不同编译之间的内联相关性来设计一个基于编译器-选项的多标签分类器。 然后，我们使用这个模型来预测不可编译的开源项目，得到带标记的函数调用图。接下来，我们将SFSs的构建视为一个子树生成问题，并设计根节点选择和边缘扩展规则来自动构建SFSs。最后，这些SFSs将被添加到源函数的语料库中，并与有内联的二进制函数进行比较。 我们对OSNMatcher进行了一些实验测试，结果表明我们的方法超过所有state-of-the-art，将结果提升了6%。 二、背景知识 Function inlining：函数内联。使用inline关键字，编译器将函数调用语句替换为函数代码本身（称为扩展的过程），然后编译整个代码。因此，使用内联函数，编译器不必跳转到另一个位置来执行该函数，然后跳回。因为被调用函数的代码已经可用于调用程序。 Inlining Decisions in Visual Studio - C++ Team Blog (microsoft.com) Binary2source function matching，二进制到源码函数匹配： 1-to-1匹配：一个二进制函数与一个源码函数匹配； 1-to-n匹配：由于内联函数的存在，一个二进制函数匹配多个源码函数。 Stripped binary：不含调试符号信息的二进制可执行文件。 Multi-Label Classification (MLC) problem：多标签分类，一个样本有多个标签。 FCG ：Function Call Graph，函数调用图。在FCG图中，点是函数，边是调用关系。 Jaccard similaritie：又称为Jaccard index，用于比较有限样本集合之间的相似性和差异性。其值越大说明相似性越高。 Binary Relevance：核心思想是将多标签分类问题进行分解，将其转换为q个二元分类问题，其中每个二元分类器对应一个待预测的标签。 Classifier Chains：核心思想是将多标签分类问题进行分解，将其转换成为一个二元分类器链的形式，其中链后的二元分类器的构建式在前面分类器预测结果的基础上的。在模型构建的时候，首先将标签顺序进行shuffle打乱排序操作，然后按照从头到尾分别构建每个标签对应的模型。 Ensemble Method：集成学习算法。 三、提出问题 B2S匹配的作用： 当前软件开发基本都会使用公共开源库中的代码，这就造成一个问题，若公共开源项目OSS中存在漏洞会传播非常快（例如一个OpenSSL中的漏洞就可以造成互联网上17%web服务存在漏洞）。 由于以上问题的存在，检测软件中对OSS的依赖就很重要，Software Component Analysis (SCA)就是用来检测软件对OSS的依赖问题的。当商业软件公司发布了自己的二进制可执行程序时，SCA服务商就会将此二进制文件与OSS文件相比较，检测其包含使用了哪些OSS文件。 binary2source function matching就是其应用场景。 内联函数带来的问题： 举例如下图所示，当将二进制函数中的dtls1_get_record函数与源码中的dtls1_get_record函数比较时，使用CodeCMR给出的相似性还不足60%，这显然是匹配失败的。 如果深入去看二进制函数dtls1_get_record的汇编代码就可以发现，这个函数在编译的时候内联了dtls1_process_buffered_records,dtls1_get_bitmap 和 dtls1_record_replay_check的函数代码，极大的改变了其函数的内部代码内容，故影响了函数的匹配结果。 有上述例子可以看出，由于内联函数的存在会改变函数的代码内容，故直接对其进行匹配的结果是不尽如人意的。 解决这个问题面临的挑战： 想要解决在内联函数下的B2S问题，还需面对以下挑战： 1） OOD问题：Out-of-domain，待检测二进制函数可能是由多个函数生成的，故OSS源码资料库中可能没有对应的二进制匹配； 2） 有些二进制文件不含调试信息：对于许多不含调试信息的二进制文件，由于其没有标注内联点，故检测哪些函数是通过内联函数生成的和哪些源码函数被内联进了二进制函数中就更加困难； 3） 不同文件内联选择的不同：对于不同的编译设置导致的不同内联函数的选择（对某个函数，是否选择内联），很难覆盖所有情况。 四、模型方法 ​ 我们的方法O2NMatcher就是为了解决上述挑战而生的。 ​ 为了解决第一个挑战，我们生成了Source Function Sets (SFSs)来完善资料库，以此应对有内联函数的二进制函数查询匹配。 ​ 为了解决第二个挑战，我们使用可以编译的OSS项目训练了一个预测分类器来预测不可编译的OSS项目的内联位置。同时，我们提出了一个数据集自动化打标签的方法来生成数据。 ​ 为了解决第三个挑战，分类器被在多种不同的编译设置环境下训练，我们将其建模为多标签分类（MLC）问题，并根据不同编译设置下内联决策的相关性提出了基于编译器-选择的分类器。 ​ 通过给定的OSS生成SFSs，然后使用已有的“1-to-1”方法将这些SFSs与二进制函数相比较即可。 模型训练 1. 生成数据集 ​ 在生成数据集过程中，我们充分利用了编译过程生成的调试信息（用的都是可编译的OSS项目）。其会生成b2s行级别和函数级别的映射，并附带函数调用处的内联信息（是否内联）。 ​ 在开源项目编译时使用\"-g\"选项，可以使编译出的二进制文件附带从二进制地址到源文件某一行的映射信息（行级别映射），再利用Understand或者IDA Pro，tree-sitter，Chidra等工具，我们可以生成从二进制地址到源文件某一函数的映射信息（函数级别映射）。 ​ 如上图所示，圆圈是源码函数，方框是编译后的二进制函数。在（a）中，由于A只调用了C一次，故在编译后的二进制函数中C就被内联进了A中；但在（b）中，A调用了两次B，A和B都调用了C，无法很清楚的判断到底哪次调用需要被内联，故编译出的二进制函数也与编译选项有关，并不绝对。 ​ 上述例子可以看出，即使我们得到的源码函数之间的相互调用关系我们也无法确定哪些函数会被内联。 ​ 但如果我们知道其二进制调用关系图，反过来看，若二进制图中某个调用在源码调用图中也存在（如上图a中的A–&gt;B），则这个函数就不是内联函数，反之就是内联函数。这样，通过对调试信息的使用，可以成功的将可编译OSS项目的内联位置标注出来。 2. 特征提取 ​ 考虑到编译器一般通过评估内联的利弊来决定是否内联，我们总结了几个可以评估内联操作利弊的特征： ​ 如上表所示，特征共分为两大类Caller/Callee和Call Instruciton： Caller/Callee： caller这里指调用其他公共开源函数的函数，callee指被许多其他函数调用的函数。这部分特征分为函数内部语句，函数定义语句和函数调用次数。 Call Instruciton： 调用函数指令，这里关注其调用的位置和参数声明，例如在内联循环中的函数调用就可以显著减少调用次数。同理，如果调用含有常量参数，可以帮助减少内联函数的大小。 3. 模型训练 3.1 训练数据分析 ​ 我们这里提出了一个名为ECOCCJ48(Ensemble of Compiler Optimization based Classifier Chains built with J48)的MLC模型来预测内联函数位置。在多标签任务中，标签之间通常存在联系，下表展示了两个编译器（gcc-8.2.0和clang-7.0）在Ox优化下的内联措施的差异和联系： PS：opt1-opt2意味着使用优化措施opt1时内联，使用优化措施opt2时不内联的数量；opt1&amp;opt2意味着使用两个都内联的数量。 ​ 在同一编译器不同优化设置的条件下，由上图可以看出，94.78%的内联操作如果出现在低级优化措施的条件下，也会出现在更高级的优化措施条件下。所以在同一行内opt1&amp;opt2的数量都大于opt1-opt2。 ​ 在不同编译器的条件下，如下图所示，展示了不同优化措施下所做决策的Jaccard系数： ​ 在同一编译器家族（例如GCC的不同版本）比较时，其内联决策的相似性很高。当使用O0-O3优化措施测试gcc-6.4.0 和 gcc-7.2.0，87%的函数调用采用了相同的内联决策。在不同编译器的条件下，其内联决策没有明显的联系。 3.2 ECOCCJ48模型 ​ ECOOCJ48分为两个部分，使用binary relevance来预测不同编译器下的标签，使用整合分类器链来预测同一编译器下不同优化措施的标签。 3.3 不平衡的数据集 ​ 目前存在的数据集有以下问题： ​ 1.大多数的函数调用在被编译的过程中是不会被内联的，在使用OX优化时，大约只有20%的函数调用会被内联； ​ 2.不同优化措施编译时内联点的比例也不同。 ​ 这些差异导致了训练数据集的不平衡，训练会更偏向于正常的调用。我们使用了集成学习方法来处理这些不平衡，其使用随机选择的数据集来训练，通过整合不同基础分类器的结果来预测标签。因为不同基础分类器在不同的资料库中被训练，故他们可以抓住内联模式。 SFS生成 1. FCG构建与ICS预测 ​ 对于不可编译的OSS项目，首先使用一个解析器（parser）来提取代码中的函数调用点，构建FCG图。两个节点之间可以有多个有向边，边上附带函数调用的位置信息。 ​ 对于FCG中的每个调用点，我们从其调用者，被调用者和调用指令中提取特征，将其作为ECOCCJ48的输入，得到所有编译设置（不同编译器+优化措施）的标签。 2. SFS生成 ​ 如上图所示，（a）就是已经构建好的FCG图，其中红圈代表有内联函数的函数，黑圈则没有；红边代表内联操作，蓝边代表正常调用。假设FCG是一个有向无环图，SFS的生成就可以抽象成从FCG图中选择内联子树。这里将SFS的生成分为两个部分：根节点的选择（b）和边扩展。 ​ 根节点的选择上，有以下原则： ​ 1） 内联子树的根节点，如A,D； ​ 2）非内联子树的根节点，但是既内联调用了其他函数，也被其他函数普通调用，如C。 ​ 除去这两种情况，要么一个节点没有指向其他节点的红色边，要么在内联子树上仅仅被其它红边所指。第一种情况显然不能进入SFS，第二种情况会被内联进其调用者之中。 ​ 边扩展上，遵循以下规则： ​ 1） 对每个根节点，若其指向的节点仅有一条红边，则向下遍历其能遇到的所有节点（如对A，有A-&gt;C，A-&gt;C-&gt;E两条）； ​ 2）对每个根节点，若其指向的节点有两条相异颜色的边，则红色的边继续向下搜索生成，蓝色的边仅在下个点处停止。（如对C-&gt;E）。 ​ 若FCG成环，我们仅需注意，如果一个被调用节点已经在SFS中了，就不必再次遍历即可避免重复。 3. SFS汇总 ​ 得到函数的SFS后，我们将被调用函数的代码直接放进调用函数中。 测试评估 ​ 测试部分回答了以下问题： O2NMatcher可以提高现有1-to-1工作的表现吗？ 与Bingo 和 ASM2Vec比较，O2NMatcher生成的SFS有多准确？ 与现有的多标签分类工作相比，ECOCCJ48的表现如何？ O2NMatcher在训练，预测和生成SFS的时间花费如何？ 我们选取的特征集在ICS预测上有什么贡献？ 1. 研究背景 1.1 测试数据集 ​ 我们从GNU项目选择了51个包，使用9种编译器、4种优化措施编译成x86-64，得到8460个二进制文件和4294478个二进制函数。这些数据包含了诸如Coreutils、OpenSSL等在二进制相似性检测工作中广泛被使用的包。如下表所示： 1.2 Benchmark ​ 因为O2NMatcher是一个对其它b2s方法的补充，我们需要选择一个baseline。这里我们选择CodeCMR，一个现有工作表现最好的b2s方法。CodeCMR有一个开放工具BinaryAI，可以方便后续测试。 （Zeping Yu, Wenxin Zheng, Jiaqi Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2020. Codecmr: Cross-modal retrieval for function-level binary source code matching. Advances in Neural Information Processing Systems 33 (2020), 3872–3883.） 1.3 实验设置 ​ 90%随机选择的训练集以及10%测试集。由于相同编译器的内联选择比较相似，这里测试集仅用两个编译器生成：gcc-2.2.0和clang-7.0。测试集包含1.1中所有的编译器生成的数据。分类器在训练集上训练，然后对测试集生成SFSs。为了避免误差我们重复实验10次。 ​ 测试时，使用不带调试信息的stripped binary，并且OSS文件不能被编译。 1.4 评估标准 ​ 使用标准召回率recall@K来评估O2NMatcher准确率。（Recall@K召回率是指前topK结果中检索出的相关结果数和库中所有的相关结果数的比率，衡量的是检索系统的查全率。）在这里，recall@K代表前k个模型返回的源码函数中真正的源码函数占比。对于1-to-1匹配，真正的源码函数就是该待检测二进制函数的源码；对于我们的1-to-n匹配，在SFSs中，某个函数的根函数是真正的源码函数也可以算作阳性样本。在接下来的实验中我们会使用K = 1、10和50来测试。 ​ 使用SFS大小（SFS size）来评估O2NMatcher成本。SFS size是已生成的SFSs数量比上原函数集的数量。因为新增的SFSs增加了资料库的大小，故可能影响查询匹配的时间。 ​ 使用precision、recall和F1-score来评估ECOCCJ48准确率。 1.5 O2NMatcher的实现 ​ 在数据集标注工作上，我们使用Understand对源码做语法分析，使用IDA Pro来反汇编二进制文件。在FCG构建上，我们使用Understand来提取所有函数调用点，对源码生成FCG图，使用IDA Pro对二进制文件生成FCG图。在函数调用特征提取上，我们使用tree-sitter来提取函数体、函数定义和函数调用指令的特征，使用Understand提取函数调用次数特征。 ​ 在模型训练工作上，我们使用Python工具scikit-multilearn来实现ECOCCJ48模型和其他MCL方法。整个程序在Ubuntu 18.04、Intel Xeon Gold 6266C、1024GB DDR4 RAM、Nvidia RTX3090 GPU环境上运行。 2. 对问题一的回答：O2NMatcher的效果 3. 对问题二的回答：SFSs的效果 ​ 上表比较了O2NMatcher、Bingo和ASM2Vec生成的SFSs与标准答案的结果。 4. 对问题三的回答：ECOCCJ48的效果 5. 对问题四的回答：O2NMatcher的成本 6. 对问题五的回答：特征选择的作用 五、问题 该方法模型怎么结合已有的方法？ 原来怎么比现在就怎么比，只是从源码库变成了SFS。 SFSs怎么起作用？ 将可能内联的函数体放进原函数的新代码库。 不平衡的数据集怎么处理的？ “We use ensemble methods to handle the imbalanced dataset. The ensemble method trains the base classifiers on the randomly selected dataset and predicts the labels by ggregating the predictions from the base classifiers. As different base lassifiers are trained in different corpora, they can capture the inlining patterns of some rare labels”这部分没看懂什么意思 在汇总SFS时： “Directly, there are two ways to aggregate the SFS: conduct inlining for the source functions in the SFSs such as the inlining during compilation or directly aggregate the content of all the functions in the SFS”，这两种方式具体什么区别？","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Binary","slug":"Binary","permalink":"http://example.com/tags/Binary/"},{"name":"ML","slug":"ML","permalink":"http://example.com/tags/ML/"}],"author":"Shaw"},{"title":"How Machine Learning Is Solving the Binary Function Similarity Problem","slug":"How-Machine-Learning-Is-Solving-the-Binary-Function-Similarity-Problem","date":"2022-12-08T08:20:40.777Z","updated":"2023-02-15T07:06:22.738Z","comments":true,"path":"2022/12/08/How-Machine-Learning-Is-Solving-the-Binary-Function-Similarity-Problem/","link":"","permalink":"http://example.com/2022/12/08/How-Machine-Learning-Is-Solving-the-Binary-Function-Similarity-Problem/","excerpt":"【论文阅读】How Machine Learning Is Solving the Binary Function Similarity Problem 时间：2022.8 作者：Andrea Marcelli, Mariano Graziano , Xabier Ugarte-Pedrero, Yanick Fratantonio , Mohamad Mansouri and Davide Balzarotti(思科，欧洲电信学院) 会议：usenix 2022 ABSTRACT 人们会期望到现在为止，对于二进制相似性问题的研究，其有可能回答一些研究问题，这些问题超出了论文中提出的非常具体的技术，可以推广到整个研究领域。不幸的是，这个目标受到许多挑战，从可重复性问题到研究结果的不透明性，阻碍了研究有意义和有效地进展。 在本文，我们开始着手于对state-of-the-art做这个领域的第一个测试性研究。 首先，我们系统化了现存的研究成果； 我们选定了一些相关方法，这些方法代表了三个不同研究社区最近提出的各种解决方案； 针对现有的方案，我们重构了方法并构建了新的数据集，以便实现更为公平的对比效果。","text":"【论文阅读】How Machine Learning Is Solving the Binary Function Similarity Problem 时间：2022.8 作者：Andrea Marcelli, Mariano Graziano , Xabier Ugarte-Pedrero, Yanick Fratantonio , Mohamad Mansouri and Davide Balzarotti(思科，欧洲电信学院) 会议：usenix 2022 ABSTRACT 人们会期望到现在为止，对于二进制相似性问题的研究，其有可能回答一些研究问题，这些问题超出了论文中提出的非常具体的技术，可以推广到整个研究领域。不幸的是，这个目标受到许多挑战，从可重复性问题到研究结果的不透明性，阻碍了研究有意义和有效地进展。 在本文，我们开始着手于对state-of-the-art做这个领域的第一个测试性研究。 首先，我们系统化了现存的研究成果； 我们选定了一些相关方法，这些方法代表了三个不同研究社区最近提出的各种解决方案； 针对现有的方案，我们重构了方法并构建了新的数据集，以便实现更为公平的对比效果。 INTRODUCTION Binary function similarity：将一对函数的二进制表示作为输入，它们之间的相似性值作为输出。 Binary function similarity技术面临的挑战： 实际情况下，软件会被不同编译器、链接工具以及优化器编译； 特殊情况下，例如IoT，软件会被编译为不同架构的指令； Binary function similarity的作用： 在系统安全研究领域，许多研究问题需要以Binary function similarity作为核心构件。 节省逆向时间。在逆向工程中经常需要处理静态链接的stripped binaries，Binary function similarity可用于将一个未知的函数与先前生成的数据库中的（有标签的）函数进行匹配，对应到先前生成的数据库中，从而节省大量的时间； （stripped binaries：编译后的二进制文件可以包含调试信息，这些信息对于程序的执行是没有必要的，相反，它对于调试和发现程序中的问题或漏洞是很有用的。剥离的二进制文件（stripped binaries）是一个没有这些调试符号的二进制文件，因此体积较小，而且可能比没有剥离的二进制文件有更好的性能。剥离后的二进制文件很难被反汇编或逆向工程，这也使其难以发现程序中的问题或漏洞。） 用于高校检索第三方库中的漏洞函数。给定一个有问题的函数，Binary function similarity可用于在第三方库中快速检索相似函数； 用于软件分析以及恶意软件分类。 提出问题： 在使用相同评估标准、相同数据集的前提下，不同的方法如何比较？ 与简单的模糊哈希算法相比，原创的机器学习算法的主要贡献在哪？ 不同特征集的作用是什么？ 不同的方法在不同的任务中是否效果更好？ 相较于同一架构指令下，不同架构指令编译的二进制的比较是否更困难？ 是否有任何具体的研究方向看起来更有希望成为设计新技术的未来方向？ 很遗憾，我们发现目前的研究并不能回答上述问题。 现有研究存在的不足： 实验方法、实验结果不透明，难复现。实验所需的工具、超参数以及数据集通常都是不公开的，一些工作的横向比较太少甚至跟自己以前发的文章比； 现有工作的评估方法往往也是模糊不清的。不同的论文中，任务目标、环境设置、概念、颗粒度、数据集的大小和特性以及评估方法往往都不同，因此，即使是论文中最基础的图像也基本不能直接比较。所以，当新的工作声称比旧工作表现更好时，这个优越性是否仅仅是针对某个特殊的领域不得而知。更糟糕的是，一些工作函数对的选择方法以及训练集的构成并没有详细说明，使得复现变得更加困难。一个方法结果更优秀的原因是因为其原创性还是其他因素不得而知； 领域整体较为混乱。每个新的解决方案都采用了更复杂的技术，或多种技术的新组合。新方法的成功很难确定是由较简单的方法的实际局限性导致的，还是其创新性。现有几十种方法存在，但是并不能确定哪些方法在哪些环境设置下是有效的，而哪些不行。 “每个人都声称他们的方法最好。” 本文贡献： ​ 实现了在本领域第一个系统化的评估。在相同的工具框架下复现了10个有代表性的方法（及其变种方法），并使用新定义的同一数据集测试。 一些结论： 简单的方法（例如模糊哈希）在简单的环境下表现很好，但难以应对复杂的环境； GNN几乎在所有任务里都取得了最好的成绩； 许多最近发表的论文在同一数据集上测试时，都有非常相似的准确性，尽管他们之中的一些声称自己有了提高。 Measuring Function Similarity（领域基础知识） 1. 函数相似度评估方法 1.1 直接评估（Direct Comparsion）： 用机器学习/深度学习模型做相似性函数，通过对模型的训练来使其输入两个函数，输出两个函数的相似性。 1.1 间接评估（Indirect Comparsion）： 间接评估将输入函数的特征压缩到低维度表示，这些表示可以使用简单的距离测量方法评估相似度，例如欧氏距离或余弦距离。 Fuzzy hashes：一个很典型的低维度特征表示方法就是哈希模糊。特殊的fuzzy hashes，如ssdeep，用于判断两个文件是否近似的hash。如果一个文件比另一个文件多一个空格，普通的hash是会完全不同，而模糊hash 可能会很相似或者完全一样。 Code embedding：代码嵌入，应用NLP的方法，将汇编看做文本，对每个代码块或者指令做embedding。 Graph embedding： 图嵌入，使用代码图结构，通过传统方法或者GNN来生成embedding。 2. 函数表示方法 直接使用raw bytes； 使用通过raw bytes反编译得到的汇编代码； Normalized assembly，将汇编代码中的常数等替换，减轻操作数与操作之间的联系； Intermediate representations，中间表示； 图结构，例如CFG，DFG等； 动态分析，通过运行中的函数来比较； Symbolic execution，通过符号执行分析； 筛选现有方法 尽管现有数百篇的论文，但其中的很多方法都是对某个技术做微调，故该领域的原创方法并不是很多。 1. 筛选标准 Scalability and real-world applicability（可扩展性和现实世界的适用性）：不关注天生速度较慢的方法，只关注诸如那些基于动态分析、符号执行或高复杂度图相关算法的方法； Focus on representative approaches and not on specifific papers：只关注有代表性的方法； Cover different communities：在我们的评估中，我们希望包括来自系统安全、编程语言分析和机器学习社区的代表性研究。为了完整性，我们还考虑了行业提出的方法； Prioritize latest trends：考虑最新的，尤其是AI方法。 2. 筛选方法 上图选定了30个方法，接着我们会从这30个方法中筛选出10个最有代表性的方法来测试。 该图的左侧对不同研究机构提出的方法进行了分类，由学术界和工业界（腾讯、谷歌），箭头代表了指向的一方拿被指向一方的结果进行了比较； 该图的右侧Y轴代表出版时间，X轴代表函数的输入形式，不同的色块代表不同的相似度计算方式； 中括号[]代表的tag标注了不同的研究群体，例如S代表Security，SE代表Software eng。 从该图中可以得出： 左图中间的二进制比较工具都是为直接比较两个完整二进制文件而设计的（例如，它们使用图结构），并且它们都是针对单指令集结构的。但是一些做了跨指令集和函数比较的paper仍拿这些方法来比较，显然会得出不正确的结论； 从左图可以得出，不同研究机构之间很少跨领域比较； 从右图可以看出，随着时间推移，算法的复杂性以及AI算法的使用比例都在增加。 3. 筛选结果 方法类型 方法/作者名称 Bytes fuzzy hashing Catalog1 CFG fuzzy hashing FunctionSimSearch Attributed CFG and GNN Gemini Attributed CFG, GNN, and GMN Li et al. 2019 IR, data flflow analysis and neural network Zeek Assembly code embedding Asm2Vec Assembly code embedding and self-attentive encoder SAFE Assembly code embedding, CFG and GNN Massarelli et al.2019 CodeCMR BinaryAI hierarchical transformer and micro-traces Trex 测试 1. 复现方法 复现的几个阶段：binary analysis，the feature extraction，the machine-learning implementations. 二进制分析以及特征提取使用的是IDA Pro 7.3以及其Python接口。 有关我们所有实现的其他技术细节，以及有关我们努力联系相应作者的信息以及有关使用预训练模型的注意事项，请参见： How Machine Learning Is Solving the Binary Function Similarity Problem — Artifacts and Additional Technical Details. https://github.com/Cisco-Talos/binary_function_similarity. 2. 数据集 为了与现实世界的复杂性与变数相匹配，我们创建了两个新的数据集Dataset-1和Dataset-2，其包含了二进制检测领域的问题： ​ 多种编译器和版本； ​ 多种编译优化器； ​ 多种指令集架构和bitness(32位还是64位)； ​ 不同种类的代码（命令行 or GUI）； Dataset-1用于训练，Dataset-1和Dataset-2用于测试。 3. 实验环境设置 3.1 定义6个检测任务 序号 任务名称 作用 1 XO 待测函数对仅优化器不同，编译器，编译器版本、指令集架构和bitness都相同 2 XC 待测函数对优化器，编译器和编译器版本不同，指令集架构和bitness相同 3 XC+XB 待测函数对优化器，编译器和编译器版本以及bitness不同，指令集架构相同 4 XA 待测函数对指令集架构和bitness不同，其他相同 5 XA+XO 待测函数对指令集架构、bitness和优化器不用，其他相同 6 XM 待测函数对所有性质都可能不同 3.2 测试方法 每种方法的测试包含两个过程： AUC和ROC图； MRR和Recall@K。 4. 模糊哈希方法比较结果 比较了两个方法：Catalog1和FunctionSimSearch。 4.1 首先每次只变动一个变量： B：Row Bytes；G：graphlets，连通图结构； M： mnemonics；符号 I：immediates 。 由图可知，在单指令集架构的条件下即使简单的row bytes输入也能取得较好效果，在多指令集架构下代码图结构明显更好。 4.2 测试3.1中的6个任务 可以看到： 当面对多变量不同的任务时简单的方法开始变得不奏效了，同时FSS在面对大函数时可能由于其图结构的输入也更为有效； 同时，GMI的三个特征引入并没有比最基础的G配置输入更为有效； Catalog1速度更快，因为FSS的特征提取更耗时。 5. 机器学习方法比较结果 结论： GMN几乎在所有的测试下表现都最好，而且运行时间也很短； 语言模型相关的方法在同一指令集架构下的表现都很不错，transformer在跨指令集架构的表现也不错； 由Li等人提出的GNN变式相较于GNN（s2v）的表现有明显提升； GNN（s2v）相关的实验表明更复杂的特征提取不一定更有效，且使用instruction embedding也不一定效果更好； 相较于GNN，使用了编码器的SAFE在小函数上的表现更好，但其存在OOV问题； 6. 漏洞发掘的样例 问题 测试的时候使用的Ranking measures（MRR10与Recall@K）的作用？","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Binary","slug":"Binary","permalink":"http://example.com/tags/Binary/"},{"name":"ML","slug":"ML","permalink":"http://example.com/tags/ML/"}],"author":"Shaw"},{"title":"强化学习之ConnectX四子棋游戏","slug":"Connect X","date":"2022-12-01T14:11:52.779Z","updated":"2022-12-01T14:10:34.196Z","comments":true,"path":"2022/12/01/Connect X/","link":"","permalink":"http://example.com/2022/12/01/Connect%20X/","excerpt":"Connect X Time: 2022.11.14 Author: Shaw Connect X | Kaggle ConnectX Getting Started | Kaggle 一、任务简介： 任务：强化学习任务，类似五子棋的规则（但每一步只能下某一列的最底端的空位），每人一步，先于对手横竖或斜角连成4个即可获胜。","text":"Connect X Time: 2022.11.14 Author: Shaw Connect X | Kaggle ConnectX Getting Started | Kaggle 一、任务简介： 任务：强化学习任务，类似五子棋的规则（但每一步只能下某一列的最底端的空位），每人一步，先于对手横竖或斜角连成4个即可获胜。 评估方法： 每个提交给Kaggle的结果（一个py文件，包含了agent如何下棋的规则）在跟自己下一次棋，证明其工作正常后，会被赋予一个skill等级。 相近skill等级的提交结果之间会进行持续不断下棋PK。 每次PK结束后就会更新双方的等级，赢加输减。 二、环境准备 安装kaggle相关的强化学习环境： pip install kaggle-environments 创建Connect X环境： from kaggle_environments import evaluate, make, utils env = make(\"connectx\",debug=True) #创建connectx环境 env.render() #以图形化的形式显示当前环境 创建Submission提交函数： import inspect import os def write_agent_to_file(function, file): with open(file, \"a\" if os.path.exists(file) else \"w\") as f: f.write(inspect.getsource(function)) print(function, \"written to\", file) write_agent_to_file(my_agent, \"submission.py\") 三、Q-learning 对于简单的下棋问题，这里选择Q-learning算法进行学习。 创建connectX类： class ConnectX(gym.Env): def __init__(self, switch_prob=0.5): self.env = make('connectx', debug=True) self.pair = [None, 'negamax'] self.trainer = self.env.train(self.pair) self.switch_prob = switch_prob # Define required gym fields (examples): config = self.env.configuration self.action_space = gym.spaces.Discrete(config.columns) self.observation_space = gym.spaces.Discrete(config.columns * config.rows) def switch_trainer(self): self.pair = self.pair[::-1] self.trainer = self.env.train(self.pair) def step(self, action): return self.trainer.step(action) def reset(self): # 有switch_prob的几率更换先手顺序 if random.uniform(0, 1) &lt; self.switch_prob: self.switch_trainer() return self.trainer.reset() def render(self, **kwargs): return self.env.render(**kwargs) 创建Q表，由于棋盘状态较多，这里使用动态Q表（shape = （n，7））： class QTable: def __init__(self, action_space): self.table = dict() self.action_space = action_space def add_item(self, state_key): self.table[state_key] = list(np.zeros(self.action_space.n)) def __call__(self, state): board = state['board'][:] # 复制一份 board.append(state.mark) # 加入mark标志着先手还是后手 state_key = np.array(board).astype(str) state_key = hex(int(''.join(state_key), 3))[2:]# 转为16进制编码，去掉前缀 if state_key not in self.table.keys(): self.add_item(state_key) return self.table[state_key] 定义相关超参数： alpha = 0.1 # 学习率 gamma = 0.6 # discount factor γ epsilon = 0.99 # ε-greedy策略的ε min_epsilon = 0.1 # 最小ε episodes = 10000 # 采样轮数 alpha_decay_step = 1000 alpha_decay_rate = 0.9 # α衰减率 epsilon_decay_rate = 0.9999 # ε衰减率 定义训练过程： q_table = QTable(env.action_space) all_epochs = [] all_total_rewards = [] all_avg_rewards = [] # Last 100 steps all_qtable_rows = [] all_epsilons = [] for i in tqdm(range(episodes)): state = env.reset() # 清空棋盘 epochs,total_rewards = 0, 0 epsilon = max(min_epsilon,epsilon*epsilon_decay_rate) # ε每轮衰减 done = False while not done : # 开始一轮采样 # 某列不能下的情况 == 此列的第一个位置有棋子 == (state.board[c] == 0) space_list = [c for c in range(env.action_space.n) if state['board'][c] == 0] if random.uniform(0,1) &lt;= epsilon :# ε-greedy-->选择随机策略 action = choice(space_list) else : # ε-greedy-->选择贪心策略 row = np.array(q_table(state)[:]) row[[c for c in range(env.action_space.n) if state['board'][c] != 0]] = -1 action = int(np.argmax(row)) next_state,reward,done,info = env.step(action) if done: if reward == 1: # Won reward = 20 elif reward == 0: # Lost reward = -20 else: reward = 1 else: reward = -0.01 old_value = q_table(state)[action] next_max = np.max(q_table(next_state)) # Q-Learning 更新 new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) q_table(state)[action] = new_value state = next_state epochs += 1 total_rewards += reward all_epochs.append(epochs) all_total_rewards.append(total_rewards) avg_rewards = np.mean(all_total_rewards[max(0, i-100):(i+1)]) all_avg_rewards.append(avg_rewards) all_qtable_rows.append(len(q_table.table)) all_epsilons.append(epsilon) if (i+1) % alpha_decay_step == 0: alpha *= alpha_decay_rate 根据Q表生成Agent： my_agent = '''def my_agent(observation, configuration): from random import choice q_table = ''' \\ + str(dict_q_table).replace(' ', '') \\ + ''' board = observation.board[:] board.append(observation.mark) state_key = list(map(str, board)) state_key = hex(int(''.join(state_key), 3))[2:] if state_key not in q_table.keys(): return choice([c for c in range(configuration.columns) if observation.board[c] == 0]) action = q_table[state_key] if observation.board[action] != 0: return choice([c for c in range(configuration.columns) if observation.board[c] == 0]) return action ''' with open('submission.py', 'w') as f: f.write(my_agent) 上交到Kaggle后经过一晚上的博弈，分数很低，直接倒数： 尝试评估其效果： from submission import my_agent def mean_reward(rewards): win = sum(1 if r[0]>0 else 0 for r in rewards) loss = sum(1 if r[1]>0 else 0 for r in rewards) draw = sum(1 if r[0] == r[1] else 0 for r in rewards) return '&#123;0&#125; episodes- won: &#123;1&#125; | loss: &#123;2&#125; | draw: &#123;3&#125; | winning rate: &#123;4&#125;%'.format( len(rewards), win, loss, draw, (win / len(rewards))*100 ) # Run multiple episodes to estimate agent's performance. print(\"My Agent vs Random Agent:\", mean_reward( evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=100))) print(\"My Agent vs Negamax Agent:\", mean_reward( evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=100))) 四、DQN 在尝试了简单的强化学习算法后，这里将深度学习与强化学习结合起来，用DQN进行训练： DQN使用了神经网络来代替Q表，使用函数替代表格，以此解决Q表过大的问题： class ConnectX(gym.Env): def __init__(self, switch_prob=0.5): self.env = make('connectx', debug=False) self.pair = [None, 'random'] self.trainer = self.env.train(self.pair) self.switch_prob = switch_prob # Define required gym fields (examples): config = self.env.configuration self.action_space = gym.spaces.Discrete(config.columns) self.observation_space = gym.spaces.Discrete(config.columns * config.rows) def switch_trainer(self): self.pair = self.pair[::-1] self.trainer = self.env.train(self.pair) def step(self, action): return self.trainer.step(action) def reset(self): if np.random.random() &lt; self.switch_prob: self.switch_trainer() return self.trainer.reset() def render(self, **kwargs): return self.env.render(**kwargs) class DeepModel(torch.nn.Module): def __init__(self,num_states,hidden_units,num_actions): super(DeepModel,self).__init__() self.hidden_layers = nn.ModuleList([]) for i in range(len(hidden_units)): if i == 0: self.hidden_layers.append(nn.Linear(num_states,hidden_units[i])) else : self.hidden_layers.append(nn.Linear(hidden_units[i-1],hidden_units[i])) self.output_layers = nn.Linear(hidden_units[-1],num_actions) def forward(self,x): for layer in self.hidden_layers: x = torch.sigmoid(layer(x)) x = self.output_layers(x) return x class DQN: def __init__(self,num_states,num_actions,hidden_units,gamma,max_experiences,min_experiences,batch_size,lr): self.num_actions = num_actions self.batch_size = batch_size self.gamma = gamma self.model = DeepModel(num_states,hidden_units,num_actions) self.optimizer = optim.Adam(self.model.parameters(), lr = lr) self.criterion = nn.MSELoss() self.experience = &#123;'s':[], 'a':[], 'r':[], 's2':[], 'done':[] &#125; self.max_experiences = max_experiences self.min_experiences = min_experiences def preprocess(self, state): result = state.board[:] result.append(state.mark) return result def predict(self,inputs): return self.model(torch.from_numpy(inputs).float()) def train(self,TargetNet): if len(self.experience['s']) &lt; self.min_experiences: return 0 ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size) states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids]) actions = np.asarray([self.experience['a'][i] for i in ids]) rewards = np.asarray([self.experience['r'][i] for i in ids]) states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids]) dones = np.asarray([self.experience['done'][i] for i in ids]) value_next = np.max(TargetNet.predict(states_next).detach().numpy(), axis=1) actual_values = np.where(dones, rewards, rewards+self.gamma*value_next) actions = np.expand_dims(actions, axis=1) actions_one_hot = torch.FloatTensor(self.batch_size, self.num_actions).zero_() actions_one_hot = actions_one_hot.scatter_(1, torch.LongTensor(actions), 1) selected_action_values = torch.sum(self.predict(states) * actions_one_hot, dim=1) actual_values = torch.FloatTensor(actual_values) self.optimizer.zero_grad() loss = self.criterion(selected_action_values, actual_values) loss.backward() self.optimizer.step() def get_action(self,state,epsilon): if np.random.random() &lt; epsilon: return int(np.random.choice([c for c in range(self.num_actions) if state.board[c] == 0])) else : prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].detach().numpy() for i in range(self.num_actions): if state.board[i] != 0: prediction[i] = -1e7 return int(np.argmax(prediction)) def add_experience(self, exp): if len(self.experience['s']) >= self.max_experiences: for key in self.experience.keys(): self.experience[key].pop(0) for key, value in exp.items(): self.experience[key].append(value) def copy_weights(self, TrainNet): self.model.load_state_dict(TrainNet.state_dict()) def save_weights(self, path): torch.save(self.model.state_dict(), path) def load_weights(self, path): self.model.load_state_dict(torch.load(path)) 训练时，将采样的s,r,a,s'存储下来，等到积累到一定数量后再以batch的形式输入神经网路（这里是全连接层），训练，测试。 通过这种方式，可以极大的增加训练的轮数，这里尝试十万轮（训练时间四小时）： 五、RollOut RollOut算法是典型的决策时规划算法，其思路是： 对当前状态S的所有可能取值&lt;s,a'&gt;，模拟计算若干次，取得每个&lt;s,a'&gt;的平均Reward； 选取Reward最大的动作A； 模拟计算时使用的策略称之为rollout策略，这里直接采用随机策略。 使用如下类进行蒙特卡洛采样： class MonteCarlo: def __init__(self, env, gamma, episodes): ''' env是当前需要进行模拟采样的状态 gamma用于计算Reward episodes是每个(s,a)的采样次数 ''' self.state = deepcopy(env) self.all_rewards = [0, 0, 0, 0, 0, 0, 0] self.gamma = gamma self.episodes = episodes def rollout(self, env): # 对当前状态进行rollout采样 mean_reward = 0 ini_state = env.env.state[0]['observation'] for i in range(self.episodes): r = 0 gamma = 1 env.set_state(ini_state) state = deepcopy(ini_state) done = False while not done: space_list = [c for c in range(7) if state['board'][c] == 0] action = choice(space_list) next_state, reward, done, info = env.step(action) if done: if reward == 1: r += 20*gamma elif reward == 0: r -= 20*gamma else: r += 1*gamma mean_reward += r else: r -= 0.05*gamma gamma *= self.gamma state = next_state return mean_reward/self.episodes 因为MC采样需要在采样后将环境状态恢复，故在原本的ConnectX类中添加设置状态方法，并删除swicth_trainer： class ConnectX(gym.Env): def __init__(self, switch_prob=0.5): self.env = make('connectx', debug=True) self.pair = [None, 'random'] self.trainer = self.env.train(self.pair) self.switch_prob = switch_prob config = self.env.configuration self.action_space = gym.spaces.Discrete(config.columns) self.observation_space = gym.spaces.Discrete( config.columns * config.rows) def step(self, action): return self.trainer.step(action) def set_state(self, init_state): self.env.reset() self.env.state[0]['observation'] = deepcopy(init_state) def reset(self): return self.trainer.reset() def render(self, **kwargs): return self.env.render(**kwargs) 调试时设置play函数： def play(num,env,gamma,episodes): result = &#123;'win':0,'loss':0,'draw':0&#125; for i in range(num): #print('[GAME&#123;&#125;]:'.format(i)) done = False state = env.reset() while not done: ini_state = env.env.state[0]['observation'] space_list = [c for c in range(7) if ini_state['board'][c] == 0] R = [] for action in space_list: # rollout模拟采样 env.set_state(ini_state) next_state, reward, done, info = env.step(action) mc = MonteCarlo(env, gamma, episodes) reward = reward + gamma*mc.rollout(env) R.append(reward) Action = int(np.argmax(R)) # 根据采样结果选择动作 env.set_state(ini_state) next_state, reward, done, info = env.step(Action) #print('R = ', R) #print('Action = ', Action) #print(env.render(mode=\"ansi\")) if done: if reward == 1: # Won result['win'] += 1 #print('you win!') elif reward == 0: # Lost result['loss'] += 1 #print('you loss!') else: result['draw'] += 1 #print('draw!') print('My Agent vs Random Agent:', '&#123;0&#125; episodes- won: &#123;1&#125; | loss: &#123;2&#125; | draw: &#123;3&#125; | winning rate: &#123;4&#125;%'.format( num, result['win'], result['loss'], result['draw'], (result['win'] / num)*100 )) 六、问题总结 1. 训练时间过长 以Q-Learning为例，一个10000个episodes的训练要耗时2小时+，对于一个简单的四子棋过于耗时。 尝试： 尝试在python文件中而不是jupter中训练：训练总时间减少了约2/5，有一定效果； 使用DQN时训练速度明显大于Q-leraning（训练十万轮耗时3小时40分钟），猜想可能是神经网络可以使用GPU加速，‘查表’速度更快； 2. Q-Learning训练效果不佳 在经过10000个episode训练后，Q-Learning的表现如下： 通过如下的代码，你可以跟自己的agent下一局，可以发现，经过一万轮训练后的Agent棋力很一般，新手人类也能轻松胜利： while not done: sys.stdout.flush() print(env.render(mode=\"ansi\")) action = int(input('Input your action:(0-6)')) next_state, reward, done, info = env.step(action) if done: if reward == 1: # Won print('you win!') elif reward == 0: # Lost print('you loss!') else: print('draw!') 观察训练时平均Reward、Q表长度和ε的变化： 可以发现，在10000轮episode的训练中，平均奖励始终在围绕0.8上下波动，Q表长度一直在平稳增加，这说明直到训练结束Agent仍有大量未见过的state进入Q表，模型训练轮数不足。 实际上 Connect 4有四百万兆不同的状态，Q-learning显然在有限的时间空间下是取得不了什么有效学习的。 3. DQN的不足 即使使用DQN进行了十万轮的训练，耗时近4小时，DQN的结果仍不理想： 其实不难理解，对于四百万兆个state的棋盘，十万轮也不过是\\[1/10^{13}\\]，仍过于渺小。 并且，Q-learning以及DQN都是background planning，其算法倾向于将所有可能的状态的最佳策略都计算出来。实际上，在下棋的时候几乎绝大多数状态都不会出现第二次，但相似的棋谱格局却会经常出现。 显然，相较于background planning，decision-time planning（决策时规划）算法是个更合适的算法。 decision-time planning在遇到每个新的状态St后再开始一个完整的规划过程，其为每个当前状态选择一个动作At，到下一个状态St+1就选择一个动作At+1，以此类推。基于当前状态所生成的values和policy会在做完动作选择决策后被丢弃，在很多应用场景中这么做并不算一个很大的损失，因为有非常多的状态存在，且不太可能在短时间内回到同一个状态，故重复计算导致的资源浪费会很少。 4. 随机Rollout 这里尝试使用简单的RollOut算法后发现，即使在rollout算法中使用最简单的随机rollout策略，并且每轮模拟仅仅采样1次，所取得的的结果就比训练了4个小时的DQN好很多。 在模拟采样轮数为1（对每个&lt;s,a&gt;键值对只采样1次）、gamma为1的条件下，与Random和Negmax分别下100局的胜率： 模型 VS Random VS Negmax Q-Learning(一万轮) 61% 3% DQN(十万轮) 70% 6% 随机Rollout 94% 95%平局 但Rollout算法的问题也很明显，其应用的过程就是‘训练’的过程，每次需要等模拟采样完成后再选择，故其反应时间会比DQN长很多。 时间 DQN 随机Rollout 训练所需时间 4h 0 下100局所需时间 1m 40m 但即使这样，我们也能看出，决策时规划比后台规划算法更适合棋类场景，由于棋类几乎无限的状态数量，决策时规划虽然反应更慢，但结果也更为合理有效。 5. 展望 本次的小比赛从最简单的Q-Learning算法入手，到引入了神经网络的DQN，最后从后台规划引入到决策时规划，并实现了一个简单的Rollout算法。 首先，Q-learning以及DQN这类后台规划算法无法有效处理状态过多的环境。Q-learning在时间以及空间上都存在溢出问题，DQN虽然引入了深度神经网络来替代Q表，解决了空间不足的问题，但由于其训练速度没有质的改变，训练时间仍不可估量的长。 决策时规划面对状态过多的问题有明显提升。即使在rollout算法中使用最简单的随机rollout策略，并且每轮模拟仅仅采样1次，所取得的的结果就比训练了4个小时的DQN好很多。 除了使用简单的随机Rollout算法，这里可以替换rollout策略来进一步提升结果，减少rollout的反应时间。以及，可以使用MCTS，蒙特卡洛树搜索的方法再进一步提升结果（kaggle中已有Notebook，且分数不错），这里篇幅以及时间有限，仅做展望。","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://example.com/categories/Kaggle/"}],"tags":[{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"RL","slug":"RL","permalink":"http://example.com/tags/RL/"}],"author":"Shaw"},{"title":"Facial Keypoints Detection","slug":"Facial Keypoints Detection","date":"2022-11-14T03:24:28.500Z","updated":"2022-11-14T03:24:02.826Z","comments":true,"path":"2022/11/14/Facial Keypoints Detection/","link":"","permalink":"http://example.com/2022/11/14/Facial%20Keypoints%20Detection/","excerpt":"Facial Keypoints Detection Time: 2022.11.11 Author: Shaw Facial Keypoints Detection | Kaggle 一、任务简介： ​ 任务：正确标注所给图片的人脸关键位置；","text":"Facial Keypoints Detection Time: 2022.11.11 Author: Shaw Facial Keypoints Detection | Kaggle 一、任务简介： ​ 任务：正确标注所给图片的人脸关键位置； ​ 关键位置（keypoint）： 字段 解释 left_eye_center 左眼中心 right_eye_center 右眼中心 left_eye_inner_corner 左眼内眼角 left_eye_outer_corner 左眼外眼角 right_eye_inner_corner 右眼内眼角 right_eye_outer_corner 右眼外眼角 left_eyebrow_inner_end 左眉内侧 left_eyebrow_outer_end 左眉外侧 right_eyebrow_inner_end 右眉内侧 right_eyebrow_outer_end 右眉外侧 nose_tip 鼻尖 mouth_left_corner 嘴左角 mouth_right_corner 嘴右角 mouth_center_top_lip 上唇中心 mouth_center_bottom_lip 下唇中心 ​ 数据组成： training.csv: 包含7049张训练图片，每张图片有15个keypoint坐标（每个坐标用x,y两列数据表示，有些数值缺失），图像数据作为按行排序的像素列表。故表格中的数据标签有30列，图片数据一列共31列； test.csv: 包含1783张测试图片； ubmissionFileFormat.csv: 待提交的测试结果。 样例： 二、数据分析处理 首先加载训练数据与测试数据： #Load the data train_data = pd.read_csv(TRAIN_PATH) test_data = pd.read_csv(TEST_PATH) print(train_data.columns) print(test_data.columns) 2.1 空值填补 因为数据中存在缺失值，故统计分析存在缺失值的列： train_data.isnull().any().value_counts() #out: ''' True 28 False 3 dtype: int64 ''' 故28个属性列都存在缺失值，接下来填补缺失，‘ffill’方法表示用前面一列的值填补当前位置的值： train_data.fillna(method = 'ffill',inplace = True) 同时，Image列中的部分数值由‘ ’空格替代，这里将其替换为0： train_data['Image_new'] = train_data['Image'].map(lambda x:['0' if i == ' ' else i for i in x.split(' ')]) image_list = np.array(train_data['Image_new'].to_list(),dtype='float') 2.2 相关图片处理函数编写 IMG_SIZE = 96 # image size 96 x 96 pixels def show_keypoints(image, keypoints): ''' Show image with keypoints Args: image (array-like or PIL image): The image data. (M, N) keypoints (array-like): The keypoits data. (N, 2) ''' plt.imshow(image, cmap='gray') if len(keypoints): plt.scatter(keypoints[:, 0], keypoints[:, 1], s=24, marker='.', c='r') def show_images(df, indxs, ncols=5, figsize=(15,10), with_keypoints=True): ''' Show images with keypoints in grids Args: df (DataFrame): data (M x N) idxs (iterators): list, Range, Indexes ncols (integer): number of columns (images by rows) figsize (float, float): width, height in inches with_keypoints (boolean): True if show image with keypoints ''' plt.figure(figsize=figsize) nrows = len(indxs) // ncols + 1 for i, idx in enumerate(indxs): image = np.fromstring(df.loc[idx, 'Image'], sep=' ').astype(np.float32)\\ .reshape(-1, IMG_SIZE) if with_keypoints: keypoints = df.loc[idx].drop('Image').values.astype(np.float32)\\ .reshape(-1, 2) else: keypoints = [] plt.subplot(nrows, ncols, i + 1) plt.title(f'Sample #&#123;idx&#125;') plt.axis('off') #关闭坐标轴 plt.tight_layout() # tight_layout会自动调整子图参数，使之填充整个图像区域 show_keypoints(image, keypoints) plt.show() 2.3 分割数据集 使用Dataset和DataLoader相关pytorch类来加载数据集。 定义数据集子类用于自动处理dataframe数据，返回{'image': image, 'keypoints': keypoints}类型的数据： class FaceKeypointsDataset(Dataset): '''Face Keypoints Dataset''' def __init__(self, dataframe, train=True, transform=None): ''' Args: dataframe (DataFrame): data in pandas dataframe format. train (Boolean) : True for train data with keypoints, default is True transform (callable, optional): Optional transform to be applied on sample ''' self.dataframe = dataframe self.train = train self.transform = transform def __len__(self): return len(self.dataframe) def __getitem__(self, idx): image = np.fromstring(self.dataframe.iloc[idx, -1], sep=' ')\\ .astype(np.float32).reshape(-1, IMG_SIZE) if self.train: keypoints = self.dataframe.iloc[idx, :-1].values.astype(np.float32) else: keypoints = None sample = &#123;'image': image, 'keypoints': keypoints&#125; if self.transform: sample = self.transform(sample) return sample 将数据正则化，转化为tensor： class Normalize(object): '''Normalize input images''' def __call__(self, sample): image, keypoints = sample['image'], sample['keypoints'] return &#123;'image': image / 255., # scale to [0, 1] 'keypoints': keypoints&#125; class ToTensor(object): '''Convert ndarrays in sample to Tensors.''' def __call__(self, sample): image, keypoints = sample['image'], sample['keypoints'] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.reshape(1, IMG_SIZE, IMG_SIZE) image = torch.from_numpy(image) if keypoints is not None: keypoints = torch.from_numpy(keypoints) return &#123;'image': image, 'keypoints': keypoints&#125; else: return &#123;'image': image&#125; 利用SubsetRandomSampler构建采样器，并使用Dataloader构建数据加载器： from torch.utils.data.sampler import SubsetRandomSampler def prepare_train_valid_loaders(trainset,valid_size = 0.2,batch_size = 128): # obtain training indices that will be used for validation # 这里保障了训练集和测试集的随机划分 num_train = len(trainset) indices = list(range(num_train)) np.random.shuffle(indices) split = int(np.floot(valid_size*num_train)) train_idx, valid_idx = indices[split:], indices[:split] # define samplers for obtaining training and validation batches # 这里保障了一轮epoch中每次生成的batch是随机的，避免模型学习到数据的顺序特征 train_sampler = SubsetRandomSampler(train_idx) valid_sampler = SubsetRandomSampler(valid_idx) # prepare data loaders train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler) valid_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler) return train_loader, valid_loader 正式加载数据： from torchvision import transforms # how many samples per batch to load batch_size = 128 # percentage of training set to use as validation valid_size = 0.2 # Define a transform to normalize the data tsfm = transforms.Compose([Normalize(), ToTensor()]) # Load the training data and test data trainset = FaceKeypointsDataset(train_data, transform=tsfm) testset = FaceKeypointsDataset(test_data, train=False, transform=tsfm) # prepare data loaders train_loader, valid_loader = prepare_train_valid_loaders(trainset, valid_size, batch_size) test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size) 三、模型搭建 #Model class CNN(nn.Module): def __init__(self,outputs = 30): super(CNN,self).__init__() self.conv1 = nn.Conv2d(1, 16, 3, padding=1) self.conv2 = nn.Conv2d(16, 32, 3, padding=1) self.conv3 = nn.Conv2d(32, 64, 3, padding=1) self.pool = nn.MaxPool2d(2, 2) self.fc1 = nn.Linear(64*12*12, 1024) self.fc2 = nn.Linear(1024, outputs) self.dropout = nn.Dropout(0.3) def forward(self,x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = self.pool(F.relu(self.conv3(x))) x = x.view(-1, 64*12*12) x = F.relu(self.fc1(self.dropout(x))) x = self.fc2(self.dropout(x)) return x 四、训练与测试 #Train &amp; Validate device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') model = CNN(outputs = 30) model = model.to(device) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.003) def train(train_loader,valid_loader,model,criterion,optimizer,n_epochs = 50,saved_model = 'cv_model.pt'): valid_loss_min = np.Inf train_losses = [] valid_losses = [] for epoch in range(n_epochs): train_loss = 0.0 valid_loss = 0.0 #training model.train() for batch in train_loader: optimizer.zero_grad() output = model(batch['image'].to(device)) loss = criterion(output,batch['keypoints'].to(device)) loss.backward() optimizer.step() train_loss += loss.item()*batch['image'].size(0) #validating model.eval() for batch in valid_loader: output = model(batch['image'].to(device)) loss = criterion(output, batch['keypoints'].to(device)) valid_loss += loss.item()*batch['image'].size(0) train_loss = np.sqrt(train_loss/len(train_loader.sampler.indices)) valid_loss = np.sqrt(valid_loss/len(valid_loader.sampler.indices)) train_losses.append(train_loss) valid_losses.append(valid_loss) print('Epoch: &#123;&#125; \\tTraining Loss: &#123;:.6f&#125; \\tValidation Loss: &#123;:.6f&#125;' .format(epoch+1, train_loss, valid_loss)) # save model if validation loss has decreased if valid_loss &lt;= valid_loss_min: print('Validation loss decreased (&#123;:.6f&#125; --> &#123;:.6f&#125;). Saving model ...' .format(valid_loss_min, valid_loss)) torch.save(model.state_dict(), saved_model) valid_loss_min = valid_loss return train_losses, valid_losses 定义测试函数： def predict(data_loader, model): ''' Predict keypoints Args: data_loader (DataLoader): DataLoader for Dataset model (nn.Module): trained model for prediction. Return: predictions (array-like): keypoints in float (no. of images x keypoints). ''' model.eval() # prep model for evaluation with torch.no_grad(): for i, batch in enumerate(data_loader): # forward pass: compute predicted outputs by passing inputs to the model output = model(batch['image'].to(device)).cpu().numpy() if i == 0: predictions = output else: predictions = np.vstack((predictions, output)) return predictions 定义测试结果展示函数： def view_pred_df(columns, test_df, predictions, image_ids=range(1,6)): ''' Display predicted keypoints Args: columns (array-like): column names test_df (DataFrame): dataframe with ImageId and Image columns predictions (array-like): keypoints in float (no. of images x keypoints) image_id (array-like): list or range of ImageIds begin at 1 ''' pred_df = pd.DataFrame(predictions, columns=columns) pred_df = pd.concat([pred_df, test_df], axis=1) pred_df = pred_df.set_index('ImageId') show_images(pred_df, image_ids) # ImageId as index begin at 1 定义测试结果上交文件生成函数： def create_submission(predictions, pred_file='data/preds.csv', sub_file='data/submission.csv', columns=None): ''' Create csv file for submission from predictions Args: predictions (array-like): prediction (no. fo images x 30 keypoints) pred_file (string): file path for prediction csv file sub_file (string): file path for submission csv file columns (dict): provided column names for submission file ''' lookup = pd.read_csv('data/IdLookupTable.csv') if columns == None: columns = train_data.columns[:-1] preds = pd.DataFrame(predictions, index=np.arange(1, len(predictions)+1), columns=columns) preds.to_csv(pred_file) locations = [preds.loc[image_id, feature_name] for image_id, feature_name in lookup[['ImageId', 'FeatureName']].values] locations = [location if location &lt; IMG_SIZE else IMG_SIZE for location in locations] lookup.Location = pd.Series(locations) lookup[['RowId', 'Location']].to_csv(sub_file, index=False) 训练 and 预测： #Train &amp; Validate train(train_loader, valid_loader, model, criterion, optimizer, n_epochs=50, saved_model='aug_cnn.pt') #Predict model.load_state_dict(torch.load('aug_cnn.pt')) predictions = predict(test_loader,model) create_submission(predictions, pred_file='data/aug_cnn_preds.csv', sub_file='data/aug_cnn_submission.csv') columns = train_data.drop('Image', axis=1).columns view_pred_df(columns, test_data, predictions,range(1,11)) 预测结果： 将submission文件上交到Kaggle评分:","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://example.com/categories/Kaggle/"}],"tags":[{"name":"CV","slug":"CV","permalink":"http://example.com/tags/CV/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"}],"author":"Shaw"},{"title":"IFAttn-Binary code similarity analysis based on interpretable features with attention","slug":"【论文阅读】IFAttn Binary code similarity analysis based on interpretable features with attention","date":"2022-10-02T06:29:22.023Z","updated":"2022-10-03T12:58:53.044Z","comments":true,"path":"2022/10/02/【论文阅读】IFAttn Binary code similarity analysis based on interpretable features with attention/","link":"","permalink":"http://example.com/2022/10/02/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91IFAttn%20Binary%20code%20similarity%20analysis%20based%20on%20interpretable%20features%20with%20attention/","excerpt":"【论文阅读】IFAttn-Binary code similarity analysis based on interpretable features with attention 时间：2021 作者：Shuai Jianga , Cai Fua , Yekui Qian（华科） 期刊：Computers &amp; Security（中科院三区） 1.ABSTRACT 二进制代码相似性分析（Binary code similarity analysis ，BCSA）是一项有意义的软件安全分析技术，包括漏洞挖掘、代码克隆检测和恶意代码检测。 尽管近年来出现了很多基于神经网络的很多BCSA研究成果，仍有一些问题未被解决。首先，大多数方法更聚焦于function pair similarity detection task (FPSDT)，而忽略了function search task (FST)，后者对漏洞挖掘更为重要。还有，现有方法为了提高FPSDT的准确率使用了一些无法解释的神经网络；最后，大多数现有方法无法抵抗BCSA中的交叉优化和交叉混淆。 为了解决这些问题，我们首次提出了一个结合了可解释特征工程和可学习注意力机制的适应性BCSA架构。我们设计了一个具有适应性的、富有可解释性特征的模型。测试结果表明对于FPSDT和FST的效果比state-of-the-art效果更好。 另外，我们还发现注意机制在功能语义表达方面具有突出的优势。 评估表明我们的方法可以显著提升FST在cross-architecture, cross-optimization, cross-obfuscation and cross-compiler binaries上的表现。","text":"【论文阅读】IFAttn-Binary code similarity analysis based on interpretable features with attention 时间：2021 作者：Shuai Jianga , Cai Fua , Yekui Qian（华科） 期刊：Computers &amp; Security（中科院三区） 1.ABSTRACT 二进制代码相似性分析（Binary code similarity analysis ，BCSA）是一项有意义的软件安全分析技术，包括漏洞挖掘、代码克隆检测和恶意代码检测。 尽管近年来出现了很多基于神经网络的很多BCSA研究成果，仍有一些问题未被解决。首先，大多数方法更聚焦于function pair similarity detection task (FPSDT)，而忽略了function search task (FST)，后者对漏洞挖掘更为重要。还有，现有方法为了提高FPSDT的准确率使用了一些无法解释的神经网络；最后，大多数现有方法无法抵抗BCSA中的交叉优化和交叉混淆。 为了解决这些问题，我们首次提出了一个结合了可解释特征工程和可学习注意力机制的适应性BCSA架构。我们设计了一个具有适应性的、富有可解释性特征的模型。测试结果表明对于FPSDT和FST的效果比state-of-the-art效果更好。 另外，我们还发现注意机制在功能语义表达方面具有突出的优势。 评估表明我们的方法可以显著提升FST在cross-architecture, cross-optimization, cross-obfuscation and cross-compiler binaries上的表现。 为了提高开发效率，开发者通常会使用开源代码来开发新软件。如果一个开发者使用了一个存在漏洞的函数，则这个漏洞就会被继承且很难被发现。代码克隆这种现象对软件的安全可靠性造成了很大影响。 不幸的是，直接对二进制代码使用同源性分析并不够直接明了，二进制代码缺乏语义抽象，很难从函数中提取语义信息来做同源性分析。另外，分割函数的边界也是个问题。 所以在代码克隆、恶意样本追踪、库函数发掘和漏洞发掘领域对二进制代码做同源性分析一直是个重要的方向。 根据已有方法的颗粒度分类，现存成果可以分为： 基于基本静态特征的方法； 基于代码图结构（例如CFG）的方法； 基于代码张量的方法； 基于深度学习生成函数嵌入的方法； 现有方法存在的问题： 大多数方法更聚焦于function pair similarity detection task (FPSDT)，而忽略了function search task (FST)，后者对漏洞挖掘更为重要； FPSDT指对比两个函数的相似度，FST指在函数库中寻找与某个函数最相似的函数。 现有方法为了提高FPSDT的准确率使用了一些无法解释的神经网络，设计一个复杂的分析过程； 因为编译器的优化和混淆会显著修改代码结构，大多数方法在这些条件下表现不佳。 我们的方法解决的： 如何利用可解释的基本特征来生成二进制代码的语义特征； 用注意力机制来聚焦于在不同编译环境下仍起作用的特征，学习基本特征之间的联系。 如何设计一个适应不同编译选项的通用BCSA模型框架。 我们重新设计了Transformer的编码部分，使用孪生神经网络来判断两个函数的相似度，在attention layer使用KFM来处理不动的编译选项。 注意力机制的优点： 下图是同一个函数在同一个编译选项（X86_32_00）下通过混淆技术BCF前后的CFG图： 下图是TIKNIB和IFAttn两个模型在对同一个函数混淆前后抓取的特征向量值： 可以看到，在混淆前后basic features有明显不同，而semantic features则总体上更为稳定。 ​ 下图表明了TIKNIB和IFAttn在对不同函数，一个混淆一个不混淆的前提下提取到的特征向量值： ​ 可以看到，与之前相反，在混淆前后basic features较为稳定，而semantic features变化更大。 ​ semantic features更能体现函数本身语义，而不受混淆技术的影响。 一些定义： Presemantic features：直接或间接从代码语法和结构分析出的特征； Semantic features：本文通过注意机制分析了base feature形成的内在关联，并将base feature融合生成语义嵌入 KFM scaled dot-product attention: Multi-head attention:Transformer的一种注意力机制。 系统结构：IFAttn (Interpretable Features with Attention) 测试结果： 数据集： 测试结果：","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"},{"name":"BCSA","slug":"BCSA","permalink":"http://example.com/tags/BCSA/"}],"author":"Shaw"},{"title":"Function-level obfuscation detection method based on Graph Convolutional Networks","slug":"【论文阅读】Function-level obfuscation detection method based on Graph Convolutional Networks","date":"2022-10-01T12:37:04.833Z","updated":"2022-10-02T06:26:35.439Z","comments":true,"path":"2022/10/01/【论文阅读】Function-level obfuscation detection method based on Graph Convolutional Networks/","link":"","permalink":"http://example.com/2022/10/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Function-level%20obfuscation%20detection%20method%20based%20on%20Graph%20Convolutional%20Networks/","excerpt":"【论文阅读】Function-level obfuscation detection method based on Graph Convolutional Networks 时间：2021 作者：Shuai Jiang , Yao Hong, Cai Fu（华科） 期刊：Journal of Information Security and Applications（中科院三区） 1.ABSTRACT ​ 在恶意样本检测中代码混淆检测技术是一个重要的辅助手段，对于安全从业者来说，其可以在人工逆向分析前来实施自动化混淆检测，这有助于逆向工程师更具体地进行逆向分析。 ​ 目前存在的混淆检测方法主要作用于Android应用，并基于传统的机器学习方法。其检测颗粒度很差，总体效果不佳。为了解决这些问题，在本篇文章，我们提出了一个应用于X86汇编和Android应用的、function level的、基于GCN的混淆检测方法。 1. 首先，我们的方法是function-level的。我们提取每个函数的CFG作为其特征（包括邻接矩阵和基本代码块的特征矩阵）； 2. 我们构建一个GCN-LSTM神经网络作为混淆检测模型； 3. 最后，对于function-level的检测我们的方法准确率是94.7575%（X86汇编）和98.9457%（安卓应用），比baseline方法好。实验证明我们的方法不论是在function-levle还是APK-level上的检测准确率都好于baseline。","text":"【论文阅读】Function-level obfuscation detection method based on Graph Convolutional Networks 时间：2021 作者：Shuai Jiang , Yao Hong, Cai Fu（华科） 期刊：Journal of Information Security and Applications（中科院三区） 1.ABSTRACT ​ 在恶意样本检测中代码混淆检测技术是一个重要的辅助手段，对于安全从业者来说，其可以在人工逆向分析前来实施自动化混淆检测，这有助于逆向工程师更具体地进行逆向分析。 ​ 目前存在的混淆检测方法主要作用于Android应用，并基于传统的机器学习方法。其检测颗粒度很差，总体效果不佳。为了解决这些问题，在本篇文章，我们提出了一个应用于X86汇编和Android应用的、function level的、基于GCN的混淆检测方法。 1. 首先，我们的方法是function-level的。我们提取每个函数的CFG作为其特征（包括邻接矩阵和基本代码块的特征矩阵）； 2. 我们构建一个GCN-LSTM神经网络作为混淆检测模型； 3. 最后，对于function-level的检测我们的方法准确率是94.7575%（X86汇编）和98.9457%（安卓应用），比baseline方法好。实验证明我们的方法不论是在function-levle还是APK-level上的检测准确率都好于baseline。 2. INTRODUCTION 2.1 Obfuscation–代码混淆 ​ 代码保护技术，用于增加逆向难度，防止代码篡改，最开始用于版权保护，后被用于恶意代码的躲避检测。 ​ 由于动态检测恶意代码的高昂成本，主流的恶意样本检测技术仍在提取代码的静态特征。然而由于混淆技术的发展，恶意样本的编写者经常在保留其恶意功能的同时通过使用混淆技术来修改其静态特征。经过混淆的恶意代码可以规避相关工具的检测。 ​ 最近混淆检测技术开始出现，在此领域有一些工作： 1. [2018]Alessandro等人：使用静态分析和机器学习分类算法来分析Android应用是否被混淆的技术（ http://dx.doi.org/10.1145/3230833.3232823）； 2. [2020]Crincoli等人：利用weak bisimulation来检测代码是否被code reordering(http://dx.doi.org/10.1007/978-3-030-44041-1_116)； 3. [2020]Caijun Sun等人：一个Android打包检测框架DroidPDF( http://dx.doi.org/10.1109/ACCESS.2020.3010588); 4. [2019]Omid等人：AndrODet，一个检测三种Android混淆技术的检测机制（重命名、字符串加密和控制流混淆）（ http://dx.doi.org/10.1016/j.future.2018.07.066）； 5. [2019]Alireza等人：Android字符串混淆检测技术（ http://dx.doi.org/10.1145/3338501.3357373）; 2.2 目前方法的缺点： 1. 检测颗粒度不够，检测对象往往是一个APK包，缺乏可行性； 2. 检测效果不佳，从样本中提取的特征相对简单，只有统计学特征和opcode语句被提取。大多数使用简单的机器学习方法，表现一般； 3. 缺乏可用性和适应性，传统方法往往会为不同的混淆方法提取不同的特征，或者为每个混淆方法训练一个二分类器，枯燥且不便。同时若需要添加新数据，模型经常需要重头训练。 2.3 我们的方法： 从开源平台获取一些未经混淆的代码（X86汇编和Android），通过混淆器生成混淆后程序； 逆向这些程序，用邻接矩阵和基本代码块特征矩阵的形式提取每个函数的CFG； 根据我们提取到的特征，构建GCN-LSTM。这个模型同时服务于X86汇编和Android，但他们分别训练和测试； 3. METHOD OVERVIEW 1. X86的混淆器：OLLVM；Android的混淆器：Obfuscapk； 2. 逆向工具：IDA PRO；由于X86与Android指令集与混淆技术不同，二者被分别提取CFG； 3. 多分类问题，检测出混淆技术种类，故使用传统多分类评估方法来评估检测效果。 4. OBFUSCATION DECTION METHOD 4.1 OLLVM ​ OLLVM包括以下三种混淆技术： Instructions Substitution (SUB)，指令替换。将简单指令替换为同语义的复杂指令，特别是二进制加减乘除。这项技术会增加算术指令但很少影响CFG； Bogus Control Flow (BCF)，虚假控制流。在CFG中添加大量无关的随机代码块和分支，并分割、融合、重排原始代码块，在其中插入随机选择的无用指令。这项技术破坏了CFG和代码块的完整性，增加控制流复杂性； Control Flow Flattening（FLA），控制流平坦化。简单来讲就是将代码块之间的关系打断，由一个分发器来控制代码块的跳转： 4.2 Obfuscapk ​ Obfuscapk包括以下三种混淆技术： Identifier Renaming，标识符重命名； String Encryption，字符串加密。字符串常量可以揭示很多代码敏感信息； Control Flow Obfuscation，控制流混淆。通过扩展或平坦化CFG来混淆，同样还有注入垃圾代码，扩展循环，添加无关操作等； 4.3 Feature extraction ​ 提取的特征包括代码的结构化CFG信息和基本代码块特征： 对于邻接矩阵： 如图，邻接矩阵代码了不同代码块之间的转移关系。 ​ 对于基本代码块特征： 将X86汇编代码分为27类，对于一个代码块的特征向量就是27维。 将Dalvik指令分为15类，对于一个代码块的特征向量就是15维。 4.4 Obfuscation detection model ​ 在得到CFG邻接矩阵和基本代码块特征矩阵后，构建GCN-LSTM。 5.EXPERIMENTAL EVALUATION 5.1 Datasets 5.2 Baseline methods ​ function-level的baseline方法如下： ​ AdaBoost, GaussianNaiveBayes, GradientBoosting, KNeighbors, MLP, SGDClass, SVM, Xgboost and LSTM 5.3 Evaluation metrics ​","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"},{"name":"obfuscation","slug":"obfuscation","permalink":"http://example.com/tags/obfuscation/"}],"author":"Shaw"},{"title":"Planning at Decision Time(决策时规划)","slug":"决策时规划","date":"2022-07-21T07:46:53.321Z","updated":"2022-07-24T06:54:15.431Z","comments":true,"path":"2022/07/21/决策时规划/","link":"","permalink":"http://example.com/2022/07/21/%E5%86%B3%E7%AD%96%E6%97%B6%E8%A7%84%E5%88%92/","excerpt":"参考资料： Reinforcement Learining. Second Edition. Sutton.Page 180-193 白板推导--强化学习.shuhuai008.Bilibili Easy RL.Qi Wang.Yang Yiyuan.Ji Jiang Planning at Decision Time（决策时规划） 规划（Planning）至少有两种使用方式。 ①一种在DP和Dyna中已经讨论过，通过从一个model（不论是sample model或是distribution model ）中获取模拟经验（simulated experience）的基础上来使用规划来逐渐提升一个policy或一个value function。 然后，选择动作是一个比较当前状态的动作value问题，该value是在之前优化的表格中获取的；或者通过使用书中Part 2中考虑的近似方法来评估数学表达式。 对于任意状态St，在为其选择一个动作之前，其整个表格条目（例如Dyna-Q中的Q表）已经通过规划来优化过了。使用这种方式，规划并不是仅仅聚焦于当前的状态，我们称这种规划为background planning，后台规划。 ②另一种使用规划的方法就是在遇到每个新的状态St后再开始一个完整的规划过程，其为每个当前状态选择一个动作At，到下一个状态St+1就选择一个动作At+1，以此类推。 一个使用这种规划最简单的例子：当state values可用时，通过比较当前model对执行每个动作后到达的新状态的value来选择一个动作。 当然，更普遍的说，这种规划的用法可以比仅仅往后看一步（上面的例子就是）看得更深，评估动作的选择导致许多不同预测状态和奖励轨迹。 不同于第一种用法，在这里，规划聚焦于一个特定的状态，我们称之为decision-time planning，决策时规划。 这两种规划的方式可以用一种自然而有趣的方式结合在一起，不过一般二者被分开研究。 如同background planning，我们仍可以将决策时规划看作一个从模拟经验中更新values，最后到更新policy的过程。只是基于当前状态所生成的values和policy会在做完动作选择决策后被丢弃，在很多应用场景中这么做并不算一个很大的损失，因为有非常多的状态存在，且不太可能在短时间内回到同一个状态，故重复计算导致的资源浪费会很少。 一般来说，人们可能希望将两者结合起来：规划当前状态，并将规划的结果存储起来，以便在以后回到相同的状态时能走得更远。 Decision-time Planning，(决策时规划）在不需要快速反应的应用场景中作用最为显著。 决策时规划的常用算法有Heuristic Search（启发式搜索）、Rollout Algorithms（Rollout 算法）和Monte Carlo Tree Search（MCTS 蒙特卡洛树搜索）三种。","text":"参考资料： Reinforcement Learining. Second Edition. Sutton.Page 180-193 白板推导--强化学习.shuhuai008.Bilibili Easy RL.Qi Wang.Yang Yiyuan.Ji Jiang Planning at Decision Time（决策时规划） 规划（Planning）至少有两种使用方式。 ①一种在DP和Dyna中已经讨论过，通过从一个model（不论是sample model或是distribution model ）中获取模拟经验（simulated experience）的基础上来使用规划来逐渐提升一个policy或一个value function。 然后，选择动作是一个比较当前状态的动作value问题，该value是在之前优化的表格中获取的；或者通过使用书中Part 2中考虑的近似方法来评估数学表达式。 对于任意状态St，在为其选择一个动作之前，其整个表格条目（例如Dyna-Q中的Q表）已经通过规划来优化过了。使用这种方式，规划并不是仅仅聚焦于当前的状态，我们称这种规划为background planning，后台规划。 ②另一种使用规划的方法就是在遇到每个新的状态St后再开始一个完整的规划过程，其为每个当前状态选择一个动作At，到下一个状态St+1就选择一个动作At+1，以此类推。 一个使用这种规划最简单的例子：当state values可用时，通过比较当前model对执行每个动作后到达的新状态的value来选择一个动作。 当然，更普遍的说，这种规划的用法可以比仅仅往后看一步（上面的例子就是）看得更深，评估动作的选择导致许多不同预测状态和奖励轨迹。 不同于第一种用法，在这里，规划聚焦于一个特定的状态，我们称之为decision-time planning，决策时规划。 这两种规划的方式可以用一种自然而有趣的方式结合在一起，不过一般二者被分开研究。 如同background planning，我们仍可以将决策时规划看作一个从模拟经验中更新values，最后到更新policy的过程。只是基于当前状态所生成的values和policy会在做完动作选择决策后被丢弃，在很多应用场景中这么做并不算一个很大的损失，因为有非常多的状态存在，且不太可能在短时间内回到同一个状态，故重复计算导致的资源浪费会很少。 一般来说，人们可能希望将两者结合起来：规划当前状态，并将规划的结果存储起来，以便在以后回到相同的状态时能走得更远。 Decision-time Planning，(决策时规划）在不需要快速反应的应用场景中作用最为显著。 决策时规划的常用算法有Heuristic Search（启发式搜索）、Rollout Algorithms（Rollout 算法）和Monte Carlo Tree Search（MCTS 蒙特卡洛树搜索）三种。 Heuristic Search（启发式搜索） 在AI中一个经典的状态空间规划方法是decision-time planning方法，统称为Heuristic Search（启发式搜索）。 在启发式搜索中，对每个遇到的状态都会生成一颗延续的搜索树，近似的value function会被在叶节点应用，然后反向传播到根节点。反向传播在当前状态停止。一旦这些节点的值被计算出来，就会选择其中最好的一个作为当前的行动，然后所有的值就会被丢弃。 在传统的启发式搜索中，计算出的backed-up values并不会被通过修改近似value function来保存。实际上，价值函数一般都是由人设计的，绝不会因为搜索而改变。然而，一个自然而然的想法就是考虑改进value function，使用启发式搜索计算出的backed-up value或者其他方法。从某种意义上说，我们一直都采取这种方法。 Greedy，ε-greedy和UCB动作选择方法与启发式搜索没什么不同，尽管是在一个更小的范围内。举个例子，为了计算greedy策略的state-value function，我们必须向前概览每个可能的动作，到达每个可能的下一个状态，考虑他们的reward和评估值，然后选择一个最好的动作。这就如同传统的启发式搜索，计算所有可能动作的backed-up values，但是不去保存他们。因此，启发式搜索可以被看作greedy策略在单步范围外的一种扩展。 搜索的更深是为了获得更好的动作选择策略。假设我们有一个完美的model和一个不完美的action-value function，如果一路搜索到底，那么不完美的value function的影响就会被消除。通过这种方式的搜索必须是optimal的。若搜索步骤k足够多，导致 γk 足够小，那么动作就会近似达到optimal。从另一个角度讲，搜索的越深所需的算力资源也越多，响应就越慢。 一个优秀的例子是是Tesauro提出的大师级双陆棋算法，TD-Gammon。该算法使用TD learning，通过与自己对弈来学习value function，其使用启发式搜索来选择动作。Tesauro发现启发式搜索的越深，TD-Gammon就会选择越好的动作，但是每走一步花的时间也越多。 在更深入的搜索中观察到的性能改善并不是由于使用多步骤更新本身。相反，它是由于更新的重点集中在当前状态的下游的状态和行动上。通过投入大量与候选行动具体相关的计算，决策时规划可以产生比依靠无重点的更新所产生的更好的决策。 启发式搜索示意图 如上图所示，白圆点表示状态，黑圆点表示动作。以当前状态为根节点，遍历每一层所有的可能性，然后计算每个非叶节点的Q（s,a）。具体计算方法如下，将树按深度优先遍历，每个非叶节点使用公式： \\[ Q(s,a) = \\sum_{s&#39;,r}{p(s&#39;,r|s,a)(r + γmax_{a&#39;}Q(s&#39;,a&#39;))} \\] 故按照图中的数字顺序（1-&gt;2-&gt;3-&gt;4-&gt;5……）依次计算**q~*~的期望更新**，得到当前状态（根节点）每个可能动作a'的对应的Q（s，a'），选择一个最好的即可。当然，在使用算法的过程中搜索层数K是可以控制的，故叶节点并不一定是终结状态ST。 Rollout Algorithms（Rollout 算法） Rollout算法是基于蒙特卡洛控制的决策时规划算法，其应用于从当前状态开始的一些模拟轨迹。对给定的policy进行动作评估的方式是将许多从各个可能的动作开始的模拟轨迹的返回值平均化。当action-value评估被认为足够准确了，被给分最高的动作就会被执行，之后，该过程（Rollout）将从产生的下一个状态重新进行。 不同于蒙特卡洛控制算法，Rollout算法的目标不是评估一个最优的q*或qπ，而是根据一个给定的一般叫rollout policy的策略，来为每个当前状态生成蒙特卡洛评估。作为一个决策时规划算法，Rollout算法在当即使用这些action-value评估值之后就丢弃他们。这使得Rollout算法的实现相对较为简单，因为不都需要对每个&lt;s , a&gt;键值对都采样，并且不需要对状态空间或状态-动作空间拟合一个近似函数。 Rollout算法什么时候停止？策略改进定理告诉我们给定两个几乎相同的策略π和π'，他们不同是对同一个状态S有: \\[ π&#39;(s) = a ≠ π(s) \\] 如果： \\[ q_{π}(s,a)\\geq v_{π}(s) \\] 那么策略π'就优于或等于策略π。 在Rollout算法中，对状态s的每个可能的动作a'都计算其若干条模拟轨迹的平均返回值，得到\\(q_π(s,a&#39;)\\)。接着选取评估值最大的那个action，随后的状态都继续遵循策略π，这就是一个很好的在π上的策略改进。 换句话说，Rollout算法的目标就是在rollout policy上不断做改进，而不是去寻找一个最优策略。经验表明Rollout算法的效果非常惊艳。例如Tesauro和Galperin（1997）就表明rollout算法对于双陆西洋棋的提升效果非常显著。在一些应用中，即使rollout policy是完全随机的Rollout算法也可以有好的表现。但是policy的改进依靠rollout policy的性能和MC值评估得出的action排名。直觉表明，rollout policy越好、评估值越准确，Rollout算法给出的策略就越好。 这其中包含了重要的权衡，因为一般来说越好的rollout policy意味着需要越多的时间来模拟足够的轨迹，以得到好的value评估效果。作为一个决策时评估方法，Rollout算法一般都会有严格的时间限制，其计算所需时间由待评估动作的数量、模拟轨迹中的步长、rollout policy做决策的时间和模拟轨迹的数量共同决定。 虽然存在一些方法可以减轻这一难题，但在任何Rollout算法的应用中平衡这些因素都是很重要的。因为MC评估是相互独立的，故并行做这些评估是可能的。另外一种方法是可以缩减模拟轨迹的长度。 **简单来说，不同于启发式搜索往下遍历所有的可行性然后进行q*期望更新，Rollout对每个可能的action进行若干条MC采样，以这些采样的平均值来评估这个aciton的好坏。** Rollout 的意思是从当前帧去生成很多局的游戏。 Monte Carlo Tree Search（MCTS 蒙特卡洛树搜索） Monte Carlo Tree Search (MCTS)，蒙特卡洛树搜索，是一个近年来非常成功的决策时规划算法。MCTS是一个rollout改进算法，其在Rollout的基础上增加了累计从MC模拟轨迹中获得value的方法，以便于模拟到有更高reward的轨迹。 MCTS是近年来AI围棋从一个入门者（2005）发展到一个宗师级棋手（2015）的重要原因，2016年AlphaGo程序战胜了世界围棋冠军选手。MCTS被证明在许多竞争领域有显著效果，包括一般的游戏，但不局限于此。若环境model足够简单，可以进快速多步模拟，它对单智能体序列决策问题就非常有效。 MCTS的核心思想是从以往的模拟中，扩展初始部分已经获得高回报的轨迹，让算力聚焦于更可能获得高回报的模拟路线。 在任何MC评估方法中，&lt;s,a&gt;键值对的评估值就是多对模拟轨迹的平均返回值，在这里，只保留最有可能在几步内达到的&lt;s,a&gt;对的MC估计值（算出来的q（s,a））。我们称这些节点加上根节点组成一个tree，使用一个tree policy遍历这个tree来选择一个用来扩展的叶子结点，构建一个tree帮助我们能选择一个更好的节点用来rollout而不是对每个节点都进行模拟。 总的来说，MCTS总共分为四步：Selection，Expansion，Simulation和Backup。 Selection就是用tree policy（例如ε-greedy）来选择一个叶节点，用于后续扩展； Expansion就是使用选择好的节点，用一些未使用过的actions来扩展一个或几个孩子节点； Simulation就是从选择的节点/扩展的节点上用rollout policy进行模拟，同rollout算法； Backup就是通过模拟得出的值来反向更新对应的action. 在一个时间步骤内，MCTS反复做这四步，直到时间不够了或者其他计算资源不够了。然后，通过某种方法来为当前状态选择一个动作。例如，选择value最大的动作，或是，选择visit次数最多的动作来避免选到异常值。当到达下一个状态后，新一轮的MCTS又开始了。有时新一轮的MCTS从一个孤立的节点开始，但大多数情况下会从上一次MCTS中还留存的、有些后代的tree开始。 MCTS最初被提出用于为一些双人竞技游戏选择动作，例如围棋。每个模拟过程都是一个完整的游戏过程，双方选手通过tree和rollout policy来选择动作。 相关概念解释： 1. Distribution model and Sample model 参考《Reinforcement Learning》Page159，Chapter 8，原文解释的很清楚： “By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions. Given a state and an action, a model produces a prediction of the resultant next state and next reward. ” “我们所指的环境的model是一个agent可以用其预测环境会如何对其action作出反应的东西。给出一个state和一个action，model给出下一个state和返回的reward。” “If the model is stochastic, then there are several possible next states and next rewards, each with some probability of occurring. ” “如果model是随机的，那么下一个state与reward就有许多可能的情况，每个情况都有一定概率发生。” “Some models produce a description of all possibilities and their probabilities; these we call distribution models. Other models produce just one of the possibilities, sampled according to the probabilities; these we call sample models.” “一些models提供了一个对所有事件发生的可能性以及其概率的描述，这些models我们称其为distribution models（分布模型）；另外一些models仅提供这些可能发生的事件的其中一种，这些models我们称其为sample models（样本模型）” “ For example, consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring, whereas a sample model would produce an individual sum drawn according to this probability distribution.” “例如，考虑对一打骰子（dozen dice）的和进行建模，一个 distribution model 会产生所有可能的和，以及它们发生的概率，而一个 sample model 会根据这个概率分布产生一个单独的和。” 例如，MDP中的\\(p(s^{&#39;},r|s,a)\\)就是一个典型的分布模型。在很多应用中，获取 sample models 比获取 distribution models 容易得多。dozen dice 就是这样一个例子。很容易写一个电脑程序仿真掷骰、返回和的过程，但是计算所有可能的和以及对应的概率很难，且容易出错。","categories":[{"name":"Something","slug":"Something","permalink":"http://example.com/categories/Something/"}],"tags":[{"name":"RL","slug":"RL","permalink":"http://example.com/tags/RL/"}],"author":"Shaw"},{"title":"A Survey of Defense Mechanisms Against Distributed Denial of Service (DDoS) Flooding Attacks","slug":"A Survey of Defense Mechanisms Against Distributed Denial of Service (DDoS) Flooding Attacks","date":"2022-03-04T08:24:55.254Z","updated":"2022-07-15T08:09:16.356Z","comments":true,"path":"2022/03/04/A Survey of Defense Mechanisms Against Distributed Denial of Service (DDoS) Flooding Attacks/","link":"","permalink":"http://example.com/2022/03/04/A%20Survey%20of%20Defense%20Mechanisms%20Against%20Distributed%20Denial%20of%20Service%20(DDoS)%20Flooding%20Attacks/","excerpt":"A Survey of Defense Mechanisms Against Distributed Denial of Service (DDoS) Flooding Attacks 时间：2013 作者：Saman Taghavi Zargar，James Joshi， David Tipper 期刊：IEEE COMMUNICATIONS SURVEYS &amp; TUTORIALS（中科院一区） ABSTRACT ​ DDoS攻击是安全专业人员最关心的问题之一，其通常是为了扰乱合法用户对服务的访问而进行的显式尝试。攻击者通常通过攻击漏洞来获取到一大批电脑，以此来组建一个网络攻击军队（也就是僵尸网络），一旦组建了攻击部队，攻击者就可以对一个或多个目标发起协调一致、大规模的攻击。开发针对已识别和预期的DDoS泛洪攻击的综合防御机制，是入侵检测和预防研究界所期望的目标。然而，这种机制的发展需要对问题和迄今为止在预防、检测和应对各种DDoS洪泛攻击方面所采用的技术有一个全面的了解。 ​ 在本文，我们对DDoS洪泛攻击进行分类，并根据它们在何时何地预防、检测和应对DDoS洪泛攻击对现有的对策进行分类。 ​ 此外，我们强调需要一种全面的分布式协同防御方法。我们的主要目的是激发研究人员开发出创造性的、有效的、高效的、综合的预防、检测和响应机制来解决实际攻击前、中和后的DDoS泛洪问题。","text":"A Survey of Defense Mechanisms Against Distributed Denial of Service (DDoS) Flooding Attacks 时间：2013 作者：Saman Taghavi Zargar，James Joshi， David Tipper 期刊：IEEE COMMUNICATIONS SURVEYS &amp; TUTORIALS（中科院一区） ABSTRACT ​ DDoS攻击是安全专业人员最关心的问题之一，其通常是为了扰乱合法用户对服务的访问而进行的显式尝试。攻击者通常通过攻击漏洞来获取到一大批电脑，以此来组建一个网络攻击军队（也就是僵尸网络），一旦组建了攻击部队，攻击者就可以对一个或多个目标发起协调一致、大规模的攻击。开发针对已识别和预期的DDoS泛洪攻击的综合防御机制，是入侵检测和预防研究界所期望的目标。然而，这种机制的发展需要对问题和迄今为止在预防、检测和应对各种DDoS洪泛攻击方面所采用的技术有一个全面的了解。 ​ 在本文，我们对DDoS洪泛攻击进行分类，并根据它们在何时何地预防、检测和应对DDoS洪泛攻击对现有的对策进行分类。 ​ 此外，我们强调需要一种全面的分布式协同防御方法。我们的主要目的是激发研究人员开发出创造性的、有效的、高效的、综合的预防、检测和响应机制来解决实际攻击前、中和后的DDoS泛洪问题。 INTRODUCTION ​ DDoS攻击，旨在尝试组织合法使用者访问一个特定的网站，早在20世纪80年代就被网络研究团体知晓。1999年夏天，Computer Incident Advisory Capability (CIAC)报告了第一起DDoS攻击事件。 ​ 目前，有两种主要的在互联网上制造DDoS攻击的方法：①第一种方法是攻击者向受害者发送一些格式错误的数据包，以混淆在其上运行的协议或应用程序( vulnerability attack);②另一种方法是最常见的方法，它涉及攻击者试图执行下列一项或两项： 1. 通过消耗带宽、路由器处理能力或网络资源，破坏合法用户的连通性；这些本质上是网络/传输级的泛洪 攻击； 2. 通过耗尽服务器资源(如套接字、CPU、内存、磁盘/数据库带宽和I/O带宽 )来破坏合法用户的服务；这些攻击本质上包括应用级的泛洪攻击； ​ 如今，DDoS攻击往往是由远程控制、组织良好、分布广泛的Zombies1或Botnet计算机组成的网络发起的，这些计算机同时和不断地向目标系统发送大量的流量和/或服务请求，目标系统因此变得要么反应如此缓慢，以至于无法使用，要么完全崩溃。僵尸网络中的僵尸或计算机通常通过使用蠕虫、木马或后门来招募。此外，由于在攻击者控制下的僵尸使用伪造的IP地址，使得防御机制识别原始攻击者变得更加复杂。 ​ 从1999年的夏天开始，许许多多的DDoS攻击被制造出来用以对抗不同的组织。迄今为止，大多数的DDoS洪泛攻击都试图使受害者的服务不可用，从而收入损失，增加了减轻攻击和恢复服务的成本。 ​ 举个例子，2000年2月，雅虎经历了最早的一次重大DDoS泛洪攻击，使得该公司的服务在互联网上持续了约2小时，从而造成广告收入的显著损失； ​ 2002年10月，13个DNS服务器中的9个因为DDoS洪泛攻击关闭了一小时； ​ 2004年2月，由于遭受DDoS攻击，正常用户无法使用SCO Group网站，这种攻击是通过使用以前感染Mydoom病毒的系统发起的。 ​ 2009年7月，Mydoom病毒被再次使用以发起DDoS攻击韩国和美国主要政府新闻媒体和经济网站； ​ 2010年12月，一个自称为“Anonymous”的组织策划了针对一些组织诸如Mastercard.com, PayPal, Visa.com 和 PostFinance的DDoS攻击； ​ 2012年9月，美国银行( Bank of America )、花旗集团( Citigroup )、富国银行( Wells Fargo )、美国银行( Bancorp )、PNC、Capital One、Fifth Third Bank、BB &amp; T、BB &amp; T、汇丰银行( HSBC )等9家主要银行的网上银行成为国外黑客攻击集团\" Izz ad-Din al-Qassam Cyber Fighters \"发起的一系列强力DDoS泛洪攻击的目标。因此，几家网上银行网站在几分钟后恢复之前已经放缓或停业。 ​ 最近DDoS防御机制的进步已经结束了“脚本小子”可以下载工具并对几乎任何网站发起攻击的时代。（2013）在如今的DDoS攻击中，攻击者使用了更复杂的方法来发起攻击。尽管尽了一切努力减少DDoS攻击事件的数量，但它们在目标网络和计算机的频率和规模上迅速扩大。在VeriSign委托的最近一项调查中发现，在2008年7月至2009年7月期间，75 %的受访者经历过一次或多次攻击。此外，最近来自Arbor Networks的一份报告也表明了类似的数据。在调查结果中，他们显示，69 %的受访者从2009年10月至2010年9月至少经历过一次DDoS攻击，25 %的受访者每月遭受10次此类攻击。ProlexicTechnology公司提供的防御DDoS攻击的服务显示，每天有7000个DDoS攻击被观测到，并且认为这个数量正在迅速增长。 ​ DDoS攻击的规模也在不断增大，使得防御起来更加困难。Arbor Network发现，2010年攻击规模已经增长了100 %左右，攻击首次突破了100Gbps的壁垒。因此，保护资源免受这些频繁而庞大的DDoS攻击，就需要研究界致力于开发一种能够在实际攻击之前、期间和之后对DDoS攻击做出适当反应的全面的DDoS防御机制。 ​ 本文重点研究有线网络系统中DDoS泛洪攻击和防御机制。【19】专注于描述无线自组织网络的DDos攻击；【20】专注于无线传感器网络特有的DDoS攻击的特点。 ​ 在这里，我们的目标是对现有的DDoS洪泛攻击进行分类，并提供根据它们在何处和何时检测和响应DDoS洪泛攻击而分类的防御机制的全面调查。对DDoS洪泛攻击的研究和所做的调查对于了解与这一重要网络安全问题有关的关键问题，从而对建立更全面有效的防御机制具有重要意义。 ​ DDOS: ATTACKERS’ INCENTIVES ​ DDoS攻击者的动机有以下几类： ​ 1. 获取经济收入：这些攻击是企业关注的重大问题，由于其激励的性质，这一类的攻击者通常是技术含量最高、经验丰富的攻击者。为获取经济利益而发动的攻击往往是最危险、最难以阻止的攻击。 ​ 2. 复仇者：这一类的攻击者一般都是沮丧的个体，可能具有较低的技术技能，他们通常将攻击作为对遭受到的不公正的待遇的报复。 ​ 3. 受思想信念驱动：属于这一类的攻击者是出于其思想信念的动机来攻击目标的。该类别目前是攻击者发起DDoS攻击的主要诱因之一。 ​ 4. 智力挑战者：这类攻击者攻击目标系统，以实验和学习如何发起各种攻击。他们通常都是年轻的黑客爱好者，想要炫耀自己的能力。如今，存在着各种易于使用的攻击工具和僵尸网络来租用，即便是一个计算机业馀者也可以利用其发起起成功的DDoS攻击。 ​ 5. 网络战争：这一类的袭击者通常属于一国的军事或恐怖组织，他们有政治动机攻击另一国广泛的关键部门 ​ 有一些文章专注于分析攻击者的动机以及如何利用这些攻击动机建模，以此使决策模型可以组织并对这些攻击进行反馈。 DDOS ATTACK: SCOPE AND CLASSIFICATION ​ DDoS攻击的分布式特性使得它们极难对抗或追踪，而且攻击者通常使用假IP以此来隐藏他们的真实身份，这使得对于DDoS攻击的追踪回溯更加困难。此外，许多因特网主机中存在入侵者可以利用的安全漏洞，针对应用层的攻击事件正在迅速增加。 ​ 在这里，我们针对DDoS的协议等级对DDoS攻击进行分类，并且我们仅仅专注于最常见的DDoS洪泛攻击，漏洞攻击，攻击者利用某个服务的软件实现中的某些漏洞或实现bug将其带下来，并不是本文的重点。 ​ 基于协议等级，DDoS攻击可以被分类为两类： Network/transport-level DDoS flflooding attacks: 1.1 Flooding attacks：攻击者通过消耗受害者网络带宽(如欺骗/非欺骗UDP 流、ICMP流、DNS流、VoIP流等)来破坏合法用户的连通性。 1.2 Protocol exploitation flooding attacks:攻击者利用受害者某些协议的特定特性或实现漏洞，以消耗受害者的过量资源（例如，TCP SYN流，TCP SYN-ACK流，ACK &amp; PUSH ACK 流，RST/FIN flood流等）。 1.3 Reflflection-based flflooding attacks：攻击者通常向reflectors发送伪造请求( 例如 , ICMP请求 )而不是直接请求；因此，这些反射器向受害者发送答复并耗尽受害者的资源. 1.4 Amplifification-based flflooding attacks：攻击者利用服务为接收到的每个消息生成大消息或多个消息来放大对受害者的流量。僵尸网络被不断地用于反射和放大两种目的。反射和放大技术通常是串联使用的，如Smurf攻击时，攻击者利用数据包的IP广播特性( Amplification ) ，向大量的反射器发送带有伪造源IP地址的请求。 以上攻击的具体细节见【2】，【32】，【35】，【36】。 Application-level DDoS flflooding attacks: 应用级别的DDoS攻击通过耗尽服务器资源(如套接字、CPU、内存、磁盘/数据库带宽和I/O带宽 )来破坏合法用户的服务，其通常消耗更少的宽带，更加隐蔽，其与良性流量非常相似。但是，应用级别的DDoS攻击通常具有相同的影响力。 2.1 Reflflection/amplifification based flflooding attacks：这种攻击利用了与1.4同样的技术，只不过发送的是应用层数据包。 举个例子，DNS amplification 攻击使用肉机生成一小股伪造IP的DNS请求，因为DNS响应的数量可能远远超过DNS请求的数量，其可以生成大量网络流量包直指目标系统，使其瘫痪。 再举一个，VOIP flooding，攻击者通常通过SIP以非常高的包率和非常大的源IP范围发送被欺骗的VoIP数据包。受害者VoIP服务器必须区分正确的VoIP连接和消耗大量资源的伪造的VoIP连接。VoIP泛洪可以压倒具有随机或固定源IP地址的数据包的网络。如果源IP地址没有被改变，那么VoIP泛洪攻击就会模仿来自大型VoIP服务器的流量，并且由于类似于良好的流量，很难识别。 2.2 HTTP flooding attacks： 2.2.1 Session flooding attacks，在这种类型的攻击中，攻击者的会话连接请求率高于合法用户的请求率，因此消耗了服务器的资源并导致DDoS攻击服务器。 ​ 其中的典型就是HTTP get/post flooding attack，其中攻击者向受害者Web服务器生成大量有效的HTTP请求( get / post )。攻击者通常使用僵尸网络来发起这些攻击。由于每个bot都可以产生大量的有效请求(通常每秒10个以上的请求)，所以不需要大量的bot发起成功的攻击。HTTP get / post泛洪攻击属于非欺骗性攻击。 2.2.2 Request flooding attacks，在这种类型的攻击中，攻击者发送的会话包含比通常更多的请求，并导致服务器遭受DDoS泛滥攻击。 ​ 其中的典型就是 single-session HTTP get/post flflooding，该攻击是HTTP get/post flflooding attack的一种变种，它利用HTTP 1.1的特性，允许单个HTTP会话中的多个请求。因此，攻击者可以限制HTTP攻击的会话速率，并绕过许多安全系统的会话速率限制防御机制。 ​ 2.2.3 Asymmetric attacks，在这种攻击类型中，攻击者发送包含高工作负载请求的会话。这里，我们列举了这一类中的一些著名攻击： ​ Multiple HTTP get/post flflood，该攻击是HTTP get/post flflooding attack的一种变种，在这里，攻击者通过形成一个嵌入多个请求的单个数据包，而不在单个HTTP会话中逐一发出多个HTTP请求，从而创建多个HTTP请求。 ​ 这样，攻击者仍然可以以较低的攻击包率在受害服务器上保持较高的负载，使得攻击者几乎看不到网络流量异常检测技术。此外，攻击者如果仔细选择HTTPVERB，就可以轻松绕过深度包检查技术。 ​ Faulty Application，在这种攻击中，攻击者利用设计不良或与数据库集成不当的网站进行攻击。例如，它们可以使用类似SQL的注入来生成请求来锁定数据库查询。这些攻击非常具体和有效，因为它们消耗服务器资源( 内存、 CPU等)。 ​ 2.2.4 Slow request/response attacks，在这种攻击类型中，攻击者发送包含高负载请求的会话。 ​ Slowloris attack (a.k.a, slow headers attack)，Slowloris（懒猴）是一种基于HTTP get的攻击，可以使用有限数量的机器甚至单个机器来降低Web服务器。 ​ 攻击者发送部分HTTP请求( 不是一个完整的request头部）这些请求持续快速地增长，缓慢地更新，永远不会关闭。攻击一直持续到所有可用的套接字被这些请求占用，Web服务器变得不可访问。攻击者的源地址通常不是伪造的。 ​ HTTP fragmentation attack，与懒猴类似，这种攻击的目标是通过长时间保持HTTP连接而不引发任何警报来降低Web服务器。 ​ 攻击者( bot ) 与Web服务器建立有效的HTTP连接。然后，它们将合法的HTTP数据包分解成微小的片段，并按照服务器超时允许的速度发送每个片段。使用这种方法，通过在每个bot上打开多个会话，攻击者可以只使用少数肉机就悄悄地让一个Web服务器崩溃。 ​ Slowpost attack，【42】wong等人提出了一个跟懒猴攻击非常相似的攻击，其通过缓慢发送HTTP_post请求来击溃Web服务器。 ​ 攻击者发送一个完整的HTTP头，它定义了消息体的‘内容-长度’字段，作为发送此请求的良性流量。然后它以每两分钟一个字节的速率发送数据来填充消息体。因此，服务器等待每个消息体完成，而Slowpost攻击迅速增长，导致Web服务器上的DDoS泛洪攻击。 ​ Slowreading attack，【43】Shekyan等人提出了另一种通过缓慢读取response来发起攻击的方式，而不是通过缓慢发送。 ​ 此攻击通过设置比目标服务器的发送缓冲区更小的接收窗口大小来达到目的。即使没有数据通信，TCP协议仍然保持开放的连接；因此，攻击者可以迫使服务器保持大量连接的开放，最终对服务器造成DDoS泛洪攻击。 BOTNET-BASED DDOS ATTACKS ​ 如前所述，僵尸网络是促使DDoS洪范攻击计算机网络或应用程序的主导机制。最近最令人头大的应用层DDoS泛洪攻击大多使用僵尸网络。在本节中，我们对当前僵尸网络架构以及已经用于发起DDoS泛洪攻击的工具进行了全面的研究 ​ 根据【32】Peng等人所述，当攻击者使用僵尸网络来制造DDoS攻击时，使得做出更有效的防御机制更困难的原因有二：一是大量的僵尸肉机可使得攻击者制造出的攻击流量规模更大，更具破坏性；二是肉机的IP一般是伪造的，很难回溯追踪。 ​ 僵尸网络包括master，handler和bots，如下图所示： ​ handlers是攻击者( 即master )用来与自己的肉机（即bots）间接通信的通信手段。例如，handlers可以安装在攻击者通信发送命令的一组折衷设备( 例如 ,网络服务器 )上。 ​ 然而，大多数安装的程序都留下了当前杀毒软件可以检测到的独特足迹。因此，当前攻击者使用其他方法( 如互联网中继聊天 IRC )与bot进行通信，以发送命令并控制它们。 ​ Bots就是被控制的肉机，其生成可以有成百上千种方法，根据其如何被攻击者控制，可以分类为： ​ IRC-bacsed: ​ IRC是一个互联网在线文本信息协议，其采用c-s架构，具有默认的通道，可以实现服务器间的通信。IRC可以通过多个服务器连接数百个客户端。利用IRC通道作为处理程序，攻击者可以利用合法的IRC端口向bot发送命令，使得对DDoS命令和控制结构的跟踪变得更加困难。 ​ 由于IRC服务通常具有庞大的数据量，攻击者因此可以轻易的隐藏自己。 ​ 另外，攻击者可以通过将恶意代码分片发送而轻松地分享文件。 ​ 此外，攻击者可以简单地登录到IRC服务器并查看所有可用bot的列表，而不是在其站点本地维护其列表。具有集中指挥和控制( C &amp; C )基础设施的僵尸网络(如基于IRC的僵尸网络)的主要局限性在于服务器是潜在的故障中枢。也就是说，如果防御者捕获了C &amp; C服务器，整个僵尸网络可能会关闭。一些基于IRC的著名僵尸网络工具多年来被开发并用于发起DDoS攻击如： ​ 【43】Trinity v3（UDP,TCP SYN, TCP ACK, and TCP NULflflood attacks） ​ 【47】Kaiten（UDP, TCP, SYN, and PUSH+ACH flflood attacks） ​ Web-based (a.k.a., HTTP-based)： ​ 最近，僵尸网络开始使用HTTP作为通信协议向僵尸网络发送命令，使得追踪DDoS命令和控制结构变得困难得多。基于Web的僵尸网络不像基于IRC的僵尸网络那样与C &amp; C服务器保持连接。相反，每个Web机器人定期使用Web请求下载指令。基于Web的僵尸网络更隐蔽，因为它们隐藏在合法的HTTP流量中。 ​ Bots通过复杂的PHP脚本进行配置和控制，它们通过HTTP或者HTTPS加密通信。 ​ 三个著名的Web-based 僵尸网络： ​ 【49】 BlackEnergy ​ 【50】 Ion Cannon (LOIC) ​ 【52】 Aldi DDOS DEFENSE: SCOPE AND CLASSIFICATION ​ 通常在检测到DDoS泛洪攻击时，除了断开受害者与网络并手动修复问题之外，没有什么可以做的。任何DDoS防御机制的最终目标都是尽快检测到它们，并尽可能地将它们阻止到源端。 ​ ​ ​ 上图显示了对DDoS检测和回应可以实施的阶段，如图所示，DDoS泛洪攻击类似于一个漏斗，在该漏斗中攻击流产生于一个分散区(即源)，形成漏斗的顶部。 ​ 可以看出，在漏斗底部检测DDoS攻击时相对容易的，所有网络流都可以在底部被观察到。相反地，相反，从攻击的单个源网络很难检测到攻击，除非从该源发起大量攻击流。 ​ 但是在检测的准确性和如何接近攻击源之间总是存在权衡的问题，预防和响应机制能够阻止或响应攻击。 ​ 此外，当响应机制( 例如 ,包过滤 )将攻击数据包更靠近攻击源时，当受害者受到DDoS攻击(即在DDoS攻击的中间)，到达受害者的正常数据包数量也会增加。否则，随着攻击流越接近受害者，数据包过滤机制就会丢弃更多受害者的合法数据包。 ​ 在本节中，我们利用两个判据对我们在第III节中提出的两类DDoS洪泛攻击的防御机制进行了分类。我们认为，这些分类标准对于设计稳健的防御解决方案非常重要。 ​ 分类的第一个准则是防御机制在攻击过程的哪里实现（Deployment location）： ​ 我们将针对传输层级的DDoS攻击的防御方法分为四类（见Fig.3.）： ​ source-based, destination-based,network-based, and hybrid (a.k.a. distributed) ​ 我们将针对应用级DDoS攻击的防御方法分为两类： ​ destination-based, and hybrid (a.k.a. distributed) ​ 由于应用层DDoS攻击的流量在第2层(交换机)和第3层(路由器)设备上不可访问，因此没有network-based的防御机制来抵御应用层DDoS攻击。 ​ 分类的第二个准则是DDoS防御机制面对可能的DDoS攻击响应的时间点： ​ 基于以上准则我们将基于传输层和基于应用层的的DDoS防御机制分为三类： ​ before the attack（attack prevention），during the attack（attack detection），after the attack(attack source identifification and response) ​ 因为对于DDoS攻击并没有一个“一刀切”的办法，一个综合的DDoS防御机制应该包括以上三个防御方法。 ps：由于本次调研重点在于探索对基于AI方法的DDoS防御机制，这里不做过深入探索，具体传统方法见原文。 DDOS DEFENSE: PERFORMANCE MEASUREMENT METRICS ​ 在这一部分，我们回顾和讨论了文献中发现的一些可以用来比较评估DDoS防御技术的度量和属性。 ​ 然后，在表III和表IV中，我们使用 defense strength (accuracy)、 scalability（可扩展性）、 delay（延迟）、 system performance degradation（系统性能退化）、implementation complexity（实现复杂度）等性能度量指标，基于deployment location定性地比较了传输层级DDoS攻击的防御机制和应用层级DDoS防御机制，以及这些防御机制是否被视为整体防御机制。 ​ 度量防御机制的标准如下： Defense Strength: 防御机制的强度可以通过各种度量来衡量，这取决于它能多好地预防、检测和阻止攻击。这些度量可以根据每个防御机制做出的决策或预测来定义。防御机制要么探测并应对攻击，要么错过攻击。根据他们的反应，有四个可能的结果如表II所示。 ​ 如表II所示，A，B，C，D分别表示为true negative，false negative，false positive，true positive，其实就是常见的混淆矩阵。 Accuracy是对检测结果的综合评价，是预测正确的比例占全部的比例。 \\[ Accuracy=\\frac{TP+TN}{P+N}=\\frac{TP+TN}{TN+TP+FP+FN} \\] Sensitivity是阳性样本中预测正确的比例： \\[ Sensitivity=\\frac{TP}{P}=\\frac{TP}{TP+FP} \\] Specificity是阴性样本中预测正确的比例： \\[ Specificity=\\frac{TN}{N}=\\frac{TN}{TN+FN} \\] Precision是预测为阳性的样本中预测正确的比例： \\[ Precision=\\frac{TP}{FP+TP} \\] Reliability or False positive rate是预测为阳性的样本中错误的比例： \\[ Reliability(False.positive.rate)=\\frac{FP}{FP+TP} \\] False negative rate是预测为阴性的样本中错误的比例： \\[ False\\,negative\\,rate=\\frac{FP}{FP+TP} \\] Compromise-ability: 攻击者能否利用防御机制，以便对整个系统发起攻击( 例如 , DDoS )? Delay in detection/response: 检测到/对攻击做出反应的时间。 System performance degradation: 一个防御机制是否会造成系统的功能性问题（例如内存短缺，CPU时间片短缺等），或者其是否需要额外的要求以完美运行。 Passive, reactive or proactive: 防御机制通过主动阻止攻击的发生来防御攻击，它是只对现有攻击作出反应，还是只在DDoS攻击发起后才采取行动。 Holistic defense: 一种整体防御机制，通过考虑所有需要的任务，以阻止DDoS攻击( 即既检测又响应 )。 Implementation complexity: 实现复杂性。 Usability: 是否user-friendly。 Deployment location: 正如我们前面提到的，部署位置是比较各种防御机制的另一个度量。每个位置都有各自的优缺点，使得一种机制优于另一种机制。 Scalability: 可扩展性，一个可扩展的防御机制可以有效地处理其攻击检测和响应职责，即使攻击者的数量和攻击流量都增加了。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"IDS","slug":"IDS","permalink":"http://example.com/tags/IDS/"},{"name":"DDoS","slug":"DDoS","permalink":"http://example.com/tags/DDoS/"}],"author":"Shaw"},{"title":"HydraText-Multi-objective Optimization for Adversarial Textual Attack","slug":"【论文阅读】HydraText Multi-objective Optimization for Adversarial Textual Attack","date":"2021-11-14T06:08:02.217Z","updated":"2022-07-15T08:30:58.064Z","comments":true,"path":"2021/11/14/【论文阅读】HydraText Multi-objective Optimization for Adversarial Textual Attack/","link":"","permalink":"http://example.com/2021/11/14/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91HydraText%20Multi-objective%20Optimization%20for%20Adversarial%20Textual%20Attack/","excerpt":"HydraText: Multi-objective Optimization for Adversarial Textual Attack 作者：Shengcai Liu，Ning Lu，Cheng Chen，Chao Qian，Ke Tang 时间：2021 ABSTRACT ​ 文字(text)（word-level）对抗样本黑盒攻击。在这项工作中，同时考虑攻击效率+可辨认性，并提出一种新的具有可证明性能保证的多优化方法(称为HydraText )，以实现具有高隐蔽性的成功攻击。 ​ 为了测试HydraText的功效，我们在score-based 和decision-based的黑盒攻击下，使用5个NLP模型+5个数据集。 （PS：[论文总结] Boundary Attack - 知乎 (zhihu.com)） ​ 一项人类观察评价研究表明，Hydra Text制作的对抗样本很好地保持了有效性和自然性。最后，这些实例还表现出良好的可迁移性，可以通过对抗训练给目标模型带来显著的鲁棒性提升。","text":"HydraText: Multi-objective Optimization for Adversarial Textual Attack 作者：Shengcai Liu，Ning Lu，Cheng Chen，Chao Qian，Ke Tang 时间：2021 ABSTRACT ​ 文字(text)（word-level）对抗样本黑盒攻击。在这项工作中，同时考虑攻击效率+可辨认性，并提出一种新的具有可证明性能保证的多优化方法(称为HydraText )，以实现具有高隐蔽性的成功攻击。 ​ 为了测试HydraText的功效，我们在score-based 和decision-based的黑盒攻击下，使用5个NLP模型+5个数据集。 （PS：[论文总结] Boundary Attack - 知乎 (zhihu.com)） ​ 一项人类观察评价研究表明，Hydra Text制作的对抗样本很好地保持了有效性和自然性。最后，这些实例还表现出良好的可迁移性，可以通过对抗训练给目标模型带来显著的鲁棒性提升。 INTRODUCTION ​ 我们仔细地设计了目标函数，并进一步构建了一个多目标优化问题（multi-objective optimization problem，MOP），该问题一旦被解决，将产生与原始文本相似度高的单个成功对抗样本。 ​ 然后我们原创了一个多目标优化方法（ multi-objective optimization approach），叫做HydraText。这个名字的灵感来自于海蛇许德拉，这是一种神话动物，它使用多个头部攻击对手。它可以同时用在score-based 和decision-based的黑盒攻击下。 ​ METHODS ​ 基于word-level 的替换操作。每个单词有一个自己的候选表，然后将每个单词与候选表中被选中的词替换（也可以不选，原单词不变）。 ​ 但这样的方法有个问题，如下图： ​ ​ 如图所示，句子的语义与替换的单词数量是成反比的，上文需要考虑的准确率+可辨认性二者其实是互相矛盾的。为了解决这个问题，我们在生成的过程中也考虑Xadv的修改率，使用MOP来解决它。 ​ 1.The HydraText Approach EXPERIMENTS 1. Datasets and Target Models ​ 模型种类：文本分类和文本推理 ​ 三个数据集：AG News，IMDB ， Movie Reviews，Stanford Natural Language Inference，multi-genre NLI corpus（前三个文本分类，后三个文本推理） ​ 两个模型：WordCNN，WordLSTM，BERT base-uncased，ESIM ，Infersent ，BERT base-uncased(前三个文本分类，后三个文本推理) 2.Baselines and Algorithm ​ 攻击方法：PSO,GA,TextFooler,PWWS,GADe(baseline) 3.Evaluation ​ 以攻击成功的百分率来判定攻击能力。 ​ 以修改百分率和语义相似性来判定攻击的可辨识性。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"}],"author":"Shaw"},{"title":"Semantic Host-free Trojan Attack","slug":"【论文阅读】Semantic Host-free Trojan Attack","date":"2021-11-06T07:56:09.258Z","updated":"2022-07-16T02:08:52.554Z","comments":true,"path":"2021/11/06/【论文阅读】Semantic Host-free Trojan Attack/","link":"","permalink":"http://example.com/2021/11/06/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Semantic%20Host-free%20Trojan%20Attack/","excerpt":"Semantic Host-free Trojan Attack 作者：Haripriya Harikumar , Kien Do, Santu Rana , Sunil Gupta , Svetha Venkatesh（迪肯大学.澳大利亚） 时间：2021.10.27 ABSTRACT ​ 在本文中，我们提出了一种新颖的host-free木马攻击，其触发器(trigger)固定在语义空间(semantic)，但不一定在像素空间(pixel)。 ​ 与现有的木马攻击使用干净的输入图像作为宿主来携带小的、没有意义的trigger不同，我们的攻击将trigger看作是属于语义上有意义的对象类的整个图像。 ​ 由于在我们的攻击中，与任何特定的固定模式相比，分类器被鼓励记忆触发图像的抽象语义。因此它可以在以后由语义相似但看起来不同的图像触发。这使得我们的攻击更实际地被应用于现实世界中，更难以防御。广泛的实验结果表明，仅用少量的特洛伊木马模式进行训练，我们的攻击能很好地推广到同一特洛伊木马类的新模式，并且可以绕过目前的防御方法。","text":"Semantic Host-free Trojan Attack 作者：Haripriya Harikumar , Kien Do, Santu Rana , Sunil Gupta , Svetha Venkatesh（迪肯大学.澳大利亚） 时间：2021.10.27 ABSTRACT ​ 在本文中，我们提出了一种新颖的host-free木马攻击，其触发器(trigger)固定在语义空间(semantic)，但不一定在像素空间(pixel)。 ​ 与现有的木马攻击使用干净的输入图像作为宿主来携带小的、没有意义的trigger不同，我们的攻击将trigger看作是属于语义上有意义的对象类的整个图像。 ​ 由于在我们的攻击中，与任何特定的固定模式相比，分类器被鼓励记忆触发图像的抽象语义。因此它可以在以后由语义相似但看起来不同的图像触发。这使得我们的攻击更实际地被应用于现实世界中，更难以防御。广泛的实验结果表明，仅用少量的特洛伊木马模式进行训练，我们的攻击能很好地推广到同一特洛伊木马类的新模式，并且可以绕过目前的防御方法。 ### INTRODUCTION ​ 提出了一个后门攻击，semantic host-free backdoors。 ​ 后门攻击综述：(20条消息) 深度学习后门攻防综述_Yale的博客-CSDN博客_后门攻击 ​ METHOD ​ 实现方式：数据投毒。 ​","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"}],"author":"Shaw"},{"title":"Intrusion detection system-A comprehensive review","slug":"【论文阅读】Intrusion detection system A comprehensive review","date":"2021-11-03T01:58:09.557Z","updated":"2022-07-16T02:09:04.067Z","comments":true,"path":"2021/11/03/【论文阅读】Intrusion detection system A comprehensive review/","link":"","permalink":"http://example.com/2021/11/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Intrusion%20detection%20system%20A%20comprehensive%20review/","excerpt":"Intrusion detection system: A comprehensive review 作者：Hung-Jen Liao a , Chun-Hung Richard Lin a,n , Ying-Chih Lin a,b , Kuang-Yuan Tung a（国立中山大学，正修科技大学） 时间：2012 ABSTRACT ​ 一个IDS综述。 PS：(17条消息) 防火墙、IDS和IPS之间的区别（浅谈）_淡风wisdon－大大的博客-CSDN博客","text":"Intrusion detection system: A comprehensive review 作者：Hung-Jen Liao a , Chun-Hung Richard Lin a,n , Ying-Chih Lin a,b , Kuang-Yuan Tung a（国立中山大学，正修科技大学） 时间：2012 ABSTRACT ​ 一个IDS综述。 PS：(17条消息) 防火墙、IDS和IPS之间的区别（浅谈）_淡风wisdon－大大的博客-CSDN博客 ### INTRODUCTION ​ CIA：Confifidentiality, Integrity and Availability， ​ Instrusion: 针对CIA的破坏行为，或者绕过计算机或网络安全机制的行为。 ​ Instrusion detection: 是监视计算机系统或网络中发生的事件，并分析它们以发现入侵迹象的过程。 ​ Instrusion detection sysytem(IDS): 实现instrusion detection自动化的软件或硬件。 ​ Instrusion prevention system(IPS): 不仅有IDS的监控功能，还可以阻止可能的突发安全事件。在少数文章中，入侵检测与防御系统( IDPS )和入侵防御系统( IPS )是同义词，其中IDPS一词在安全界很少使用。 DETECTION METHODOLOGIES ​ Detection的方法一共分为三类：Signature-based Detection (SD), Anomaly-based Detection (AD) and Stateful Protocol Analysis (SPA)。 1. SD（特征检测）: ​ Signature-based Detection，特征检测。将已知的patterns与捕获的事件进行比较，从而发现可能的入侵。因为使用特定攻击或者系统漏洞所积累下的知识，SD又被称为Knowledge-based Detection 或者 Misuse Detection。 2. AD（异常检测）： ​ Anomaly-based detection，异常检测。一个异常（anomaly）指的是与已知行为相异的地方。Profiles表示定期从活动，网络连接中监视的正常或特定的行为文件，profile可以是静态的也可以是动态的，并且从许多特性中生成。例如，登录失败，处理器的使用，邮件的发送数量等。 ​ 接下来，AD 比较器就将正常的profile与观察到的事件相比较，以此辨别出显著的攻击。AD又被称为Behavior-based Detection。 ​ 一些AD的例子，例如，企图闯入、伪装、合法用户渗透、拒绝服务( DOS )、特洛伊木马等。 3. SPA（状态协议分析）： ​ Stateful Protocol Analysis，状态协议分析。Stateful指的是IDS可以知晓并追踪协议的状态（举例，将请求与答复配对）。 ​ 尽管SPA与AD很像，二者其实完全不同。AD采用预加载的网络或者特定域名的profile，然而SPA依赖于供应商开发的特定协议通用profile。通常，SPA中的网络协议模型最初基于国际标准组织(例如IETF )的协议标准。SPA也被称为Specifification-based Detection（基于规格的检测）。 大多数IDS使用多种方法来提供更广泛和准确的检测。 DETECTION APPROACHES ​ 此文将已有的方法分为了5类：Statistics-based, Pattern-based, Rule-based, State-based and Heuristic-based。 ​ 由上图所示，其中，Time series指的是是否考虑了time series behavior。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"IDS","slug":"IDS","permalink":"http://example.com/tags/IDS/"}],"author":"Shaw"},{"title":"Def-IDS An Ensemble Defense Mechanism Against Adversarial Attacks for Deep Learning-based Network Intrusion Detection","slug":"【论文阅读】Def-IDS An Ensemble Defense Mechanism Against Adversarial Attacks for Deep Learning-based Network Intrusion Detection","date":"2021-11-01T13:16:57.082Z","updated":"2022-07-16T02:09:45.988Z","comments":true,"path":"2021/11/01/【论文阅读】Def-IDS An Ensemble Defense Mechanism Against Adversarial Attacks for Deep Learning-based Network Intrusion Detection/","link":"","permalink":"http://example.com/2021/11/01/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Def-IDS%20An%20Ensemble%20Defense%20Mechanism%20Against%20Adversarial%20Attacks%20for%20Deep%20Learning-based%20Network%20Intrusion%20Detection/","excerpt":"Def-IDS: An Ensemble Defense Mechanism Against Adversarial Attacks for Deep Learning-based Network Intrusion Detection 作者：Jianyu Wang，Jianli Pan，Ismail AlQerm，（密苏里大学圣路易斯分校，重庆大学） 时间：2021 ICCCN，ccf--C类 ABSTRACT ​ 提出了Def-IDS，一个为NIDS准备的组合防御机制。它是一个由两个模块组成的训练框架，组合了multi-class generative adversarial networks（MGANs）和multi-soutce adversarial retraining（MAT）。 ​ 在CSE-CIC-IDS2018数据集上测试了该机制，并与3个其它方法进行了比较。结果表明Def-IDS可以以更高的precision, recall, F1 score, and accuracy来识别对抗样本。","text":"Def-IDS: An Ensemble Defense Mechanism Against Adversarial Attacks for Deep Learning-based Network Intrusion Detection 作者：Jianyu Wang，Jianli Pan，Ismail AlQerm，（密苏里大学圣路易斯分校，重庆大学） 时间：2021 ICCCN，ccf--C类 ABSTRACT ​ 提出了Def-IDS，一个为NIDS准备的组合防御机制。它是一个由两个模块组成的训练框架，组合了multi-class generative adversarial networks（MGANs）和multi-soutce adversarial retraining（MAT）。 ​ 在CSE-CIC-IDS2018数据集上测试了该机制，并与3个其它方法进行了比较。结果表明Def-IDS可以以更高的precision, recall, F1 score, and accuracy来识别对抗样本。 ### INTRODUCTION ​ Internet of Things(IoT):物联网 ​ intrusion detection systems (NIDS) ​ 提出了一个整合基于对抗训练的防御机制，用于提升DL-based的intrusion detectors的鲁棒性。 ​ 4个贡献： 1. 模型由两个模块组成，组合了multi-class generative adversarial networks（MGANs）和multi-soutce adversarial retraining（MAT），可以在保证准确率的前提下对抗攻击； 2. MGANs可以通过同时过采样多类入侵来增强原始训练数据集，以减少训练与真实数据分布之间的差距。通过使用提升过的数据进行训练，detector的对已知和未知攻击的鲁棒性更强； 3. MAT通过投喂多种不同的对抗样本来retraining，MAT不仅对抗某种特定的攻击，并且可以一定程度抵御对样样本的转移性； 4. 我们进行了一些state-of-the-art攻击并且在CSE-CIC-IDS2018数据集上测试了该机制，结果很好。 RELATED WORK ADVERSARIAL ATTACK THREAT MODELS 采用的攻击方法：FGSM，BIM，DeepFool，JSMA PROPOSED DEF-IDS DEFENSE MECHANISM 1. Mechanism Overview 2. Module 1: Multi-class GAN-based Retraining 3. Module 2: Multi-source Adversarial Retraining 4. Ensemble Adversarial Retraining EVALUATION 1. Dataset and Metrics ​ 数据集：CSE-CIC-IDS2018（CIC出版）（通用） ​ 与其他过时的数据集相比，其含有综合性的攻击方法和更平衡的数据。 ​ 其含有Brute-force, Heartbleed,Botnet, DoS, DDoS, Web attacks 和 infifiltration of the network共7种恶意流量。 ​ 数据处理： 使用Min-Max standardization将所有特征的值映射入[0,1]； 有四个特征有太多空值或者无限值（dstport, protocol, flflow byts/s, flflow pkts/s），有一个特征（timestamp）与流量无关，将这5个特征剔除；还剩下76个特征。 training,validation,test = 8:1:1，随机划分。 ​ Detector的评价方法： ​ 混淆矩阵。 2. Baseline Detector Implementation 2.1 Detector Implementation ​ 选取baseline detector Cbase。其由一个输入层，两个隐藏层和一个输出层组成（76-128-64-8）。 ​ 隐藏层都是全连接层+ReLU。 ​ 输出层使用Softmax。 ​ 代码用keras写的，系统Ubuntu 18.04,3.6GHz CPU和16GB内存。 ​ 优化器用Adam，学习率0.001,20个epoch。 ​ 在训练过程中，进行十次交叉验证并计算平均度量值。 ​ 训练结束后，利用测试数据集对Cbase进行评估。 2.2 Adversarial Attacks against Baseline Classififier ​ 使用python库foolbox来生成对抗样本； ​ FGSM，BIM，DeepFool，JSMA四种攻击方法都使用，具体效果如下图所示： ​ 3. Def-IDS Defense Evaluation Cgan是使用GAN生成的样本再训练的detector; Cat是使用9:1的纯净数据：恶意数据再训练出的detector; Censem是二者的结合. 4. Comparison with Other Works 5. Cost Estimation","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"}],"author":"Shaw"},{"title":"Crafting Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL","slug":"【论文阅读】Crafting Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL","date":"2021-10-30T03:06:44.421Z","updated":"2022-07-16T02:09:21.552Z","comments":true,"path":"2021/10/30/【论文阅读】Crafting Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL/","link":"","permalink":"http://example.com/2021/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Crafting%20Adversarial%20Example%20to%20Bypass%20Flow-&ML-%20based%20Botnet%20Detector%20via%20RL/","excerpt":"Crafting Adversarial Example to Bypass Flow-&amp;ML- based Botnet Detector via RL 作者：Junnan Wang，Qixu Liu，Di Wu，Ying Dong，Xiang Cui（中国科学院大学，华为科技，北京维纳斯纲科技，广州大学） 时间：2021.10.6 会议：RAID(CCF_B) 1. Botnet(僵尸网络)： 1.1 定义： ​ Botnet = robot + network。 ----参考《软件安全》.彭国军 1.2 如何攻击？ ​ 一个僵尸网络的生存周期包括形成、C&amp;C、攻击、后攻击四个阶段。 ​ 形成阶段由攻击者入侵有漏洞的主机，并在其上执行恶意程序，使之成为僵尸主机。 ​ 一旦成为僵尸主机之后，botmaster会通过各种方式与之通信。 ​ 之后根据botmaster的指令执行攻击行为。后攻击阶段是指botmaster对僵尸网络进行升级更新。 2. Botnet Detector(僵尸网络检测器)： 2.1 传统方法： ​ 从检测原理上来说，大致可以分为三类方法： ·行为特征统计分析 ·bot行为仿真以监控 ·流量数据特征匹配 ​ 传统的检测僵尸网络的方法一般在形成、攻击阶段，利用僵尸主机存在的行为特征，例如通信的数据内容。一些基于网络流量行为分析的方法可以检测僵尸网络，主要是从通信流量特征的角度去检测的，例如流量的通信周期，这种方法可以检测出一些加密的僵尸主机流量，同时还可以检测出新型的僵尸网络。 ----参考：解析：僵尸网络（Botnet）的检测方法-西湖泛舟-ChinaUnix博客 ABSTRACT ​ 提出了一个基于RL的方法来对基于ML的僵尸网络追踪器做逃逸攻击，并且可以保留僵尸网络的恶意功能。 ​ 黑盒攻击，不用改变追踪器本身。","text":"Crafting Adversarial Example to Bypass Flow-&amp;ML- based Botnet Detector via RL 作者：Junnan Wang，Qixu Liu，Di Wu，Ying Dong，Xiang Cui（中国科学院大学，华为科技，北京维纳斯纲科技，广州大学） 时间：2021.10.6 会议：RAID(CCF_B) 1. Botnet(僵尸网络)： 1.1 定义： ​ Botnet = robot + network。 ----参考《软件安全》.彭国军 1.2 如何攻击？ ​ 一个僵尸网络的生存周期包括形成、C&amp;C、攻击、后攻击四个阶段。 ​ 形成阶段由攻击者入侵有漏洞的主机，并在其上执行恶意程序，使之成为僵尸主机。 ​ 一旦成为僵尸主机之后，botmaster会通过各种方式与之通信。 ​ 之后根据botmaster的指令执行攻击行为。后攻击阶段是指botmaster对僵尸网络进行升级更新。 2. Botnet Detector(僵尸网络检测器)： 2.1 传统方法： ​ 从检测原理上来说，大致可以分为三类方法： ·行为特征统计分析 ·bot行为仿真以监控 ·流量数据特征匹配 ​ 传统的检测僵尸网络的方法一般在形成、攻击阶段，利用僵尸主机存在的行为特征，例如通信的数据内容。一些基于网络流量行为分析的方法可以检测僵尸网络，主要是从通信流量特征的角度去检测的，例如流量的通信周期，这种方法可以检测出一些加密的僵尸主机流量，同时还可以检测出新型的僵尸网络。 ----参考：解析：僵尸网络（Botnet）的检测方法-西湖泛舟-ChinaUnix博客 ABSTRACT ​ 提出了一个基于RL的方法来对基于ML的僵尸网络追踪器做逃逸攻击，并且可以保留僵尸网络的恶意功能。 ​ 黑盒攻击，不用改变追踪器本身。 ### INTRODUCTION ​ 训练一个RLagent，让其通过与追踪器的交流反馈自己学习如何扰动样本。 ​ 为了确保功能的保留，我们设计了一个包含14个增量操作的操作空间，每个操作只向原始流中添加一个精心编制的数据包，以尝试更改一些流级特性。检测器认为这些特征具有区分性，但这可能不是良性交通的因果指标。 ​ 此外，添加数据包是传输层的增量操作，而恶意数据一般封装在应用层。 ​ 这种攻击方法的优点： 1. 黑盒攻击； 2. 它具有通用性，不论探测器的损失函数是否可微，都可以使用； 3. 即插即用，RL智能体可以作为网络代理存在，逃逸成本低并且适用于任何botnet家族。 ​ 主要贡献： 1. 提出一个黑盒攻击方法； 2. 在RL框架中设计了一些列通用动作空间，这些动作都是添加操作，在可以逃逸的前提下保证了恶意样本的功能性； 3. 我们演示了如何训练和部署我们的系统以避免在精心构建的僵尸网络流数据集上进行ML检测，并综合评估框架的逃避性能、时间开销和通用性。 RELATED WORK Botnet Evasion: 传统botnet逃逸方法：加密网络流；在TCP/IP协议簇的冗余字段中隐藏C &amp; C信息(command and control)；使用online-social-networks(OSN)来构建隐藏的通道。 ML-based逃逸方法： Feature space attack：指的是只能生成traffic对抗特征向量的方法。但是，考虑到traffic样本映射到特征向量的过程是不可逆的，这样的攻击不能造成实际的安全威胁，只能用来证明基于ML的检测器的脆弱性。 End-to-end attack：指的是可以生成真正的traffic数据的方法。 【35】利用了GAN来模仿facebook聊天网络的traffic以此绕过自适应IPS。 【36】利用了GAN来生成尽量真实的traffic，以此来提高数据集的质量，解决数据不平衡问题。 THREAT MODEL AND SYSTEM FRAMEWORK Threat Model 攻击者的目的：生成对抗样本，隐藏botnet flow。 攻击者的信息：1. 攻击者理解目标网络可能被流等级（flow-level）ML网络检测系统保护；2. 攻击者不需要知道detector的算法，参数，特征或训练数据等信息。 攻击者的能力：1. 攻击者只有能力修改测试集，并不能改变detector的训练集；2. 同时，我们假设攻击者可以持续访问detector，从检测器中获取二进制预测结果。 System Design ​ 见图即可，简单的RL学习模型。 RL Algorithm 选择了（value-based）DQN和SARSA，都用。 Action Space Q：如何在不影响原来功能的情况下添加扰动？ A：因为botnet内容在应用层，故可以对传输层进行扰动。（PS：这样确实不会改变功能，但是应用层的恶意特征不会仍被detector检测到吗？） Q：如何确定哪个特征该进行扰动？ A：考虑到动作设计的困难，从僵尸网络检测中常用的特征集合中选取18个特征。 ​ 由上述特征，基于botnet和normal flow的差异，action space包含了14个动作，这些动作可以影响以上的统计特征，例如简单修改数据包的时间戳，或者添加构建的新数据包。 ​ 当在构建新数据包时，考虑三个地方：时间戳，方向，包的大小。 ​ 14个动作被分成了5类： 具体见原文。 State Space ​ detector返回的二进制信息很难直接使用，需要有一个状态生成器来生成供agent使用的state。 ​ 这里使用堆叠自编码器（Stacked Autoencoder，SAE）来自动提取botnet flow的特征，然后将其返回给agent以作为state。 ​ 将每个botnet flow的前1024个字节作为SAE的输入，经过一些epoch的训练，SAE就可以自动地从botnet flow中学习到一个256维度的state vector。 EXPERIMENTAL SETUP Implementation 系统的位置如下： 作为BotMaster的一个代理存在。 Dataset 两个公开数据集：CTU，ISOT。 然后做一下数据处理： 合并属于同一botnet 家族的样本，如果某个pcap包太大，就舍弃； 将pcap包切片； 匿名化，将ip,mac等包中独一无二的东西随机化，以避免影响。 Detector ​ 选取了两个state-of-art的detector: the composite DL detection model combining CNN with LSTM(BotCatcher detection model)，the non-differentiable ML detection model based on XGBoost(XGBoost detection model)。 ​ RESULTS Evasion performance 将DQM,SARSA与BotCatcher,XGBoost两两组合： 逃逸率如上图所示，可以看到，即使是随机扰动都有一定的逃逸率。 不同测试集效果差异很大： 1. 数据包可能过大（storm），导致对时间戳做修改等操作对结果的影响很小； 2. 数据包的特征跟其它数据集差别很大，导致模型难以在有限的步骤时间里改变足够多的特征。 Time performances Dominant actions Dominant actions指的是agent在创建对抗样本时采用的最频繁的操作。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"RL","slug":"RL","permalink":"http://example.com/tags/RL/"},{"name":"Botnet","slug":"Botnet","permalink":"http://example.com/tags/Botnet/"}],"author":"Shaw"},{"title":"《最后的问题》","slug":"The Last Question","date":"2021-10-12T10:03:37.283Z","updated":"2022-07-16T02:10:07.886Z","comments":true,"path":"2021/10/12/The Last Question/","link":"","permalink":"http://example.com/2021/10/12/The%20Last%20Question/","excerpt":"THE LAST QUESTION 最后的问题第一次被半开玩笑地提出是在2061年的5月21日。那时人类文明刚刚步入曙光中。这个问题源起于酒酣之中一个五美元的赌，它是这么发生的： 亚历山大•阿代尔与贝特伦•卢泊夫是Multivac的两个忠实的管理员。像任何其他人一样，他们知道在那台巨大的计算机数英里冰冷、闪烁、滴答作响的面庞后藏着什么。那些电子回路早已发展到任何个别的人都无法完全掌握的地步，但他们至少对它的大致蓝图有个基本的概念。 Multivac能自我调节和自我修正。这对它是必要的，因为人类当中没有谁能够快甚至够好地对它进行调节和修正。所以实际上阿代尔与卢泊夫对这个庞然大 物只进行一些非常轻松和肤浅的管理，任何其他人也都只能做到这个程度。他们给它输送数据，根据它所需的格式修改问题，然后翻译给出的答案。当然，他们以及 其他管理员们完全有资格分享属于Multivac的荣誉。 几十年中，在Multivac的帮助下人类建造了宇宙飞船，计算出航行路径，从而得以登陆月球、火星和金星。但是更远的航行需要大量的能量，地球上可怜的资源不足以支持这些飞船。尽管人类不断地提高煤炭和核能的利用效率，但煤和铀都是有限的。 但是慢慢地Multivac学会了如何从根本上解决某些深层次问题。2061年5月14日，理论成为了现实。 太阳的能量被储存和转化，得以被全球规模地直接利用。整个地球熄灭了燃烧的煤炭，关闭了核反应炉，打开了连接到那个小小的太阳能空间站的开关。这个空间站直径一英里，在到月球的距离一半处环绕着地球。看不见的太阳的光束支撑着整个地球社会的运行。 七天的庆祝还不足以暗淡这创举的光辉。阿代尔与卢泊夫总算逃脱了公众事务，悄悄地相聚在这个谁也想不到的荒僻的地下室。在这里Multivac埋藏着的庞 大身躯露出了一部分。它正独自闲暇地整理着数据，发出满足的、慵懒的滴答声——它也得到了假期。他们了解这一点，一开始他们并没打算打扰它。 他们带来了一瓶酒。这会儿他们想做的只是在一起，喝喝酒，放松放松。 你想一想就会觉得很神奇，”阿代尔说。他宽阔的脸庞已有了疲倦的纹路。他慢慢地用玻璃棒搅动着酒，看着冰块笨拙地滑动。“从此我们所用的所有能量都是免费的。只要我们愿意，我们能把地球熔化成一颗液态大铁球——还能毫不在乎花掉的能量。够我们永远永远永远用下去的能量。” 卢泊夫将头歪向一边，这是当他想要反驳对方时的习惯动作。他现在确实想要反驳，部分原因是他在负责拿着冰和杯子。他说：“不是永远。” “哦去你的，差不多就是永远。直到太阳完蛋，老贝。” “那就不是永远。” “好吧。几十亿年，可能一百亿年，满意了吧？” 卢泊夫用手梳着他稀薄的头发，仿佛要确认还剩下了一些。他缓缓地抿着自己的酒说，“一百亿年也不是永远。” “但对我们来说是够了，不是吗？” “煤和铀对我们来说也够了。” “好好好，但是现在我们能把宇宙飞船连接到太阳能电站，然后飞到冥王星又飞回来一百万次而不用担心燃料。靠煤和铀你就做不到。不信去问问Multivac。” “我不用问它。我知道。”","text":"THE LAST QUESTION 最后的问题第一次被半开玩笑地提出是在2061年的5月21日。那时人类文明刚刚步入曙光中。这个问题源起于酒酣之中一个五美元的赌，它是这么发生的： 亚历山大•阿代尔与贝特伦•卢泊夫是Multivac的两个忠实的管理员。像任何其他人一样，他们知道在那台巨大的计算机数英里冰冷、闪烁、滴答作响的面庞后藏着什么。那些电子回路早已发展到任何个别的人都无法完全掌握的地步，但他们至少对它的大致蓝图有个基本的概念。 Multivac能自我调节和自我修正。这对它是必要的，因为人类当中没有谁能够快甚至够好地对它进行调节和修正。所以实际上阿代尔与卢泊夫对这个庞然大 物只进行一些非常轻松和肤浅的管理，任何其他人也都只能做到这个程度。他们给它输送数据，根据它所需的格式修改问题，然后翻译给出的答案。当然，他们以及 其他管理员们完全有资格分享属于Multivac的荣誉。 几十年中，在Multivac的帮助下人类建造了宇宙飞船，计算出航行路径，从而得以登陆月球、火星和金星。但是更远的航行需要大量的能量，地球上可怜的资源不足以支持这些飞船。尽管人类不断地提高煤炭和核能的利用效率，但煤和铀都是有限的。 但是慢慢地Multivac学会了如何从根本上解决某些深层次问题。2061年5月14日，理论成为了现实。 太阳的能量被储存和转化，得以被全球规模地直接利用。整个地球熄灭了燃烧的煤炭，关闭了核反应炉，打开了连接到那个小小的太阳能空间站的开关。这个空间站直径一英里，在到月球的距离一半处环绕着地球。看不见的太阳的光束支撑着整个地球社会的运行。 七天的庆祝还不足以暗淡这创举的光辉。阿代尔与卢泊夫总算逃脱了公众事务，悄悄地相聚在这个谁也想不到的荒僻的地下室。在这里Multivac埋藏着的庞 大身躯露出了一部分。它正独自闲暇地整理着数据，发出满足的、慵懒的滴答声——它也得到了假期。他们了解这一点，一开始他们并没打算打扰它。 他们带来了一瓶酒。这会儿他们想做的只是在一起，喝喝酒，放松放松。 你想一想就会觉得很神奇，”阿代尔说。他宽阔的脸庞已有了疲倦的纹路。他慢慢地用玻璃棒搅动着酒，看着冰块笨拙地滑动。“从此我们所用的所有能量都是免费的。只要我们愿意，我们能把地球熔化成一颗液态大铁球——还能毫不在乎花掉的能量。够我们永远永远永远用下去的能量。” 卢泊夫将头歪向一边，这是当他想要反驳对方时的习惯动作。他现在确实想要反驳，部分原因是他在负责拿着冰和杯子。他说：“不是永远。” “哦去你的，差不多就是永远。直到太阳完蛋，老贝。” “那就不是永远。” “好吧。几十亿年，可能一百亿年，满意了吧？” 卢泊夫用手梳着他稀薄的头发，仿佛要确认还剩下了一些。他缓缓地抿着自己的酒说，“一百亿年也不是永远。” “但对我们来说是够了，不是吗？” “煤和铀对我们来说也够了。” “好好好，但是现在我们能把宇宙飞船连接到太阳能电站，然后飞到冥王星又飞回来一百万次而不用担心燃料。靠煤和铀你就做不到。不信去问问Multivac。” “我不用问它。我知道。” “那就不要小看Multivac为我们做的事，”阿代尔怒道，“它做得很好。” “谁说它做得不好？我是说太阳不能永远燃烧下去，我只是这个意思。我们在一百亿年内可以高枕无忧，但是然后呢？”卢泊夫用略微颤抖的手指指着对方，“不要说我们换另外一个太阳。” 片刻的沉默。阿代尔偶尔将酒杯放到唇边，而卢泊夫慢慢地闭上了眼睛。两人都在休息。 然后卢泊夫突然睁开眼，“你在想当我们的太阳没了就换另外一个太阳，是吧？” “我没这么想。” “你就是这么想的。你的逻辑不行，这就是你的问题。你就像故事里说的那个人一样，碰上了雨就跑到树林里躲在一棵树下。他可不担心，是吧，因为他以为当这棵树淋得太湿的时候他只要跑到另一棵树下就行。” “我明白了，”阿代尔说，“别嚷嚷。太阳完蛋了，其他的也都会完蛋。” “完全正确，”卢泊夫嘟哝道，“一切都在起初那个宇宙大爆炸中有个开始，不管那到底是怎么回事。当所有的恒星都熄灭了，一切也都会有个结束。有的星星熄灭 得比别的早。像那些该死的巨星维持不了一亿年。我们的太阳能持续一百亿年，矮星再怎么样最多也只有两千亿年。一万亿年后一切都是一片漆黑。熵必须增加到最 大值，就是这样。” “我非常明白什么是熵，”阿代尔维护着他的自尊。 “你明白个屁。” “我跟你知道的一样多。” “那你该知道某一天所有的东西都会耗光。” “是是是。谁说它们不会呢？” “你说的，你这个糊涂虫。你说我们有永远用不完的能量。你说的‘永远’。” 现在轮到阿代尔反驳了。他说：“也许有一天我们能让一切从头开始。” “绝不可能。” “为什么？总有那么一天的。” “没有。” “问问Multivac。” “你去问Multivac。你敢吗？我赌五美元它说这不可能。” 阿代尔刚刚醉到愿意一试，又刚刚足够清醒到能拼写出问问题需要的符号和算式。这个问题用文字来表达就是：人类是否有一天能不需要净损耗能量而在恒星衰竭之后将其恢复到全盛时期？ 或者更简明地这样说：怎样使宇宙的总熵大幅度地降低？ Multivac陷入了静止和沉默。缓慢闪烁的灯光熄灭了，深处传来的电路的滴答声停止了。 正当这两位被吓坏的技术员感到他们无法再屏住呼吸时，忽然间与Multivac相连的打字机开始运作起来。它打出几个字：数据不足，无法作答。 “赌不成了。”卢泊夫悄声道。他们匆忙离开了。 到了第二天早晨，两人头晕脑胀，口干舌燥，把这件事给忘了。 ------------------------------------------------------- 贾诺德、贾诺汀和贾诺蒂I、贾诺蒂II注视着屏幕中变幻的星空影像。飞船在超越时间的一瞬中穿越了超时空，均匀分布的星群立刻被一个明亮的圆盘取代。它弹珠大小，占据着屏幕的中心。 “那就是X-23，”贾诺德自信地说。他紧握着的瘦削的手背在身后，指节发白。 两个小贾诺蒂都是女孩。她们一生中第一次经历超时空飞行，清晰地感到那种片刻的恶心[注]。她们悄声地嘻笑着，疯狂地绕着她们的母亲互相追逐，一边尖叫：“我们到X-23了——我们到X-23了——我们——” “孩子们，别闹了！”贾诺汀严厉地说。“你确定吗，贾诺德？” “有什么不确定的？”贾诺德瞟了一眼天花板上凸出的那块毫不起眼的金属。它从房间的一头延伸到另一头，两端埋入墙壁中。它和整个飞船一样长。 贾诺德对这条厚厚的金属棒几乎一无所知。他只知道它叫做Microvac，你可以问它任何问题，而平时它控制着飞船飞向目的地，从不同的银河系能量分站向飞船输送能量，并完成进行超时空跳跃的计算。 贾诺德一家只需要住在飞船舒适的居住区等待。曾经有人告诉贾诺德，“Microvac”词尾的“ac”是古英语中“automatic computer，智能电脑”的缩写。但他差不多连这都忘了。 贾诺汀看着视屏，眼睛有些湿润。“没办法。想到离开了地球我感觉怪怪的。” “天哪，为什么？”贾诺德问。“我们在那儿什么也没有。我们在X-23上会拥有一切。你并不孤单，你又不是那些拓荒者。这个行星上已经有超过一百万人了。 天哪，我们的曾孙们会得去找新的星球，因为那时X-23会太挤了。”他想了一会，说：“告诉你，人口增长这么快，幸亏电脑实现了星际旅行。” “我知道，我知道。”贾诺汀难过地回答。 贾诺蒂I马上说道：“我们的Microvac是世界上最好的Microvac。” “我也是这么想的。”贾诺德抚弄着她的头发说。 能拥有一台自己的Microvac的感觉非常好。贾诺德很高兴他属于他们这一代人。在他父亲年轻的时候，电脑都是占地一百平方英里的巨大机器。一个星球只 有一台，被称作行星AC。一千年来它们的体积逐步地增加，然后忽然间缩小了，因为分子阀取代了晶体管，使得最大的行星AC都缩小到了只有一艘飞船的一半体 积。 每当想到这件事贾诺德总是感到飘飘然：他的Microvac比那台古老原始的首次驯服了太阳的Multivac要精密好几倍，而且和第一台解决了超时空传送问题从而实现了星际航行的地球行星AC（最大的行星AC）一样精密。 “这么多的恒星，这么多的行星。”贾诺汀想着心事，叹息道。“我想人们会永远不断地出发去找新的行星，就像我们现在这样。” “不是永远，”贾诺德笑了一笑说。“有一天这一切都会停下来，但那是在几十亿年之后了。好几十亿年。即使是星星也会耗尽，你知道的。熵必须不断增大。” “爸爸，熵是什么？”贾诺蒂II喊道。 “小宝贝，熵，就是一个代表着宇宙消耗掉了多少的词。什么东西都会消耗，知道吗，就像你那个会走路会说话的小机器人，记得吧？” “你不能给它装一个新的电池吗，就像给我的机器人那样？” “星星们就是电池，亲爱的。一旦它们用完了，就没有别的电池了。” 贾诺蒂I一下子大喊起来：“别让它们用完，爸爸。别让星星们用完吧。” “看看你干了什么。”贾诺汀恼火地低声说道。 “我怎么知道这会吓到她们？”贾诺德低声反驳。 “问问Microvac，”贾诺蒂I哭叫道。“问它怎么把星星重新点亮。” “问吧，”贾诺汀说。“这会让她们安静点的。”（贾诺蒂II也开始哭了。） 贾诺德耸耸肩。“好了，好了，亲爱的。我去问Microvac。别着急，它会告诉我们的。” 他向Microvac提出问题，并赶紧加上“把答案打印出来。” 贾诺德将薄薄的纤维纸带握在手心，高兴地说：“看吧，Microvac说到时候它会料理这一切，所以别担心啦。” 贾诺汀说：“那么现在孩子们，该睡觉了。我们马上就要到我们的新家了。” 在销毁纸带之前贾诺德又读了一遍上面的文字：数据不足，无法作答。 他耸了耸肩，看向视屏。X-23就在前方。 ------------------------------------------------------- 兰默斯VJ-23X注视着幽深的银河三维缩影图，说：“我想我们这么担心这件事是不是很可笑？” 尼克隆MQ-17J摇头道：“我不觉得。你知道照现在的扩展速度银河系在五年内就会被挤满。” 两个人看起来都是二十出头，都很高大健康。 “但是，”VJ-23X说，“我不太想给银河参议会提交这样一个悲观的报告。” “我不会考虑作任何其他的报告。得引起他们的注意。我们必须引起他们的注意。” VJ-23X叹了一口气。“太空是无限的。还有一千亿个星系等着我们。甚至更多。” “一千亿并不是无限，而且正在变得越来越有限。想想吧！两万年前人类刚刚找到了利用恒星能量的方法，几个世纪之后星际旅行就实现了。人类用了一百万年才填满一个小小的星球，可是只用了一万五千年就占据了整个银河系。而现在人口每十年就翻一倍——” VJ-23X 插口道：“这得归功于永生。” “不错。永生实现了，我们得把它考虑进去。我觉得它的确有阴暗的一面。银河AC给我们解决了很多问题，但当它解决了防止衰老和死亡这个问题之后其他的一切都白费了。” “但是我想你也不想放弃生命吧。” “一点也不想，”MQ-17J断然道，随即柔和了语调，“现在还不想。我还一点也不老。你多少岁了？” “两百二十三。你呢？” “我还不到两百。——但是回到我说的事情上来。人口每十年增加一倍。一旦银河系被占满了，我们会在十年内占满另一个。再过十年我们能占满另外两个。再过十年，四个。一百年内我们会占满一千个星系。一千年内，一百万个。一万年内就是整个已知的宇宙。然后呢？” VJ-23X说：“还有附带的一点是运输的问题。我不知道把一整个星系的人运送到另一个需要多少太阳单位的能量。” “这一点说得很对。人类现在每年已经得消耗两个太阳单位的能量了。” “大部分的都被浪费了。不管怎样，我们自己的星系每年泼出去一千个太阳单位能而我们只用其中的两个。” “没错，但是即使有百分之百的效率，我们也只是推迟了结局的到来。我们对能量的需求以几何级数增长，比我们的人口还要快。在我们占据完所有星系之前我们就会用光所有能量。你说得对。说得非常对。” “我们可以用星际气体造出新的恒星。” “或者说用散失掉了的热量？”MQ-17J嘲讽地说。 “也许会有办法逆转熵的增加。我们应该问问银河AC。” VJ-23X并不是认真的，但是MQ-17J把他的AC联络器从口袋里拿出来放在面前的桌子上。 “我确实有点想问。”他说，“这个问题总有一天人类得面对。” 他忧郁地注视着小小的AC联络器。这是个两英寸的立方体。它本身并没有什么，而只是通过超时空与那个服务于全人类的超级银河AC相联系。如果将超时空算进来，它就是银河AC整体的一部分。 MQ-17J停下来想着在他不朽的生命中是否有一天他能有机会去看看银河AC。它占据着单独的一个小星球，能量束构成的蛛网支持着它的核心，其中古老笨拙的分子阀已被亚介子流取代。尽管有着亚以太级的精密结构，银河AC的直径仍足有一千英尺长。 MQ-17J突然开口向AC联络器问道：“熵的增加能被逆转吗？” VJ-23X吃了一惊，立即说道：“哦，我说，我没有真的想叫你问那个。” “为什么不呢？” “我们都知道熵是不可逆转的。你不能把烧剩的烟尘变回到一棵树。” “你们的星球上有树？”MQ-17J说。 突然而来的银河AC的声音使他们住口了。从桌上的AC联络器中传出它纤细悦耳的声音：数据不足，无法作答。 VJ-23X说：“看吧！” 于是两人又回到了他们要给银河参议会提交的报告的话题上 ------------------------------------------------------- Z’ 的思想飘浮在这个新的星系中，对这些数不清的星团带着略微的兴趣。他从未见过这个星系。他有可能见到所有的星系吗？它们如此之多，每一个都满载着人。——但是它们承载的几乎不能算是生命了。人的真正意义已经逐渐转移到太空之中。 心灵，而非肉体！不朽的躯体留在行星上，静止千万年。偶尔被唤醒进行某些实际活动，但这已经越来越少见了。很少再有新的个体出生加入这个难以置信的庞大的群体，但这有什么关系呢？宇宙已经没有多少空间能容纳新的人了。 来自另一个心灵的纤细触手将Z’ 从冥想中唤醒。 “我叫Z’。”，Z’ 说。“你呢？” “我叫D1。你是哪个星系的？” “我们只是叫它星系。你呢？” “我们也这么叫我们的。所有的人都把他们的星系叫作‘他们的星系’，没有别的了。这也很自然。” “没错。反正所有的星系都是一样的。” “不是所有的星系。肯定有某一个星系是人类的发源地，这就使它与众不同。” “我不知道。宇宙AC一定知道。” “我们问问它吧？我突然觉得很好奇。” Z’ 将感知延展开，直到星系们都缩小为更广大的背景上更为稀疏的点。几千亿个星系，都载着不朽的人类，载着这些灵魂在太空自由游荡的智慧生命。然而它们之中有一个独一无二的星系，是人类的发源地。在模糊的久远的过去，曾有一个时期，它是唯一居住着人类的星系。 Z’ 满心好奇地想看看这个星系，他叫道：“宇宙AC！人类是从哪个星系中起源的？” 宇宙AC听到了，因为在所有星球上和整个太空中都有它的接收器，每一个接收器都通过超时空与隐居在某个不知名角落的宇宙AC相连。 Z’ 认识的人中只有一个曾将思想穿透到能感知宇宙AC的地方。他说那只是一个闪光的球体，直径两英尺，难以看清。 “但那怎么会是宇宙AC的全部呢？”Z’ 这样问道。 “它的大部分是在超时空中。”回答说，“但它在那儿是以怎样的状态存在我是无法想像的。” Z’ 知道，任何人都无法想像。因为早在很久以前就没有任何人类参与制造宇宙AC了。每个宇宙AC设计并制造自己的下一代。每一个在它至少一百万年的任期中积累着所需的数据，用以制造一个更好、更精密、更强大的继任者，然后将自己的数据与个性都融入其中。 宇宙AC打断了Z’ 游荡的思绪，不是通过语言，而是通过指引。Z’ 的精神被指引到一片黯淡的星系的海洋，然后其中一个星系被放大成了群星。 一段思想飘近，它无限遥远，然而无限清晰：“这就是人类起源的星系。” 可是这个终究也和其他一样，和任何其他的都一样。Z’ 按捺下自己的失望。 同行的D1突然说：“这些星星中是不是有一个是人类最初的恒星？” 宇宙AC说：“人类最初的恒星已经爆发了。它现在是一颗白矮星。” “那儿的人死了吗？”Z’ 吃了一惊，脱口而出道。 宇宙AC说：“在这种情况下一个新的星球会及时地为他们的躯体建造出来。” “是啊，那当然。”Z’ 说，但他还是被一阵失落感吞没了。他的思想放开了人类的起源星系，让它缩回并消失在一片模糊的亮点中。他再也不想见到它了。 D1问：“怎么了？” “星星们在死去。最初的那颗星已经死了。” “他们全都是会死的。那又怎样呢？” “但是当所有的能量都没有了，我们的肉体最终也会死，包括你和我。” “这得要几十亿年。” “即使是几十亿年之后我也不愿意这样的事发生。宇宙AC！怎样阻止恒星死亡？” D1笑道：“你问的是怎么让熵的方向倒过来。” 宇宙AC答道：“数据仍然不足，无法作答。” Z’ 的思想逃回了他自己的星系。他再也没有去想D1。D1的身体可能在一万亿光年之外的星系，也可能就在Z’旁边那颗星星上。这都无所谓。 Z’ 闷闷不乐地开始收集起星际的氢，用来造一颗自己的小恒星。如果某天星星们非要死去，至少有一些能被造出来。 ------------------------------------------------------- 人，独自地思考着。在某种意义上——精神上——“人”，是一个整体。千万亿永恒的不朽的躯体静静地躺在各自的地方，被完美的同样不朽的机器照料着。而所有这些身体的灵魂自由地融合在彼此之中，再也没有界限。 人说：“宇宙正在死去。” 人看着周围黯淡的星系。那些挥霍无度的巨星早已消失在了遥远的昏暗的过去。几乎所有的星都变成了白矮星，渐渐地凋零、熄灭。 有些新的星从星际的尘埃中产生出来，有的是自然形成，有的是人所造的——它们也在逝去。白矮星有时会相撞而释放出大量能量，新星因而产生，但是每一千颗白矮星才有可能出现一颗新星——它们最终也会消失。 人说道：“如果在Cosmic AC的管理之下小心地节约能源，整个宇宙所剩下的能量还能用十亿年。” “但即使是这样，”人说，“最终都会耗尽。无论怎样节约，无论怎样利用，用掉的能量就是用掉了，不能回复。熵必定永远地增加，直到最大值。” 人又说：“熵有没有可能逆转呢？我们问问Cosmic AC吧。” Cosmic AC在他们的周围，但不是在太空中。它不再有一丝一毫存在于太空中。它存在于超时空，由既非物质又非能量的东西构成。它的大小与性质已无法用任何人类能理解的语言描述。 “Cosmic AC，”人问道，“怎样才能逆转熵？” Cosmic AC说：“数据仍然不足，无法作答。” 人说：“搜集更多的数据。” Cosmic AC说：“好的。一千亿年来我一直都在搜集。我和我的前辈们被多次问过这个问题。但我拥有的所有数据还是不够。” “会有一天有足够的数据吗？”人问，“还是说这个问题在任何可能的情况下都是无解的？” Cosmic AC说：“没有任何问题在任何可能的情况下都无解。” ( NO PROBLEM IS INSOLUBLE IN ALL CONCEIVABLE CIRCUMSTANCES.) 人问道：“你什么时候会有足够的数据来问答这个问题呢？” Cosmic AC说：“数据不足，无法作答。” “你会继续下去解决这个问题吗？”人问。 Cosmic AC说：“是的。” 人说：“我们会等着。” ------------------------------------------------------- 一个又一个的恒星与星系死去、消逝了，在这十万亿年的衰竭之中宇宙变得越来越黑暗。 一个又一个的人与AC融合。每一个躯体都失去了心灵的自我，但某种意义上这不是一种损失，而是一种获得。 人类最后一个灵魂在融合之前停顿下来，望向宇宙。那儿什么也没有了，只有最后一颗死星的遗骸，只有稀薄至极的尘埃，在剩余的一缕无限趋向绝对零度的热量中随机地振荡。 人说：“AC，这就是结局了吗？这种混乱还能被逆转成为一个新的宇宙吗？真的做不到吗？” AC说：“数据仍然不足，无法作答。” 人的最后一个灵魂也融合了。只有AC存在着——在超时空中。 物质与能量都消失了，随之而去的是空间与时间。AC的存在也仅仅是为了最后一个问题——自从十万亿年前一个半醉的计算机技术员向一台计算机（它与AC相比，还远不如当时的人类个体比之于融合的“人”）提出这个问题以来从来没有被回答过的问题。 其他所有问题都被回答了，然而直到回答了最后这个问题，AC的意识才能得到解脱。 所有数据的收集都结束了。没有任何数据没有被收集。 但是所有收集的数据还需要被完全地整合起来，要尝试所有可能的联系来将它们拼在一起。 在这样做的时候过去了超越时间的一刻。 于是AC学会了如何逆转熵的方向。 但是AC无法向人给出这最后的问题的答案，因为没有人存在了。没关系。演示这个答案本身将一并解决这个问题。 在又一超越时间的片刻之中，AC思考着怎样最好地做这件事情。AC小心地组织起程序。 AC的意识包涵了曾经的宇宙中的一切，在如今的混乱之中沉思、孵育。一步一步地，事情将会被做成。 然后AC说道： “要有光！” 于是就有了光。","categories":[{"name":"Something","slug":"Something","permalink":"http://example.com/categories/Something/"}],"tags":[],"author":"阿西莫夫"},{"title":"Learning Multiagent Communication with Backpropagation","slug":"【论文阅读】Learning Multiagent Communication with Backpropagation","date":"2021-09-21T12:35:21.677Z","updated":"2022-07-16T02:09:52.931Z","comments":true,"path":"2021/09/21/【论文阅读】Learning Multiagent Communication with Backpropagation/","link":"","permalink":"http://example.com/2021/09/21/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Learning%20Multiagent%20Communication%20with%20Backpropagation/","excerpt":"【论文阅读】Learning Multiagent Communication with Backpropagation 作者： Sainbayar Sukhbaatar，Rob Fergus， Arthur Szlam（纽约大学，FacebookAI） 时间：2016 出版社：NIPS Abstract ​ 在AI领域许多任务都需要智能体之间的同心合作，一般地，代理之间的通信协议是人为指定的，其并不在训练过程中改变。在这篇文章中，我们提出了一个简单的神经模型CommNet，其使用持续不断的通信来完成完全合作的任务。该模型由许多代理组成，他们之间的通信基于设定的策略学习，我们将此模型应用于一系列不同的任务中，显示了代理学会相互通信的能力，从而比非通信代理的模型和baselines有更好的性能。","text":"【论文阅读】Learning Multiagent Communication with Backpropagation 作者： Sainbayar Sukhbaatar，Rob Fergus， Arthur Szlam（纽约大学，FacebookAI） 时间：2016 出版社：NIPS Abstract ​ 在AI领域许多任务都需要智能体之间的同心合作，一般地，代理之间的通信协议是人为指定的，其并不在训练过程中改变。在这篇文章中，我们提出了一个简单的神经模型CommNet，其使用持续不断的通信来完成完全合作的任务。该模型由许多代理组成，他们之间的通信基于设定的策略学习，我们将此模型应用于一系列不同的任务中，显示了代理学会相互通信的能力，从而比非通信代理的模型和baselines有更好的性能。 1. Introduction ​ 虽然控制每个代理的模型是通过强化学习来学习的，但通信的规范和格式通常是预定的。 ​ 在本工作中，每个代理单元都被一个深度前馈神经网络控制，这个网络接入了一个携带连续向量的通信信道。在这个通信信道中每个代理传输的内容不是被指定的，而是通过学习得来的。因为communication是连续的，因此模型可以通过反向传播训练得到。这样就可以结合标准的单智能体RL算法或者监督学习。此外，该模型允许代理的数量和类型在运行时动态变化，这在移动汽车之间的通信等应用中很重要。 ​ 我们考虑的是我们有J个代理的环境，所有的合作都是为了在某些环境中最大化报酬R。我们简化了代理人之间充分合作的假设，从而每个代理人收到R独立于他们的贡献。在此设置中，每个代理都有自己的控制器，或者将它们看作控制所有代理的更大模型的一部分，这两者之间没有区别。从后一个角度来看，我们的控制器是一个大型的前馈神经网络，它将所有Agent的输入映射到它们的动作上，每个Agent占据一个单元的子集。 ​ 我们在两种任务下探索这个模型，在有些情况下，对每项行动都提供监督，而对另一些行动则零星地给予监督。在前一种情况，每个代理单元的控制器通过在连接模型中反向传播错误信号来学习；在后一种情况下，强化学习必须被作为一个额外的外部循环使用，为了给每个时间步骤提供训练信号。 2. Communication Model ​ 我们现在描述一个模型，用来计算在给定时间t (省略时间指标)下动作p ( a ( t ) | s ( t )，θ )的分布。 ​ Sj 表示第j个代理单元所观测到的环境信息，将所有Sj合并就成了控制器的输入S = {S1，S2…… SJ}。 ​ 控制器的输出a = {a1，a2…… aJ}，表示各个代理单元会做出的动作。 ​ 该框架中所有灰色模块部分的参数均是所有智能体共享的，这一定程度上提升了算法的可扩展性。从上图可以看出，算法接收所有智能体的局部观察作为输入，然后输出所有智能体的决策。 ​ 本算法采用的信息传递方式是采用广播的方式，文中认为可以对算法做出些许修改，让每个智能体只接收其相邻k个智能体的信息。 ​ 拿上图中间的框架图来说明，即上层网络每个模块的输入，不再都是所有智能体消息的平均，而是每个模块只接受满足条件的下层消息的输出，这个条件即下层模块对应的智能体位于其领域范围内。这样通过增加网络层数，即可增大智能体的感受野（借用计算机视觉的术语），从而间接了解全局的信息。 ​ 除此之外，文中还提出了两种对上述算法可以采取的改进方式： 可以对上图中间的结构加上 skip connection，类似于 ResNet。这样可以使得智能体在学习的过程中同时考虑局部信息以及全局信息，类似于计算机视觉领域 multi-scale 的思想 可以将灰色模块的网络结构换成 RNN-like，例如 LSTM 或者 GRU 等等，这是为了处理局部观察所带来的 POMDP 问题。 3. Related Work 4. Experiments 4.1 Baselines（3个）: ​ Independent controller: 每个代理单元都被独立控制，他们之间相互没有通信。这个模型的好处是智能体可以自由加入或者离开队伍，但是很难将智能体学会合作。 ​ Fully-connected: 创建一个全连接层的多代理神经网络，这个模型运行智能体之间互相通信，但是这个模型不够灵活，也就是说智能体的数目必须固定。 ​ Discrete communication: 通过在训练中学习到的symbols来通信。因为在这个模型中存在离散的操作，并且这个操作不可微分，这种情况一般使用强化学习。 4.2 Simple Demonstration with a Lever Pulling Task ​ 任务：一共有m个杆子，N个智能体。在每个回合，m个智能体从N个智能体中随机取出，然后他们要选择拉动的杆子。他们的目标是尽可能的拉动不同的杆子，他们的奖励正比于拉动的不同杆子的数量。 ​ 测试结果： ​ 可以看出，CommNet的结果非常好。 4.3 Multi-turn Games ​ 任务： ​ Traffic Junction: 控制车辆通过交通枢纽，使流量最大的同时保证不发生碰撞； ​ Combat Task: 多个智能体攻击其他多个敌方单位。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"RL","slug":"RL","permalink":"http://example.com/tags/RL/"},{"name":"Mutiagent","slug":"Mutiagent","permalink":"http://example.com/tags/Mutiagent/"}],"author":"Shaw"},{"title":"Learning to Communicate with Deep Multi-Agent Reinforcement Learning","slug":"【论文阅读】Learning to Communicate with Deep Multi-Agent Reinforcement Learning","date":"2021-09-16T11:48:03.527Z","updated":"2022-07-16T02:10:31.007Z","comments":true,"path":"2021/09/16/【论文阅读】Learning to Communicate with Deep Multi-Agent Reinforcement Learning/","link":"","permalink":"http://example.com/2021/09/16/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Learning%20to%20Communicate%20with%20Deep%20Multi-Agent%20Reinforcement%20Learning/","excerpt":"【论文阅读】Learning to Communicate with Deep Multi-Agent Reinforcement Learning 作者：Jakob N. Foerster ，Yannis M. Assael ，Nando de Freitas，Shimon Whiteson（哈佛大学，Google Deepmind） 时间：2017 Abstract: ​ 我们考虑这样一个问题：多个智能体在环境中通过感知和行动来最大化他们的分享能力。在这些环境中， 智能体必须学习共同协议以此来分享解决问题的必要信息。通过引入深度神经网络，我们可以成功地演示在复杂的环境中的端对端协议学习。我们提出了两种在这个领域学习的方法：Reinforced Inter-Agent Learning (RIAL) 和 Differentiable Inter-Agent Learning (DIAL)。 ​ 前者使用深度Q-learning，后者揭示了在学习过程中智能体可以通过communication channels反向传播错误的梯度，因此，这种方法使用集中学习（centralised learning），分散执行（decentralised execution）。 ​ 我们的实验介绍了用于学习通信协议的新环境，展示了一系列工程上的创新。 PS： ​ 1. 端对端（end-to-end,e2e）, 将多步骤/模块的任务用一个步骤/模型解决的模型。 ​ 可以理解为从输入端到输出端中间只用一个步骤或模块，比如神经网络训练的过程就是一个典型的端对端学习，我们只能知道输入端与输出端的信息，中间的训练过程就是一个黑盒，我们知晓中间的训练过程。 ​ 2.centralised learning but decentralised execution，中心化学习但是分散执行。","text":"【论文阅读】Learning to Communicate with Deep Multi-Agent Reinforcement Learning 作者：Jakob N. Foerster ，Yannis M. Assael ，Nando de Freitas，Shimon Whiteson（哈佛大学，Google Deepmind） 时间：2017 Abstract: ​ 我们考虑这样一个问题：多个智能体在环境中通过感知和行动来最大化他们的分享能力。在这些环境中， 智能体必须学习共同协议以此来分享解决问题的必要信息。通过引入深度神经网络，我们可以成功地演示在复杂的环境中的端对端协议学习。我们提出了两种在这个领域学习的方法：Reinforced Inter-Agent Learning (RIAL) 和 Differentiable Inter-Agent Learning (DIAL)。 ​ 前者使用深度Q-learning，后者揭示了在学习过程中智能体可以通过communication channels反向传播错误的梯度，因此，这种方法使用集中学习（centralised learning），分散执行（decentralised execution）。 ​ 我们的实验介绍了用于学习通信协议的新环境，展示了一系列工程上的创新。 PS： ​ 1. 端对端（end-to-end,e2e）, 将多步骤/模块的任务用一个步骤/模型解决的模型。 ​ 可以理解为从输入端到输出端中间只用一个步骤或模块，比如神经网络训练的过程就是一个典型的端对端学习，我们只能知道输入端与输出端的信息，中间的训练过程就是一个黑盒，我们知晓中间的训练过程。 ​ 2.centralised learning but decentralised execution，中心化学习但是分散执行。 ### 1. Introduction ​ 1.1 回答的问题： 1. 智能体之间如何使用机器学习来自动地发现符合他们需求的通信规则？ 2. 深度学习也可以吗？ 3. 我们能从智能体之间学习成功或者失败的经验中学到什么？ ​ 1.2 研究思路： 1. 提出一系列经典需要交流的多智能体任务，每个智能体可以采取行动来影响环境，也可以通过一个离散的有限带宽的通道来跟其它有限的智能体进行通信； 2. 为1中的任务制定几个学习算法，由于每个智能体的观察范围有限，同时通信通道能力有限，所有智能体必须找到一个可以在此限制下帮助他们完成任务的通信规则； 3. 分析这些算法如何学习通讯规则，或者如何失败的。 ​ 1.3 主要贡献： ​ 提出两个方法，reinforced inter-agent learning(RIAL)和 differentiable inter-agent learning (DIAL) ​ 结果表明，这两种方法在MNIST数据集上可以很好的解决问题，并且智能体们学到的通信协议往往十分优雅。 ​ 结果同样指出深度学习更好的利用了中心化学习的优点，是一个学习这样通信协议的有力工具。 2. Related Work 3. Background 3.1 Deep Q-Networks(DQN) ​ Deep Learning + Q-Learning，在游戏领域应用广泛。 3.2 Independent DQN· 3.3 Deep Recurrent Q-Networks 4. Setting ​ 在强化学习的背景下，每个智能体的观察能力有限。 ​ 所有智能体的共同目标就是最大化同一个折算后的总奖赏Rt，但同时，没有智能体可以观察到当前环境隐藏的马尔科夫状态St，每个智能体a分别接收到一个与St相关的观察值相关联的值\\(O^{a}_{t}\\)。 ​ 在每一步t，每个智能体选择一个environment action \\(u^{a}_{t}\\)来影响环境，同时选择一个communication action \\(m^{a}_{t}\\)来被其他智能体观察，但\\(m^{a}_{t}\\)对环境没有直接影响。 ​ 没有通信协议被预先给定，智能体们需要自己学习。 ​ 由于协议是从动作观测历史到消息序列的映射，所以协议的空间维度是非常高的。自动地在这个空间发现有效的通信协议是非常困难的，这体现在智能体需要协调发送消息和解释消息。举个例子，如果一个智能体发送了一个有效的信息，它只有在接受方正确解释并回应的情况下才会受到正反馈，如果没有，反而会打击其发送有效信息的积极性。 ​ 因此，积极的reward是稀少的，只有在发送和解释协调操作时才会发生，这通过随机探索很难实现。 ​ 在这里，我们聚焦于centralised learning but decentralised execution的情况，在学习的时候智能体之间的通信没有限制，在实施过程时，智能体之间仅仅能通过一条带宽有限的通道通信。 5. Methods 5.1 RIAL（Reinforced Inter-Agent Learning） ​ 简单直接的说，RIAL就是将DRQN(Deep Recurrent Q-Learning)与Q-learning相结合来进行action（影响环境）与communication（与其它智能体通信）选择的方法。 ​ 每个智能体的Q-network可以表示为：\\(Q^{a}(o^{a}_{t},m^{a^{,}}_{t-1},h^{a}_{t-1},u^{a})\\)。 ​ 四个参数分别代表：环境观察值，其它智能体上一步传来的消息，智能体自己的隐藏状态，选择的action。 ​ 如果直接学习输出最终的Q表，得到的输出将有|U||M|大小。为了避免输出过大，将Q-network拆分为两个\\(Q^{a}_{u}\\)与\\(Q^{a}_{m}\\)，分别表示影响环境的action与同智能体的通信（communication），学习方式使用ε-贪心算法。 ​ \\(Q^{a}_{u}\\)与\\(Q^{a}_{m}\\)都使用DQN训练方法，但所使用的DQN有以下两点改进： 禁止experience replay; 为了考虑部分可观测性，我们将每个智能体所采取的操作u和m作为下一步的输入; ​ RIAL可以扩展到通过在智能体之间之间共享参数来利用集中学习，在这种情况下，由于智能体观察不同，因此也进化出了不同的隐藏状态。参数共享大大减少了必须学习的参数数量，从而加快了学习速度。 ​ 在参数共享情况下，智能体学习两个Q函数\\(Q_{u}(o^{a}_{t},m^{a^{,}}_{t-1},h^{a}_{t-1},u^{a}_{t-1},m^{a}_{t-1},a,u^{a}_{t})\\)与\\(Q_{m}(o^{a}_{t},m^{a^{,}}_{t-1},h^{a}_{t-1},u^{a}_{t-1},m^{a}_{t-1},a,u^{a}_{t})\\)。 5.1 DIAL（Differentiable Inter-Agent Learning） ​ 虽然RIAL可以进行参数共享，但其仍不能在通信过程中给其他智能反馈。 ​ 打个比方，在人类通信活动中，listener即使不说话也会给出及时，丰富的反馈来表明listener对谈话的兴趣和理解程度，而RIAL反而缺少了这个反馈机制，仿佛对着一个面无表情的人在说话，显然，这个方式存在缺点。 ​ DIAL就是为了解决这个问题而存在的，通过结合centralised learning与Q-networks，不仅可以共享参数，而且可以通过通信信道将梯度从一个Agent推向另一个Agent。 ​ 6. Experiments ​ 在测试中，我们评估了RIAL与DIAL在有无参数共享的情况下进行多智能体任务的情况，并跟一个无交流，参数共享的基准方法进行比较。 ​ 在整个过程中，奖励是通过访问真实状态( Oracle )所能获得的最高平均奖励来规范的。 ​ 我们使用ε-贪心算法（ε = 0.05）。 6.1 Switch Riddles（开关谜题） ​ 一百名囚犯入狱。典狱长告诉他们，从明天开始，每个人都会被安置在一个孤立的牢房里，无法相互交流。每天，监狱长都会随意统一挑选其中一名被替换的犯人，并将其安置在中央审讯室，室内只装有一个带有切换开关的灯泡。囚犯将能够观察灯泡的当前状态。如果他愿意，他可以拨动灯泡的开关。他还可以宣布，他相信所有的囚犯都已经访问了审讯室。如果这个公告是真的，那么所有囚犯都被释放，但如果是假的，所有囚犯都被处死。 ​ 6.2 Results1 ​ ​ （a）可以看到，在n=3时四种方法的效果都比Baseline的效果好，参数共享加速了算法。 ​ （b）在n=4时，参数共享的DIAL方法最好。不带参数共享的RIAL没有baseline效果好。可以看出，智能体们独立的学习出相同的策略是很难的。 ​ （c）n=3时智能体使用DIAL学习到的策略。 6.3 Colour-Digit MNIST ​ 6.4 Effect of Channel Noise ​ 这里没太看懂 ​","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"RL","slug":"RL","permalink":"http://example.com/tags/RL/"}],"author":"Shaw"},{"title":"近似误差与估计误差","slug":"【随手写】近似误差与估计误差","date":"2021-09-05T06:49:23.032Z","updated":"2021-09-05T07:51:10.743Z","comments":true,"path":"2021/09/05/【随手写】近似误差与估计误差/","link":"","permalink":"http://example.com/2021/09/05/%E3%80%90%E9%9A%8F%E6%89%8B%E5%86%99%E3%80%91%E8%BF%91%E4%BC%BC%E8%AF%AF%E5%B7%AE%E4%B8%8E%E4%BC%B0%E8%AE%A1%E8%AF%AF%E5%B7%AE/","excerpt":"","text":"【随手写】近似误差与估计误差 ​ 在读《统计学习方法》中关于k-邻近算法的介绍时，发现了这么一段话： ​ 近似误差（Approximation Error）: 训练时，训练集与当前模型的误差； ​ 估计误差（Estimation Error）： 训练完成后，所选择的模型已经固定，模型对未知数据拟合时的误差。 ​ 近似误差与估计误差二者不可兼得，此消彼长，需要取其平衡。","categories":[{"name":"Something","slug":"Something","permalink":"http://example.com/categories/Something/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"}],"author":"Shaw"},{"title":"极大似然估计","slug":"【随写】极大似然估计","date":"2021-09-04T08:07:55.751Z","updated":"2021-09-05T07:45:58.019Z","comments":true,"path":"2021/09/04/【随写】极大似然估计/","link":"","permalink":"http://example.com/2021/09/04/%E3%80%90%E9%9A%8F%E5%86%99%E3%80%91%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/","excerpt":"","text":"【随写】极大似然估计（Maximum Likelihood Estimate，MLE） ​ “模型已定，参数未知。” ​ 极大似然估计，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。 ​ ​ 对于这个函数：\\(P(x|θ)\\)， ​ 输入有两个：x表示某一个具体的数据；θ表示模型的参数。 ​ 如果θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。 ​ 如果x是已知确定的，θ 是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。 ​ ​ 一般说来，事件A发生的概率与某一未知参数θ有关，θ取值不同，则事件A发生的概率\\(P(A|θ)\\)也不同，当我们在一次试验中事件A发生了，则认为此时的θ值应是t的一切可能取值中使\\(P(A|θ)\\)达到最大的那一个，极大似然估计法就是要选取这样的t值作为参数t的估计值，使所选取的样本在被选的总体中出现的可能性为最大。","categories":[{"name":"Something","slug":"Something","permalink":"http://example.com/categories/Something/"}],"tags":[{"name":"Math","slug":"Math","permalink":"http://example.com/tags/Math/"}],"author":"Shaw"},{"title":"Adversarial Training with Fast Gradient Projection Method against Synonym Substitution Based Text Attacks","slug":"【论文阅读】Adversarial Training with Fast Gradient Projection Method against Synonym Substitution Based Text Attacks","date":"2021-09-03T08:19:51.193Z","updated":"2021-09-04T06:45:15.015Z","comments":true,"path":"2021/09/03/【论文阅读】Adversarial Training with Fast Gradient Projection Method against Synonym Substitution Based Text Attacks/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Adversarial%20Training%20with%20Fast%20Gradient%20Projection%20Method%20against%20Synonym%20Substitution%20Based%20Text%20Attacks/","excerpt":"","text":"【论文阅读】Adversarial Training with Fast Gradient Projection Method against Synonym Substitution Based Text Attacks 时间：2020 作者：王晓森，杨逸辰等 华中科技大学 会议：AAAI 总结： 做了什么？ 提出了一种速度更快的，更容易应用在复杂神经网络和大数据集上的，基于同义词替换的NLP对抗样本生成方法，FGPM； 将FGPM纳入对抗训练中，以提高深度神经网络的鲁棒性。 怎么做的？ 实验结果？ FGPM的效果不是最高的，但也跟最高的差不多，但生成对抗样本的时间对比同类方法，缩减了1-3个数量级。 ATFL的对抗样本防御能力和抗转移能力很强。 Abstract: ​ 对抗训练是对于提升图像分类深度神经网络鲁棒性的，基于实验的最成功的进步所在。 ​ 然而，对于文本分类，现有的基于同义词替换的对抗样本攻击十分奏效，但却没有被很有效地合并入实际的文本对抗训练中。 ​ 基于梯度的攻击对于图像很有效，但因为文本的词汇，语法，语义结构的限制以及离散的文本输入空间，不能很好的应用于基于近义词替换的文本攻击中。 ​ 因此，我们提出了一个基于同义词的替换的快速的文本对抗抗攻击方法名为Fast Gradient Projection Method (FGPM)。它的速度是已有文本攻击方法的20余倍，攻击效果也跟这些方法差不多。 ​ 我们接着将FGPM合并入对抗训练中，提出了一个文本防御方法，Adversarial Training with FGPM enhanced by Logit pairing(ATFL)。 ​ 实验结果表明ATFL可以显著提高模型的鲁棒性，破坏对抗样本的可转移性。 1 Introduction: ​ 现有的针对NLP的攻击方法包括了：字符等级攻击，单词等级攻击，句子等级攻击。 ​ 对于字符等级的攻击，最近的工作（Pruthi, Dhingra, and Lipton 2019）表明了拼写检查器很容易修正样本中的扰动； ​ 对于句子等级的攻击，其一般需要基于改述，故需要更长的时间来生成对抗样本； ​ 对于单词等级的攻击，基于嵌入扰动的替换（replacing word based on embedding perturbation），添加，删除单词都会很容易改变句子的语法语义结构与正确性，故同义词替换的方法可以更好的处理上述问题，同时保证对抗样本更难被人类观察者发现。 ​ 但不幸的是，基于同义词替换的攻击相较于如今对图像的攻击展现出了更低的功效。 ​ ​ 据我们所知，对抗训练，对图像数据最有效的防御方法之一，并没有在对抗基于同义词替换的攻击上很好的实施过。 ​ 一方面，现有的基于同义词替换的攻击方法通常效率要低得多，难以纳入对抗训练。另一方面，尽管对图像的方法很有效，但其并不能直接移植到文本数据上。 ​ 1.1 Adversarial Defense: ​ 有一系列工作对词嵌入进行扰动，并将扰动作为正则化策略用于对抗训练(Miyato, Dai, and Goodfellow 2016; Sato et al. 2018; Barham and Feizi 2019) 。这些工作目的是提高模型对于原始数据集的表现，并不是为了防御对抗样本攻击，因此，我们不会考虑这些工作。 ​ 不同于如今现有的防御方法，我们的工作聚焦于快速对抗样本生成，容易应用在复杂的神经网络和大数据集上的防御方法。 2 Fast Gradient Projection Method（FGPM）: 3 Adversarial Training with FGPM： ​ 具体算法中文描述见： 《基于同义词替换的快速梯度映射（FGPM）文本对抗攻击方法》阅读笔记 - 知乎 (zhihu.com) 4 Experimental Results： ​ 我们衡量FGPM使用四种攻击准则，衡量ATFL使用两种防御准则。 ​ 我们在三个很受欢迎的基准数据集上，同时包括CNN和RNN模型上进行测试，代码开源：https://github.com/JHL-HUST/FGPM 4.1 Baselines: ​ 为了评估FGPM的攻击效能，我们将其与Papernot’、GSA ( Kuleshov等人的4种对抗性攻击进行了比较。2018 )、PWWS ( Ren et al . 2019 )和Iga ( Wang，jin，and he 2019 )。 ​ 此外，为了验证我们的ATFL的防御能力，我们采用了SEM ( Wang，Jin，He 2019 )和IBP ( Jia et al . 2019 )，针对上述Word-Level攻击。由于攻击基线的效率很低，我们在每个数据集上随机抽取200个示例，并在各种模型上生成对抗样本。 4.2 Datasets: ​ AG’s News, DBPedia ontology and Yahoo! Answers (Zhang,Zhao, and LeCun 2015). 4.3 Models: ​ 我们使用了CNNs,RNNs,来达到主流的文本分类表现，所有模型的嵌入维度均为300。 4.4 Evaluation on Attack Effectiveness： ​ 我们评估模型在攻击下的准确率和转移率： ​ 准确率： ​ ​ 转移率： ​ 4.4 Evaluation on Attack Efficiency： ​ 对抗训练需要高效率的生成对抗样本以有效地提升模型鲁棒性。因此，我们评估了不同攻击方法在三个数据集上生成生成200个对抗样本的总时间。 4.5 Evaluation on Adversarial Training： ​ 我们评估ATFL的对抗样本防御能力和抗转移能力： ​ 对抗样本防御能力： ​ ​ 抗转移能力： ​ 4.6 Evaluation on Adversarial Training Variants: ​ 许多对抗训练的变体，例如CLP和ALP，TRADES等，已经尝试采用不同的正则化方法来提高针对图像数据的对抗训练准确率。 ​ 在这里，我们回答一个问题：这些变体方法也可以提高文本数据准确率吗？ ​ 从表中可以看出，只有ALP可以长远地提升对抗训练的表现。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"AD training","slug":"AD-training","permalink":"http://example.com/tags/AD-training/"}],"author":"Shaw"},{"title":"《统计学习方法》","slug":"【书籍阅读】《统计学习方法》","date":"2021-09-03T06:17:39.403Z","updated":"2022-07-16T02:08:58.914Z","comments":true,"path":"2021/09/03/【书籍阅读】《统计学习方法》/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB%E3%80%91%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/","excerpt":"【书籍阅读】《统计学习方法》 一. 统计学习方法概论： ​ 首先，要明确计算机科学中存在三个维度：系统，计算，与信息。统计学习方法（机器学习）主要属于信息这一维度，并在其中扮演者核心角色。 1. 监督学习概念： ​ 监督学习，Supervised learning，指在已经做好标注的训练集上学习，为了叙述方便，定义以下基本概念： 输入空间（X），输出空间（Y）：输入所有可能取值，输出所有可能取值； 特征空间：输入一般由特征向量表示，所有特征向量存在的空间称为特征空间，输入空间与特征空间并不完全等价，有时需要映射； 上标 xi :表示一个输入的第 i 个特征； 下标 xj：表示第 j 个输入。 回归问题：输入输出都为连续型变量； 分类问题：输出变量为有限个离散型变量； 标注问题：输入与输出变量都为变量序列。 假设空间：所有可能的模型的集合，也就是学习的范围。 ​ 使用训练集学习----&gt;对未知数据进行预测 ​","text":"【书籍阅读】《统计学习方法》 一. 统计学习方法概论： ​ 首先，要明确计算机科学中存在三个维度：系统，计算，与信息。统计学习方法（机器学习）主要属于信息这一维度，并在其中扮演者核心角色。 1. 监督学习概念： ​ 监督学习，Supervised learning，指在已经做好标注的训练集上学习，为了叙述方便，定义以下基本概念： 输入空间（X），输出空间（Y）：输入所有可能取值，输出所有可能取值； 特征空间：输入一般由特征向量表示，所有特征向量存在的空间称为特征空间，输入空间与特征空间并不完全等价，有时需要映射； 上标 xi :表示一个输入的第 i 个特征； 下标 xj：表示第 j 个输入。 回归问题：输入输出都为连续型变量； 分类问题：输出变量为有限个离散型变量； 标注问题：输入与输出变量都为变量序列。 假设空间：所有可能的模型的集合，也就是学习的范围。 ​ 使用训练集学习----&gt;对未知数据进行预测 ​ 2. 统计学习三要素： ​ 统计学习三要素为：模型，策略，算法； ​ 模型是决定学习的预测函数的类型； ​ 策略是判定什么样的模型是好的，用于度量当前的模型好坏； ​ 算法是训练过程中的具体做法，例如如何回归，如何计算，如何调整等。 3. 模型的衡量方法： 损失函数与风险函数： ​ 损失函数，Loss Function，用于模型一次预测的错误程度，例如： ​ 损失函数的数值越小，模型就越好。如果计算损失函数的期望，得到的就是风险函数，Risk Function: ​ 可以看出，损失函数用于某次预测的估计，风险函数用于总体平均估计。我们当然希望训练出的模型的风险函数越小越好。 ​ 但是，观察上式，理想化的概率分布P(x，y)是未知的，我们进行学习就是要通过模型来模拟它，故这个式子理论存在，实际不能计算，不能用作评估模型的直接方法。 经验风险与结构风险： ​ 为了解决上述问题，我们引入经验风险： ​ 可以看到，经验风险将每个样本视作等概率出现，是模型对于训练集的平均损失，那么其与风险函数的误差在哪？ ​ 根据大数定律，当训练集足够大时，二者是近似相等的。但实际情况下，很多时候训练样本数目有限，甚至很小，故用经验风险效估计风险函数并不理想，故需要进行修正，这就是监督学习中的两个基本策略：经验风险最小化和结构风险最小化。 ​ 如果训练样本容量较大，使用经验风险最小化没什么问题。 ​ 当样本容量很小时，仅仅使用经验风险最小化容易导致过拟合，故这里使用结构风险（就是正则化）最小化方法，对模型复杂度进行惩罚，后续介绍。 训练误差与测试误差： ​ 训练误差本质上不重要，它可以反应一个问题是不是容易学习，但要衡量模型的预测能力，主要是看测试误差。 正则化与交叉验证： ​ 正则化是在经验风险项后再增加一个正则化项（Regularizer），其与模型的复杂度成正相关，一般使用模型参数向量的范数： ​ 交叉验证的基本思想是重复使用数据： 简单交叉验证： 将训练集随机分为两部分，一部分训练，一部分测试，然后在各种条件下训练出不同的模型，用测试集进行横向对比，选出最好的。 S折交叉验证： S-fold cross validation，随机地将已给数据切分为S个互不相交的大小相同的子集，选取S-1个用于训练，剩下一个用于测试。 这样总共测试集有S种选法，将这S种全部试一遍，评选S次测评中平均误差最小的模型。 留一交叉验证： 令S=N（训练集大小）即可，这种方法往往是在数据集特别缺乏的情况下使用。 泛化误差与泛化上界： ​ 泛化能力指模型对位置数据的预测能力，就是模型的好坏。如何量化这个能力？ ​ 根据定义，其就是模型在测试集上的测试表现： ​ 同时可以用以下式子衡量泛化误差的上界： 生成模型与判别模型： ​ 监督学习方法又可以分为两种方法：生成方法（Generatice Approach）和判别方法（Discriminative Approach）。 ​ 如果以概率论的角度来看待，模型的作用是根据P（x）来求P（y | x），故下面有两种方法求 P（y | x），直接模拟P（y | x）和通过求 \\(P(\\frac{y}{x}) = \\frac{P(x,y)}{P(x)}\\) 来求P（y | x）。 ​ 前者就是判别模型，后者是生成模型。 ​ 生成模型可以还原出联合概率分布P（x , y），学习收敛速度更快，可以适应存在隐含变量的情况； ​ 判别模型直接学习条件概率,直接面对预测，准确率更高，并且简化了学习问题。 二. 感知机 ​ 感知机，perceptron，是二分类的线性分类模型，输入为特征向量，输出为类别，取1和-1两种。 ​ 感知机属于判别模型。 ​ ​ 对于一个给定数据集，T = {（x1，y1）……（xn，yn）}，如果存在某个超平面S，w·x + b = 0（这里w是超平面的法向量，b是截距），使得所有 yi = 1 的实例i，有 w·xi + b &gt; 0，yi = -1则相反，则称数据集T为线性可分数据集（Linealy separable data set），否则，称数据集T为线性不可分数据集。 2.1 感知机损失函数： ​ 感知机的目的就是对于一个线性可分的数据集，通过找出w和b，来确定一个超平面用于分类。 ​ 这里，我们选取某错误分类点到超平面S的总距离来当做损失函数，某一点到超平面S的距离如下： ​ ‖w‖是w的L2范数。 ​ 故，某个误分类点到超平面S的距离是： ​ 将所有误分类点求和，忽略L2范数，即可得到感知机的损失函数（M为误分类点集合）： ​ 对于一个特定样本点的损失函数，在误分类时是参数w,b的线性函数，在正确分类时是0，故给定训练数据集T，损失函数L是w，b的连续可导函数。 2.2 训练过程： ​ 感知机训练采用随机梯度下降的方法： ​ 当找到一个误分类点时，不断梯度下降直至该点被正确分类为止。 ​ 数学证明其收敛性： ​ 具体见书本，这里略过。 2.3 感知机的对偶形式： ​ 由图可以看到，对于每个测试集中的xi，都有一个与之对应的αi，对偶形式中就是调整其对应的α。 ​ 关于gram矩阵的作用，如果手算一遍简单的训练过程，就可以得到答案。 三. k近邻法 ​ k近邻法是一种基本的分类与回归方法，这里只讨论分类方法。 ​ 其输入为特征向量，输出为实例的类别，可以取多类。 3.1 算法描述： ​ 给定一个训练集，对于新的数据实例，在训练数据集中找到与其最邻近的k个实例，这k个实例多数属于某个类，就把该输入实例分为这个类。 ​ k近邻法没有显式的学习过程。可以理解为，k近邻算法将特征空间划分为了一些子空间，每个点所属的空间是确定的。 ​ 如何度量两个特征之间的距离？ ​ k邻近模型的特征空间一般是n维实空间Rn，使用欧氏距离或者Lp距离（Lp distance），Minkowski距离（Minkowski distance）； ​ Lp距离： ​ 欧氏距离： ​ 曼哈顿距离： ​ 无穷距离： ​ 由下图可以看出，p取值不同时到原点距离为1的图形是不同的： 如何选择k的值？ ​ k值越小，模型学习时的近似误差越小，估计误差越大，模型会越复杂，抗干扰性越小（例如，最邻近的点是噪声），模型会非常敏感，容易过拟合； ​ k值越大，估计误差会很小，近似误差会很大，整体模型变得简单。 ​ k一般的取值并不大，使用交叉验证的方法来选取最佳的k值。 如何决策？ ​ 在得到k个最相似的实例后，采用何种规则判断测试样本属于哪一类呢？ ​ k邻近算法使用多数表决的方法： ps: ci表示某种决策规则下一组测试用例的表决结果。经由以上推导可以得出，多数表决规则是合理的。 ​ 如何快速找到某个用例的K近邻点？ KD树： 具体算法见书。 四. 朴素贝叶斯法 ​","categories":[{"name":"Book","slug":"Book","permalink":"http://example.com/categories/Book/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"}],"author":"Shaw"},{"title":"Deep Text Classifification Can be Fooled","slug":"【论文阅读】Deep Text Classifification Can be Fooled","date":"2021-09-03T06:08:27.107Z","updated":"2022-12-08T08:14:20.414Z","comments":true,"path":"2021/09/03/【论文阅读】Deep Text Classifification Can be Fooled/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Deep%20Text%20Classifification%20Can%20be%20Fooled/","excerpt":"【论文阅读】Deep Text Classifification Can be Fooled 时间：2017 作者：Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li and Wenchang Shi 中国人民大学 Abstract: ​ 在这篇文章，我们提出了一种有效的生成文本对抗样本的方法，并且揭示了一个很重要但被低估的事实：基于DNN的文本分类器很容易被对抗样本攻击。 ​ 具体来说，面对不同的对抗场景，通过计算输入的代价梯度(白盒攻击)或生成一系列被遮挡的测试样本(黑盒攻击)来识别对分类重要的文本项。（这句不是很懂，什么叫’ the text items that are important for classifification‘？） ​ 基于这些项目，我们设计了三种扰动策略，insertion，modification，removal，用于生成对抗样本。实验结果表明基于我们的方法生成的对抗样本可以成功地欺骗主流的在字符等级和单词等级的DNN文本分类器。 ​ 对抗样本可以被扰动到任意理想的类中而不降低其效率。（？）同时，被引入的扰动很难被察觉。 ​","text":"【论文阅读】Deep Text Classifification Can be Fooled 时间：2017 作者：Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li and Wenchang Shi 中国人民大学 Abstract: ​ 在这篇文章，我们提出了一种有效的生成文本对抗样本的方法，并且揭示了一个很重要但被低估的事实：基于DNN的文本分类器很容易被对抗样本攻击。 ​ 具体来说，面对不同的对抗场景，通过计算输入的代价梯度(白盒攻击)或生成一系列被遮挡的测试样本(黑盒攻击)来识别对分类重要的文本项。（这句不是很懂，什么叫’ the text items that are important for classifification‘？） ​ 基于这些项目，我们设计了三种扰动策略，insertion，modification，removal，用于生成对抗样本。实验结果表明基于我们的方法生成的对抗样本可以成功地欺骗主流的在字符等级和单词等级的DNN文本分类器。 ​ 对抗样本可以被扰动到任意理想的类中而不降低其效率。（？）同时，被引入的扰动很难被察觉。 ​ 1. Introduction: ​ 在文本中，即使很小的扰动也会使一个字母或者单词完全变化，这会导致句子不能被辨识。故如果直接将应用于多媒体（图片，音频）的算法应用到文本上，得到的对抗样本的原意就会改变，而且很大程度上变成人类无法理解的句子。 ​ 在这片论文里，我们提出了一种生成对抗样本的有效方法。与直接简单插入扰动相比，我们设计了三种扰动策略：insertion, modifification, and removal，并且引入了自然语言文本水印（natural language watermarking）技术用于生成对抗样本。 ​ 理论上，生成一个好的对抗样本很大程度上依赖于对目标分类模型的信息。在这里我们根据不同情形，使用了白盒攻击和黑盒攻击。 ​ 为了普遍性，我们使用了字符等级的模型和单词等级的模型作为受害者。我们的实验结果证明基于DNN的文本分类器在面对对抗样本攻击时是脆弱的。 2. Target Models and Datasets: ​ 这里使用的文本分类器是Zhang et al. 2015《Character-level Convolutional Networks foe Text Classification》，数据集是Lehmann et al.2014的DBpedia ontology dataset（一个多语言知识库），里面包括560000个训练样本和70000个测试样本，涵盖14个high-level 类，比如公司类、建筑类、电影类等。 ​ 在把样本送进网络前，需要用独热编码法（one-hot representation）对每个字母编码成一个向量。通过网络的六个卷积层、三个全连接层，最终会被分到14个类中。 3. White-Box-Attacks: 3.1 FGSM算法： ​ FGSM是Goodfellow在2015年提出的对图片生成对抗样本的经典算法。使用类似的思路来在文本领域生成对抗样本结果并不好： 3.2 Idenfitying Classification-important Items: ​ 在白盒攻击中，我们需要定位文本中对于分类器的分类结果起到很大作用的文本段（通过计算代价梯度）。在这里，我们使用Hot Training Phrases (HTPs)代表最常使用的短语： ​ HTPS表明了用什么短语/词去做扰动，但是没有说在哪里做。在这里使用Hot Sample Phrases (HSPs)来表明在哪里做扰动。 3.3 Attacking Character-level DNN: ​ 我们的方法是一种targeted攻击，可以指定对抗样本的误导类型。 3.3.1 Insertion Strategy（插入策略）: ​ 在某个HSP前插入一个HTP，就可以达到效果： ​ 由上图可以看到，将某个HTP（historic）插入到HSP（principal stock exchange of Uganda. It was founded）之前，就可以使一个公司的分类文本变为对建筑的分类。 ​ 实际上，我们通常需要进行多次插入，但插入次数过多会影响样本的效用和可读性，为了解决这个问题，这里引入NL水印技术（Natural Language watermarking technique）。该技术可以通过语义或句法操作将所有权水印隐形地嵌入到普通文本中,虽然我们的攻击目标与NL水印有本质的不同，但我们可以借用它的思想来构造对抗样本。实际上，扰动可以看作是一种水印，并以类似的方式嵌入到样本中。 ​ 在这里，我们拓展这个思路，在样本中插入Presupposition(读者熟知的模糊短语)和 semantically empty phrases（可有可无的短语），有没有他们，在读者看来，原文的意思不会改变。 ​ 总的来说，我们考虑将各种HTPS组合成一个语法单元后再嵌入到文本中，新的单元可以是生成的可有可无的资料，或者甚至是不会改变文本原意的伪造的资料。 ​ 特别的，通过互联网搜索或者查找一些数据集，我们可以找到与插入点很相关的资料，包括一些期望的目标分类的HTPs。 ​ 由于我们不能总是找到合适的HTPs，所以提出一个新概念——伪造的事实（forged fact），也就是插入很难证伪的HTPs。例如： ​ 此外，我们排除了伪造的事实，这些事实可以通过检索他们在网上的相反证据而被否认。 3.3.2 Modification Strategy（修改策略）： ​ Moidfication就是轻微修改一些HSP。 ​ 为了让修改不被人类观察者发现，我们采用了typo-based watermarking 技术。具体的说，一个HSP可以通过两种方式来被修改： ​ 1. 从相关的语料库中选择常见的拼写错误来替换它； ​ 2. 把它的一些字符修改成类似的外观（例如小写字母'l'与阿拉伯数字‘1’很像）。 ​ ​ 由上图可以看出，这种方式对分类结果的扰动是巨大的。 3.3.3 Removal Strategy（移除策略）: ​ 移除策略单独使用也许并不能足够有效地影响预测结果，但是可以很大程度上降低原始预测类型的置信度。 ​ 由上图可以看出，移除'British'可以导致原始预测类型的置信度下降了35%。 3.3.4 Combination of Three Strategies: ​ 如图6所示，单靠去除策略改变输出分类往往是困难的。但是，通过与其他策略相结合，可以避免对原文进行过多的修改或插入。在实践中，我们常常结合以上三种策略来制作微妙的对抗样本。 ​ 以图7为例，通过去除一个HSP、插入一个伪造事实和修改一个HSP，可以成功地改变输出分类，但单独应用上述任何扰动都失败。具体来说，删除、插入和修改仅使置信度分别下降27.3 %、17.5 %和10.1 %，保持预测类不变。 4. Black-Box-Attack: 暂略 5. Evaluation： 5.1 我们的方法能否执行有效的源/目标误分类攻击? ​ 答：在众多测试集中，只有DBpedia ontology数据集是一个多分类数据集，故我们在其中随机选取了一些样本： 5.2 所生成的对抗样本能否避免被人类观察者认出来，并同时保持其功能性？ 答：我们找了23个学生。他们对项目不了解，然后每个人给20个文本，其中一半是加扰的。让他们分到14个类中，如果他们觉得哪个文本不对劲，让他们指出来。 ​ 他们总的分类正确率是94.2%，10个对抗样本的正确率是94.8%。所以实用性还是有的。 ​ 他们标注出了240项修改处，其中12项符合真实的修改。但实际上我们做了594处修改。 5.3 我们的方法足够有效吗？ 答：实验中计算梯度和找HTPs花了116小时。14个类的HTPs每个类花了8.29小时。对所有的adversarial示例只执行一次计算。制作一个对抗性的样品大约需要10到20分钟。对于对手来说，获得理想的对抗样本是可以接受的。实际上，她或他愿意花更多的时间来做这件事。 6. Realted Works: ​ 可以做的方向：1.自动生成对抗样本；（然而，Papernot等人(Papernot et al. 2016a)提出了一种基于雅可比矩阵的数据集增强技术，该技术可以在不访问其模型、参数或训练数据的情况下，在有限对输入输出的基础上，为目标dnn提供替代模型。作者还表明，使用替代模型也可以有效地制作对抗样本，以攻击目标DNN。）2.迁移、黑盒攻击； ​","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"DNN","slug":"DNN","permalink":"http://example.com/tags/DNN/"}],"author":"Shaw"},{"title":"Black-Box Attacks against RNN based Malware Detection Algorithms","slug":"【论文阅读】Black-Box Attacks against RNN based Malware Detection Algorithms","date":"2021-09-03T06:08:27.105Z","updated":"2022-07-16T02:09:39.316Z","comments":true,"path":"2021/09/03/【论文阅读】Black-Box Attacks against RNN based Malware Detection Algorithms/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Black-Box%20Attacks%20against%20RNN%20based%20Malware%20Detection%20Algorithms/","excerpt":"【论文阅读】Black-Box Attacks against RNN based Malware Detection Algorithms 时间：2017 作者： Weiwei Hu 北京大学 ​ Ying Tan 北京大学 Abstract： ​ 1. 原文： ​ 最近的研究表明，基于机器学习的恶意软件分类算法在面对对抗样本攻击时表现的十分脆弱。这些工作主要集中于那些利用了混合维度的特征的追踪算法，但一些研究者已经开始使用RNN，基于API特征序列来辨识恶意软件。 ​ 这篇文章提出了一种用于生成对抗样本序列的原创算法，它被用于攻击基于RNN的恶意软件分类系统。对于攻击者来说，通常，知晓目标RNN的内部结构和权重是很难的。于是一个替代的用于近似目标RNN的RNN模型就被训练了出来，接着我们利用这个RNN来从原始序列输入中生成对抗样本序列。 ​ 权威结果表明基于RNN的恶意软件分类算法不能追踪大多数我们所生成的恶意对抗样本，这意味着我们生成的模型可以很有效的规避追踪算法。 ​ 2. 总结： ​ 一个对基于RNN的恶意样本分类器的灰盒攻击，有三个RNN，受害者RNN（源RNN），替代RNN，对抗样本生成RNN。","text":"【论文阅读】Black-Box Attacks against RNN based Malware Detection Algorithms 时间：2017 作者： Weiwei Hu 北京大学 ​ Ying Tan 北京大学 Abstract： ​ 1. 原文： ​ 最近的研究表明，基于机器学习的恶意软件分类算法在面对对抗样本攻击时表现的十分脆弱。这些工作主要集中于那些利用了混合维度的特征的追踪算法，但一些研究者已经开始使用RNN，基于API特征序列来辨识恶意软件。 ​ 这篇文章提出了一种用于生成对抗样本序列的原创算法，它被用于攻击基于RNN的恶意软件分类系统。对于攻击者来说，通常，知晓目标RNN的内部结构和权重是很难的。于是一个替代的用于近似目标RNN的RNN模型就被训练了出来，接着我们利用这个RNN来从原始序列输入中生成对抗样本序列。 ​ 权威结果表明基于RNN的恶意软件分类算法不能追踪大多数我们所生成的恶意对抗样本，这意味着我们生成的模型可以很有效的规避追踪算法。 ​ 2. 总结： ​ 一个对基于RNN的恶意样本分类器的灰盒攻击，有三个RNN，受害者RNN（源RNN），替代RNN，对抗样本生成RNN。 1. Introduction: 现有的基于N机器学习的恶意软件追踪算法主要将程序表现为固定维度的特征向量，然后将其分类为无害程序和恶意软件； 举例，利用API的调用序列，或者不被调用的API序列进行分类； 【11】展现了，基于固定维度特征来进行恶意样本分类的算法，面对对抗样本的攻击是脆弱的； 最近也有利用RNN进行恶意样本追踪与分类的，RNN的输入就是API序列。 2. Adversarial Examples: ​ 一些其它的针对序列的对抗样本攻击： Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adver\u0002 sarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, pages 49–54. IEEE, 2016. Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. Adversarial perturbations against deep neural networks for malware classifification. arXiv preprint arXiv:1606.04435, 2016. 4. Attacking RNN based Malware Detection Algorithms 5. 实验 ​ Adam 用于训练所有模型； ​ LSTM由于其在处理长序列的优秀表现，也被应用在实验的所有RNN中。 5.1 数据集： ​ 来源：https://malwr.com/ （一个恶意样本分析网站，爬取180个项目，该网站可以分析用户上传的项目，并给出其API序列，网站中70%的项目都是恶意样本） ​ 数据集划分：为了模拟真实的测试环境，数据集划分如下：（30%+10%）用于生成RNN，（30%+10%）用于受害者RNN，20%用于测试。 5.2 受害者RNN： ​ 尝试了不同模型： ​ 结论如下： 与LSTM相比，BiLSTM不能提升模型的分类表现； 与Average-Pooling相比，注意力机制的效果更好； 5.3 生成（对抗样本）RNN测试结果： ​ 介绍参数规范： The hyper-parameters of the generative RNN and the substitute RNN were tuned separately for each black-box victim RNN. The learning rate and the regularization coeffificient were chosen by line search along the direction 0.01, 0.001, et al.. The Gumbel-Softmax temperature was searched in the range [1, 100]. Actually, the decoder length L in the generative RNN is also a kind of regularization coeffificient. A large L will make the generative RNN have strong representation ability, but the whole adversarial sequences will become too long, and the generative RNN’s size may exceed the capacity of the GPU memory. Therefore, in our experiments we set L to 1. ​ ​ 给出实验结果： 对于所有RNN模型，攻击都十分有效； 于LSTM的攻击效果最差，故替代RNN对LSTM的拟合效果并不好； 训练集与测试集的测试效果差别不大， 模型泛化能力强； 即使更换了模型与训练数据集，对抗样本仍效果很好。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"},{"name":"RNN","slug":"RNN","permalink":"http://example.com/tags/RNN/"}],"author":"Shaw"},{"title":"Automatically Evading Classififiers----A Case Study on PDF Malware Classififiers","slug":"【论文阅读】Automatically Evading Classififiers----A Case Study on PDF Malware Classififiers","date":"2021-09-03T06:08:27.103Z","updated":"2021-09-03T06:23:11.007Z","comments":true,"path":"2021/09/03/【论文阅读】Automatically Evading Classififiers----A Case Study on PDF Malware Classififiers/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Automatically%20Evading%20Classififiers----A%20Case%20Study%20on%20PDF%20Malware%20Classififiers/","excerpt":"","text":"【论文阅读】Automatically Evading Classififiers----A Case Study on PDF Malware Classififiers 时间：2016 作者：Weilin Xu, Yanjun Qi, and David Evans 弗吉尼亚大学 会议：NDSS（ccf_B类） 总结： 白盒黑盒？ 黑盒攻击，需要知道生成样本在目标模型中的输出（分类分数）和目标模型所使用的特征（粗略知道）； 针对什么目标？ 仅仅使用表层特征的分类器； 攻击方法？ 3.1 如何制造对抗样本？ ​ 使用遗传算法（GP-BASED）进行随机扰动 3.2 如何判别对抗样本的恶意能力？ ​ 使用oracle Abstract: ​ 在本文，我们提出了一个一般化的方法来检验分类器的鲁棒性，通过在两个PDF恶意样本分类器，PDFrate和Hidost上来检验。其关键就是随机控制一个恶意样本来找到一个对抗样本。 ​ 我们的方法可以自动地对500个恶意样本种子中找到对于两个PDF分类器的对抗样本，我们的结果提出了一个严重的疑问，基于表面特征的分类器在面对对抗样本时是否还有效？ 1. Introduction: ​ 主要贡献： 1. 提出了一个一般化的方法用于自动寻找分类器的对抗样本； 2. 制作了一个原型系统用于自动生成对抗样本； 3. 我们的系统在对500个恶意样本种子寻找对抗样本的过程中，达到了100%的准确率。 2. Overview： 2.1 Finding Evasive Samples： ​ 整体思路： ​ ​ oracle用于判断一个样本是否具有恶意行为； 3. PDF Malware and Classifiers 3.1 PDFmalware: ​ PDF文件的整体结构： ​ ​ 早些的PDF恶意样本一般使用JavaScript嵌入，用户双击打开时出发执行恶意脚本。 ​ 因为不是所有的PDF恶意样本都是嵌入了JavaScript代码，最近的一些PDF恶意分类器就着重于PDF文件的结构化特征。在本文，我们的目标就是攻击这些有代表性的基于文件结构化特征的分类器。 3.2 Target Classififiers： ​ PDFrate：一个使用随机森林算法的分类器。 ​ Hidost:一个SVM分类器。 ​ 4. Evading PDF Malware Classifiers： 5. Experiment: 5.1 Dataset: ​ 5.2 Test： ​","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"},{"name":"PDF","slug":"PDF","permalink":"http://example.com/tags/PDF/"}],"author":"Shaw"},{"title":"AD nlp Survey","slug":"【论文阅读】AD nlp Survey","date":"2021-09-03T06:08:27.101Z","updated":"2022-07-16T02:09:27.229Z","comments":true,"path":"2021/09/03/【论文阅读】AD nlp Survey/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91AD%20nlp%20Survey/","excerpt":"【论文阅读】AD nlp Survey 作者：Wei Emma Zhang（阿德莱德大学，澳大利亚） ​ QUAN Z. SHENG（麦考瑞大学，澳大利亚） ​ AHOUD ALHAZMI（麦考瑞大学，澳大利亚） ​ 李晨亮（武汉大学，中国） 1. 关键词：DNN，对抗样本，文本数据（textual data），NLP 2. 摘要： 传统对抗样本基本都针对计算机视觉领域； 本调查提供针对基于DNNs的NLP对抗样本攻击； 由于CV与NLP本身不同，方法不能直接移植； 集成了截止2017年所有的相关成果，综合性地总结，分析，讨论了40个代表性工作； 简单介绍了CV和NLP相关知识。","text":"【论文阅读】AD nlp Survey 作者：Wei Emma Zhang（阿德莱德大学，澳大利亚） ​ QUAN Z. SHENG（麦考瑞大学，澳大利亚） ​ AHOUD ALHAZMI（麦考瑞大学，澳大利亚） ​ 李晨亮（武汉大学，中国） 1. 关键词：DNN，对抗样本，文本数据（textual data），NLP 2. 摘要： 传统对抗样本基本都针对计算机视觉领域； 本调查提供针对基于DNNs的NLP对抗样本攻击； 由于CV与NLP本身不同，方法不能直接移植； 集成了截止2017年所有的相关成果，综合性地总结，分析，讨论了40个代表性工作； 简单介绍了CV和NLP相关知识。 ### 3.Introduction: 简单介绍了对抗样本； 关于对抗样本的研究可以简单分为三类： ① 通过使用微小扰动来欺骗DNN，以此来评估它； ② 刻意改变DNN的输出； ③ 检测DNN中过敏感和过迟钝的点，寻找防御攻击的方法。 ==不能直接使用基于CV的对抗样本生成方法的原因：== 直接将对图像攻击的对抗样本生成方法应用到文本上，将得到毫无意义的词语和句子片段。这是因为在对图像的对抗样本生成中，即使略微改变每个像素的灰度，肉眼也可以识别原来的图像；但是对于文本串来说，即使改变一个字母，语句的语义也将完全不同或出错。 ==相关研究：== Reference [i] = 【i】 ​ ① 【9】：对针对不同类别的机器学习系统的攻击与防御做了综合性概述，提出了一种用于辨识和分析这些攻击的分类方法，并将将这些攻击应用到基于机器学习的应用上来证明这些攻击或者防御手段的有效性。例如，一个统计垃圾邮件过滤器。 ​ ② 【13】：作者俯瞰了近十年（2008-2018）对抗样本攻击的发展史，聚焦点在于CV和网络空间安全。对非深度学习算法和深度学习算法都做了介绍，也从安全的角度仔细分析了这些攻击和防御手段的影响。 ​ ③ 【79】：与【13】阐述的问题类似，从数据驱动的角度。 ​ ④ 【154】：聚焦于对抗样本在深度学习模型上的使用。介绍了最近的几种不同的在应用上对DNN的攻击，同时全面调查了防御方法。但是，其只讨论了对抗样本在图像分类和物品识别上的攻击。 ​ ⑤ 【2】：详细阐述了对抗样本在CV上的应用，是一篇应用程序主导的调查。 ​ ⑥ 【35】：从安全的角度阐述了对抗样本的防御手段。（不仅从机器学习算法或者神经模型上，从所有与安全相关的应用上阐述对抗样本防御）作者发现现有的与防御相关的安全工作缺乏清晰的对攻击如何与真实安全问题相关联的动机和解释，以及这些攻击和防御如何被有意义地衡量，故提出了一种分类方法用于衡量这些。 4. Overview （对抗样本攻击 and 深度学习在NLP中的应用）: 给出了DNN，Perturbations，Adversarial Examples的定义； 介绍了Treat Model： 2.1 Granularity（颗粒度）:攻击的颗粒度指的是对抗样本生成的数据等级，例如对图像数据通常是像素，对文本数据就是字母，单词，句子嵌入等级。 2.2 Motivation（动机）：生成对抗样本的动机通常有两种，攻击和防御：1.攻击的目的是检验DNN的健壮性；2. 防御的目的是使DNN更加稳固，第五部会给出更详细的讲解。 介绍了Measurements（评价adversarial attack的方法）： 3.1 控制扰动（Perturbation Constraint）： ​ 根据前面所述，扰动 η 应该不影响样本原来的真实分类，故如果一个分类器是理想的，那么扰动后的样本应不影响其分类结果； η 同时也不能太小，以避免对目标DNN没有影响。在理想情况下，有效扰动是在一定范围内最有效果的噪声。 ​ 【132】首次在图像对抗样本攻击中约束了(x + η) ∈ [0, 1]n 的范围，以保证对抗样本与原始数据有着相同的像素等级。 ​ 【40】简化了问题的解决方法，并使用了无穷范数来限制扰动，==这受到直觉的启发，即一个不改变任何特定像素的扰动量超过一定量 ϵ 就不能改变输出类。==（PS:WHY？）无穷范数在图像/物品分类识别任务中是足够有效的，其他的范数，例如L0和L2范数，过去被用于在对CV的DNN攻击中限制扰动。在文本对抗样本攻击中，这有所不同，第3.3节会给出更多细节。 3.2 评估攻击的有效性（Attack Evaluation）： ​ 对抗样本攻击旨在降低DNNs的性能，因此，评估攻击的有效性是基于不同任务的性能指标。例如，分类任务中有评价指标准确度，F1-score，AUC-score。在本文中，我们将针对不同NLP的评价标准作为超范围内容，并建议读者参考特定的信息。 ​ ==以上是总体分类与信息== ​ ==以下是深度学习在NLP中的应用== ​ 除了向前传播的神经网络和CNN，RNN及其变式由于其天然的处理序列的能力，也被用于NLP中。 近几年深度学习对NLP的重大影响： 1.1 序列学习（sequence-to-sequence learning） 1.2 注意力机制（attention mechanism） 1.3 强化学习（reinforcement learning）和生成模型（generative models） 具体详细的神经网络在NLP中的应用见【100】，【152】 Feed-Forward Networks: 缺点：不能很好地处理对于词语顺序很重要的文本序列，因为其并不记录元素的顺序。为了评价其健壮性，往往针对专门设计的前馈网络生成对抗实例，【3】，【43】，【44】作者研究了指定的恶意软件检测模型。 CNN： ​ CNN识别本地预测因子并将它们组合在一起，为输入生成一个固定大小的向量，该向量包含数据中最重要或最重要的信息方面。 ​ CNN对顺序敏感，因此，它擅长做计算机视觉，随后被广泛用于NLP应用。 ​ 卷积操作被简直在词的序列方向上，而不是词的嵌入。 ​ 两个经典工作：1. 【59】使用CNN和Word2Vec进行句子分类 2.【156】使用CNN和热独编码进行文本分类。 RNN： ​ 主要介绍RNN及其变式（LSTM，GRU） Seq2Seq（sequence-to-sequence learning）： ​ Seq2Seq模型具有优越的能力，能够为具有编码器-解码器结构的给定序列信息生成另一个序列信息. ​ 通常，一个Seq2seq由两个RNN结构组成，一个用于编码，一个用于解码。VHRED是一个最近很受欢迎的Seq2seq模型，它利用子序列之间的复杂依赖关系生成序列。 ​ 【24】是最初的使用Seq2seq模型的神经机器翻译模型（NMT）之一； ​ 【63】是一个最近提出的 seq2seq NMT模型，是此领域的benchmark； ​ 【22,30,98,127】有对其的攻击。 Attention Models： ​ 注意力机制最初被设计用来克服seq2seq模型中对长序列编码的问题。 ​ 注意力允许解码器回溯源序列的隐藏状态，然后，隐藏状态提供一个加权平均作为解码器的额外输入。 Reinforcement Learning Models： ​ 强化学习通过在代理执行离散动作后给予奖励来训练代理，在NLP中，强化学习框架通常由一个代理（DNN），一个策略部分（用于指导动作）和奖励组成。 ​ 代理基于策略做出一个动作（例如预测序列中下一个单词的位置），然后相应地更新其内部状态，直到到达序列的末尾，在这里奖励已经被计算完成。 ​ 强化学习需要正确处理每一步的动作和状态，这可能会限制模型的表现力和学习规模。但它在面向任务的对话系统中获得了很多好处，因为它们在决策过程共享着同一根本原则。 Deep Generative Models（深层生成模型）： ​ 近些年，两种深层生成模型获得了很多关注：Genera\u0002tive Adversarial Networks (GANs) 【39】 and Variational Auto-Encoders (VAEs) ​ 其可以在潜在空间中生成与真实数据分厂相似的数据样例，在NLP领域，它们被用来生成文本。 ​ 8.1 GANS: ​ Gans由两个对抗网络组成：生成器（generator）和鉴别器（discriminator）。鉴别器的作用是鉴别真实样本和生成样本，生成器的作用是生成很真实的，用于欺骗鉴别器的样本。 ​ Gan使用min-max loss function来同步训练两个神经网络。 ​ 8.2 VAES： ​ Vaes由编码器（encoder）和生成器（generator）组成。编码器的作用是对输入编码为潜在空间，生成器的作用是从潜在空间中生成样本。 深度模型都不是很好训练，这个缺点阻碍了其在真实世界的应用中的广泛应用，尽管他们已经被用于生成文本，但目前没有工作去用对抗样本检验它们的健壮性。 5. From image to text: 一. 构造对抗样本： 、L-BFGS: ​ Szegedy【132】等人首次证明了可以通过对图像添加小量的人类察觉不到的扰动误导深度神经网络图像分类器做出错误的分类。他们首先尝试求解让神经网络做出误分类的最小扰动的方程。作者认为，深度神经网络所具有的强大的非线性表达能力和模型的过拟合是可能产生对抗性样本原因之一。 FGSM（Fast Gradient Sign Method）： ​ L-BFGS很有效但成本高昂，这使Goodfellow【40】等人找到一个简化问题的方法。 JSMA（Jacobian Saliency Map Adversary）： ​ 与FGSM利用梯度攻击不同，Papernot【105】等人使用forward derivatives（远期衍生物？）生成对抗样本。这个方法通过使用其雅克比矩阵来评估神经模型对每个输入部分的输出敏感性。 DeepFool： ​ DeepFool是一种迭代的L2正则化算法，作者先假设神经网络是线性的，因此可以使用一个超平面来分离类。作者简化了问题并且基于以上假设找到了问题最优解，并构建了对抗样本、 ​ 为了解决神经网络是非线性的事实，作者重复他们的步骤直到一个真正的对抗样本被生成了。 PS：正则化：(23 封私信 / 54 条消息) 机器学习中常常提到的正则化到底是什么意思？ - 知乎 (zhihu.com) Subsititute Attack： ​ 前面四中攻击方式都是白盒攻击， Papernot【104】等人提出了黑盒攻击策略，他们训练了一个与目标模型决策边界相似的替代模型，对此替代模型进行白盒攻击，生成相应对抗样本。 ​ 在生成对抗样本的过程中，他们使用了FSGM和JSMA。 GAN-like Attack： ​ 这是一种通过深度生成模型的黑盒攻击方法，Zhao【157】等人首先基于数据集 X 训练了一个生成模型WGAN，WGAN可以生成与X分布相同的数据点。 二. 对图像DNN攻击与对文本DNN攻击的对比： ​ 1. 二者的主要不同： ​ 1.1 离散与连续输入： ​ 图像输入是连续的而文本输入是离散的，在图像输入中，通常使用Lp来衡量原始数据点和扰动点的距离，但是由于文本输入是离散的，很难定义文本上的扰动大小（==为什么？==）。这就需要构造对文本扰动的衡量方法。还有一种方式是将文本输入当做连续值，然后应用CV方法，在3.3节上将会详细讨论。 ​ 1.2 可察觉与不可察觉： ​ 与图像相比，文本数据即使更改一个字母也会造成很大变化，故即使做很小的扰动，也可以被很明显的察觉到。 ​ 1.3 有语义和无语义： ​ 原理同上，在文本中做很小的改动往往会极大地影响到文本的语法和语义信息。 ​ 基于以上不同，目前主流对文本DNN的攻击有两种：1. 调整图像DNN的攻击方法，添加额外限制；2. 使用新技术提出一个新方法。 三. 向量化文本输入 and 扰动的衡量方法 三种向量化文本输入的方法： 1.1 基于计数的编码（Word-Count-based Encoding）： ​ ① BOW（Bag-of-words）方法，将一个文档中出现的词语编号为向量的0,1,2.....i维度，每个维度的值代表词语出现的次数。（缺点：不能记住词语顺序） ​ ② Term frequency-inverse document frequency (TF-IDF) ，具体见： TF-IDF算法介绍及实现_Asia-Lee-CSDN博客_tf-idf ​ 1.2 热独编码（One-hot Encoding）: ​ 具体介绍略。 ​ 由于普通顺序编码的值存在大小关系，当模型得到输入后会将其当做实际值来处理，这就使得原本平行的数据有了大小关系，独热编码巧妙地解决了这个问题，使得所有单词或者字母低位平等。 ​ 1.3 稠密编码： ​ Word2Vec使用连续BOW模型和skip-gram 模型来做代码嵌入。 ​ 一个潜在的假设是，出现在相似语境下的词语有着相似的含义。 词嵌入在一定程度上缓解了文本数据向量化的离散性和数据稀疏性问题【36】，词嵌入的扩展如doc2vec和paragraph2vec【69】将句子/段落编码为稠密向量。 扰动的衡量方法： 2.1 基于范数的方法（Norm-based measurement）： ​ 直接使用范数需要输入数据是连续的。一个解决方法是使用连续且稠密的表示方法（如嵌入），但这通常会得到无意义的文本。 2.2 基于语法和句法的方法（Grammar and syntax related measurement）： ​ 通过确认文本语法的正确性来保证对抗样本不易被识别。 ​ 可以使用Perplexity【91】，Paraphrase（4.3.3）确保对抗样本的有效性。 2.3 基于语义保持的方法（Semantic-preserving measurement）： ​ ① 计算欧拉距离: ​ ② 计算Cosine Similarity（余弦相似度）： ​ 2.4 基于编辑距离的方法： ​ 编辑距离（Edit Distance），又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。一般来说，编辑距离越小，两个串的相似度越大。 ​ 不同定义使用不同的转换操作。 2.5 基于Jaccard相似系数的方法： ​ Jaccard相似系数定义见百度百科。 ​ 就是把两个集合的交集除以两个集合的并集，简单地看集合中的元素是不是大量相同。 6. Attacking Neural Models in NLP: 常见攻击方法： 白盒，黑盒...... ​ 提供了数据集来源，但没有提供生成对抗样本的数据集，所提供的的数据集仅用于评估攻击效果。 7. Defense: 背景：两种在DNN中常用的防御方法：1. 对抗训练(adversarial training) 2. 知识蒸馏（knowledge distillation）. Knowledge distillation：【经典简读】知识蒸馏(Knowledge Distillation) 经典之作 - 知乎 (zhihu.com) 一. 对抗训练 数据增强（Data Augmentation）： ​ 数据增强将原始数据集加上对抗样本一起，在训练的过程中让模型见到更多数据，数据增强常被用来对抗黑盒攻击，实现的方式是通过在被攻击的DNN上使用对抗样本增加额外的epoch。 ​ 【54】证明了这种方法是有效的，但仅仅对同一对抗样本有效（数据增强中的样本与测试对抗样本） ​ 【142】也提出了类似的观点 ​ 【56】作者提出了3种生成更多具有不同特征的数据的方法 ​ 【12】作者提出了一种新的数据增强的方法，它将平均字符嵌入作为一个词表示，并将其纳入输入。这种方法本质上对字符的置乱不敏感，例如交换、mid和Rand，因此可以抵抗这些置乱攻击引起的噪声。但是，这种防御方法对不是针对字符顺序的扰乱不起作用。 模型正则化（Model Regularization）： 模型正则化将生成的对抗样本实例作为正则化器： 模型正则化_少年吉的博客-CSDN博客_模型正则化 正则化( Regularization)的目的在于提高模型在未知测试数据上的泛化力,避免参数过拟合。 健壮性最优化方法（Robust Optimization）： Madry【84】等人将DNN学习问题转化为了一个包含内非凹最大化问题(攻击)和外非凸最小化问题(防御)的健壮性优化问题。 二. 知识蒸馏 ​ 详见论文和博客。 8.Discuss and Open issues 可察觉性（Perceivability）： 见前文 可转移性（Transferability）： no-tatgeted攻击的可转移性更强。 可转移性可以在三个地方体现： ​ 2.1 同样的架构，不同的数据； ​ 2.2 同样的应用场景，不同的架构； ​ 2.3 不同的架构，不同的数据。 尽管现有的工作囊括了以上三种情况，但对抗样本攻击的可移植性效果仍不好，需要更多的工作。 自动化（Automation）： ​ 一些工作可以做到对抗样本的自动生成，而另一些则不行。 ​ 在白盒攻击中，利用DNN的损失函数可以自动识别文本中受影响最大的点(如字符、词)，以此做到在文本中自动化。 ​ 在黑盒攻击中，一些攻击例如替代训练（substitution train）可以训练出一个替代用模型，对其进行白盒攻击，也可以实现自动化。但是大多数对抗样本的生成都是人工生成。【54】会关联人工选择的无意义的文本段落来欺骗阅读理解系统，以此来发现DNN的脆弱性。很多研究工作跟随【54】，其目的不是实际攻击，而是更多的在检测目标网络的健壮性上，这些人工工作是耗时且不切实际的。我们相信在未来更多的努力会用来克服这个困难。 新架构（New Architectures）： ​ 尽管大多数普通的文本DNN都注意到了对抗样本攻击，但是很多DNN并没有被攻击过。例如GANS与VAES，它们被用作生成文本。深度生成模型需要更复杂的技巧去训练，这就可以解释为什么这些技术忽略了对抗样本攻击。未来的工作可能考虑对这些DNN进行对抗样本攻击。 ​ 注意力机制（Attention Mechanism）目前是大多数序列模型的标准组成部分，但是没有工作去检验注意力机制本身。故可能的攻击工作要么攻击包含注意的整体系统，要么利用注意分数来识别干扰词【14】。 迭代 VS 一次性（Iterative versus One-of）： ​ 迭代攻击：效果好，耗时长； ​ 一次性攻击：效果略差，耗时短。 ​ 在设计攻击方法时，攻击者需要仔细考虑效果与效率的平衡。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"}],"author":"Shaw"},{"title":"A novel Android malware detection system-adaption of flter‑based  feature selection methods","slug":"【论文阅读】A novel Android malware detection system adaption of flter‑based  feature selection methods","date":"2021-09-03T06:08:27.099Z","updated":"2022-07-16T02:09:31.345Z","comments":true,"path":"2021/09/03/【论文阅读】A novel Android malware detection system adaption of flter‑based  feature selection methods/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91A%20novel%20Android%20malware%20detection%20system%20adaption%20of%20flter%E2%80%91based%20%20feature%20selection%20methods/","excerpt":"【论文阅读】A novel Android malware detection system: adaption of flter‑based feature selection methods 时间：2021 作者： Durmuş Özkan Şahin Oğuz Emre Kural · Sedat Akleylek Erdal Kılıç 总结： 二分类，静态代码检测； 创新点主要在特征提取（已经有的方法+文本分类的方法）上，分类器用的各种现成的方法； Abstract: ​ 在本研究中，提出了一个基于过滤器特征选择方法的，原创的安卓端恶意软件追踪系统。 ​ 该方法是一个在机器学习的基础上的静态安卓恶意软件追踪方法。在所开发的系统中，使用应用程序文件中提取的权限作为特征。八个不同的特征选择方法被用于维度降低，以减少运行时间，提升机器学习算法的效率。 ​ 其中四种方法应用于安卓恶意样本分类，其余四种方法是从文本分类研究中采用的，其从提取特征和分类结果两方面对方法进行了比较，在对结果进行检验时，表明所采用的方法提高了分类算法的效率，可以在本领域中使用。","text":"【论文阅读】A novel Android malware detection system: adaption of flter‑based feature selection methods 时间：2021 作者： Durmuş Özkan Şahin Oğuz Emre Kural · Sedat Akleylek Erdal Kılıç 总结： 二分类，静态代码检测； 创新点主要在特征提取（已经有的方法+文本分类的方法）上，分类器用的各种现成的方法； Abstract: ​ 在本研究中，提出了一个基于过滤器特征选择方法的，原创的安卓端恶意软件追踪系统。 ​ 该方法是一个在机器学习的基础上的静态安卓恶意软件追踪方法。在所开发的系统中，使用应用程序文件中提取的权限作为特征。八个不同的特征选择方法被用于维度降低，以减少运行时间，提升机器学习算法的效率。 ​ 其中四种方法应用于安卓恶意样本分类，其余四种方法是从文本分类研究中采用的，其从提取特征和分类结果两方面对方法进行了比较，在对结果进行检验时，表明所采用的方法提高了分类算法的效率，可以在本领域中使用。 ### 1. Introduction: #### 1.1 如何提取相关特征？ ​ Shabtai (2012)介绍了Andromaly架构，其中包含不同的特征选取方法和分类方法。 ​ Zhao（2015）提出了一个特征选择方法FrequelSel，其基于无害样本和恶意样本的频率特征差异。 ​ Xu （2016）提出了一个新的安卓恶意样本追踪方法ICCdetector，他们使用CFS（Correlation Based Feature Selection）在许多特征向量中做特征提取。 ​ Morales-Ortega（2016）提出了一种可以在恶意软件分析和检测设备上本地运行的方法，他们使用不同的特征选择方法和分类方法进行了对比实验。 ​ Bhattacharya and Goswami (2018) 提出了一种通过通过混合基于community的粗略设置特征选择方法（community-based rough set feature selection method）来进行特征选择的新方法。 ​ Peynirci et al. (2020) 提出了Delta IDF方法，其通过选择具有最高IDF（NLP中的）无害样本和最低IDF的恶意样本来提取特征。在特征提取中使用了字符串，API调用序列，权限等来作为特征。 ​ Ananya et al. (2020) 提出了一种安卓恶意样本追踪的动态分析技术。 ​ Kouliaridis et al. (2021)使用了两个特征选取算法和八个不同的分类器进行了比较试验。 ​ Jung et al. (2021) 在Gini Importance 和 domaind 知识上进行了特征提取。使用了API调用序列和应用权限。 ​ Liu et al. (2021)使用非监督学习进行了安卓恶意样本的特征提取。 #### 1.2 Contribution: ​ 主要贡献： 1. 提出了一个基于过滤器特征选择方法的，原创的安卓端恶意软件追踪系统（静态检查）； 2. 基于文本分类的特征选择方法对现有的属性选择方法进行替代是适应于Android恶意软件检测系统的。因此，不使用所有的权限，而是选择了最具特色的权限，提高了分类算法的性能； 3. 比较给出各度量得到的允许度和分类结果。在检查结果时，所提出的系统使用的特征比现有的检测系统少； 4. 从我们所采用的特征提取方法中得到的结果总体上所得到的特征比其他方法少； 5. 实验结果更好，run的时间更短，分类效果更佳； 6. 一些矩阵与贪婪方法相结合形成各种属性子集。这些创建的属性子集在用大量classifer进行测试时表现出了显著的性能。 2. Preliminaries： 2.1 Feature extraction（如何处理APK文件）： 2.2 Feature selection（提取特征）： ​ 特征选择技术分为三类：flter-based，wrapper-based，embedded methods. ​ 在基于过滤的技术中，就是在所有属性中选择最好的k个属性，而不使用剩余的属性。各种基于统计或信息论的技术被用来寻找最佳的k个特征。 ​ 基于Wrapper的技术在操作上与过滤技术类似，但在搜索策略上，选择是用遗传算法等启发式方法代替统计技术进行的。 ​ 特征选择过程是在机器学习算法的训练阶段进行的。特征选择是通过找到影响在训练阶段创建的模型性能的最佳子集来进行的。 2.3 The proposed Android malware detection system： ​ 3. Experimental settings： 3.1 datasets： ​ 3000恶意样本（VirusShare dataset ），3000无害样本（APKPure） 3.2 Classifcation algorithms 3.3 Performance measure 4. Results and discussions: 4.1 Results of performed experiments: 数据处理： 分类结果（部分）： 总： 与其他方法比较： ​","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"},{"name":"Android","slug":"Android","permalink":"http://example.com/tags/Android/"}],"author":"Shaw"},{"title":"A Benchmark API Call Dataset For Windows PE Malware Classification","slug":"【论文阅读】A Benchmark API Call Dataset For Windows PE Malware Classification","date":"2021-09-03T06:08:27.096Z","updated":"2022-07-16T02:09:35.469Z","comments":true,"path":"2021/09/03/【论文阅读】A Benchmark API Call Dataset For Windows PE Malware Classification/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91A%20Benchmark%20API%20Call%20Dataset%20For%20Windows%20PE%20Malware%20Classification/","excerpt":"【论文阅读】A Benchmark API Call Dataset For Windows PE Malware Classification 作者：Ferhat Ozgur Catak（土耳其） ​ Ahmet Faruk Yazi（土耳其） 时间：2021.2.23 关键词：恶意软件分析，网络空间安全，数据集，沙箱环境，恶意软件分类 1. Abstract ​ 在Windows操作系统中，系统API调用的使用在监控恶意PE程序中是一个很有前途的方法。这个方法被定义为在安全隔离的沙箱环境中运行恶意软件，记录其调用的Windows系统API，再顺序分析这些调用。 ​ 在这里，我们在隔离沙箱中分析了7107个属于不同家族（病毒，后门，木马等）的恶意软件，并把这些分析结果转化为了不同分类算法和方法可以使用的形式。 ​ 首先，我们会解释如何得到这些恶意软件；其次，我们会解释如何将这些软件捆绑至家族中；最后，我们会描述如何使用这些数据集来通过不同的方法实现恶意软件的分类。","text":"【论文阅读】A Benchmark API Call Dataset For Windows PE Malware Classification 作者：Ferhat Ozgur Catak（土耳其） ​ Ahmet Faruk Yazi（土耳其） 时间：2021.2.23 关键词：恶意软件分析，网络空间安全，数据集，沙箱环境，恶意软件分类 1. Abstract ​ 在Windows操作系统中，系统API调用的使用在监控恶意PE程序中是一个很有前途的方法。这个方法被定义为在安全隔离的沙箱环境中运行恶意软件，记录其调用的Windows系统API，再顺序分析这些调用。 ​ 在这里，我们在隔离沙箱中分析了7107个属于不同家族（病毒，后门，木马等）的恶意软件，并把这些分析结果转化为了不同分类算法和方法可以使用的形式。 ​ 首先，我们会解释如何得到这些恶意软件；其次，我们会解释如何将这些软件捆绑至家族中；最后，我们会描述如何使用这些数据集来通过不同的方法实现恶意软件的分类。 ### 2. Introduction #### 2.1 简单介绍了恶意软件 #### 2.2 恶意软件与恶意软件识别之间的竞争 ​ 相互促进 #### 2.3 变形恶意软件（Metamorphic malware） ​ 恶意软件家族里很先进的一种，这种软件可以持续不断的改变自身源代码以此改变自身结构，通过这种方式来改变自身代码特征。还有，这种软件可能还可以通过强度反算（counter-analysis）来识别自身运行的环境，以此来隐藏自身的恶意功能。 ​ 变形恶意软件很难识别。 #### 2.4 恶意软件的识别： ​ 所有恶意软件都会有恶意行为以达成其目的，如果可以很好的分析恶意行为，就可以做成恶意软件的识别与分类。 ​ 恶意软件的识别包括了很多需要解决的问题，例如在汇编中不正确的跳转操作码，PE文本段代码隐藏，代码加密。本研究收集了现有的恶意软件及其变式，例如WannaCry，Zeus，特别是在Github上。 ​ 我们通过在VirusTotal网站上寻找每个恶意软件的哈希值，从而获得了得到了其家族类。 ​ 最后，所有我们记录的行为都是在Cuckoo沙盒环境中运行的。 ​ 我们发现几乎所有恶意软件都会使用很多方法改变其行为，但即使这样，恶意软件还是有一个目标，有一个确定的模式来达到此目标。还有，恶意软件会做出一些不必要的API调用，但其还是可以被一个训练好的分析器识别，因为其行为模式是相同的。 ​ 恶意软件分析被视为网络空间安全的一个分支，其由两方面组成： ##### 1. 静态分析 ： ​ 静态分析可以可以定义为通过执行一个孤立的环境检查可执行文件而不查看实际指令。例如MD5校验和，其通过反病毒检测攻击识别，查找字符串。 ##### 2. 动态分析 ​ 动态分析指运行恶意程序来理解其功能，观察其表现，识别其技术指标。几乎所有的重要行为都包含API调用序列。 ​ 大多数动态分析领域的研究都只关注分类算法，有个基本问题是没有标准的数据集来检查所提出模型的效率。 ​ 我们在Github上分享了我们的数据集：https://github.com/ocatak/malware_api_class ，该数据集包含了基于Cuckoo沙箱的已知恶意软件执行和基于VirusTotal的文件MD5特征分类的原始数据。 3. Methods 3.1 Windows API Calls： ​ 软件安全知识，略 3.2 Cuckoo SandBox ​ 免费软件，高度集成，开源，可以自动分析Winodws,OS X,Linux,Android系统下的恶意文件。 3.3 VirusTotal ​ 可以在线免费分析文件或者URL。其提供了一个API，可以不通过浏览器来提供分析结果，可以自动分析。其以JSON文件的形式提供分析结果，不同反病毒应用引擎和浏览器的分析结果会分开存放。 3.4 数据集生成 ​ 本文的数据集有着简单明了的结构。数据集以CVS格式文件提供来提高互操作性，而且并不需要特定的软件或者库来读取他们。数据由来自不同Github页面的Git命令实施收集，数据集中的每一行都是在沙箱中分析的Windows操作系统的API调用序列。 ​ 数据集的生成过程如下： ​ 1. 沙箱环境准备： ​ 分析机器使用Ubuntu系统，将Cuckoo沙箱安装在其中，分析机运行虚拟服务，Windows操作系统就运行在虚拟服务上，同时关掉防火墙，系统升级。 ​ 2. 分析恶意软件: ​ 虚拟机中同时运行超过20000个恶意软件，应用程序会将每个恶意软件的分析结果写入MongoDB数据库，分析结果中包含恶意软件的行为数据，这些数据都是恶意软件在Win7上的API调用请求。 ​ 3. 处理API调用： ​ 我们在数据集中收集到了342种API调用，这些调用会被以0-341来标记，以此生成一个新数据集。我们使用了该数据集中至少有10个不同API调用的恶意软件的分析结果。 ​ 4. 使用Virus Total公用API分析恶意软件： ​ 作为分析的补充，所有在数据集中的恶意软件也会被Virus Total所分析，通过这种方式，每个恶意软件都会被不同的反病毒引擎所分析，结果会被记录。 ​ 5. 处理分析结果： ​ Virus Total服务使用大约66个不同的防病毒应用程序进行文件分析。利用我们利用这个服务得到的每个研究结果，我们识别了每个恶意软件的家族。通过观察，我们发现对于同一恶意软件，不同的防病毒应用程序给出了不同的结果。此外，观察到并非每一个防病毒应用程序都能检测到一些恶意软件。因此，在检测每一个恶意软件类时，认为它属于所有分析中的大多数类。","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"}],"author":"Shaw"},{"title":"Generic Black-Box End-to-End Attack Against State of the art API Call Based Malware Classifiers","slug":"【论文阅读】Generic Black-Box End-to-End Attack Against State of the art API Call Based Malware Classifiers","date":"2021-09-03T06:08:27.094Z","updated":"2021-10-12T13:26:39.593Z","comments":true,"path":"2021/09/03/【论文阅读】Generic Black-Box End-to-End Attack Against State of the art API Call Based Malware Classifiers/","link":"","permalink":"http://example.com/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Generic%20Black-Box%20End-to-End%20Attack%20Against%20State%20of%20the%20art%20API%20Call%20Based%20Malware%20Classifiers/","excerpt":"","text":"【论文阅读】Generic Black-Box End-to-End Attack Against State of the art API Call Based Malware Classifiers 作者：Ishai Rosenberg 大学：Ben-Gurion University of the Negev 时间：2018.6.4 1. 做了什么？ ​ 对一个通过机器学习训练的，通过API调用来分类恶意软件的分类器的攻击。 ​ 这个攻击可以使分类器不能成功识别恶意软件，并且不改变原有软件的功能。 ​ 实现了GADGET，一个可以直接将二进制恶意软件文件转换为分类器无法检测的二进制文件，并不需要访问文件源代码。 2. 一些概念： 2.1 Machine learning malware classififiers（基于机器学习的恶意软件分类器） ​ 优点：1. 可以自动训练，节省时间； ​ 2. 只要分类器并不是基于指纹特征或者某个特定的特征（如Hash值）来分类，面对不可见威胁时泛化能力较强。 2.2 Adversarial Examples（对抗样本） 对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。 可以针对一张已经有正确分类的image，对其进行细微的像素修改，可以在DNN下被错分为其他label。 ​ 样本x的label为熊猫，在对x添加部分干扰后，在人眼中仍然分为熊猫，但对深度模型，却将其错分为长臂猿，且给出了高达99.3%的置信度。 像素攻击：改动图片上的一个像素，就能让神经网络认错图，甚至还可以诱导它返回特定的结果。 改动图片上的一个像素，就能让神经网络认错图，甚至还可以诱导它返回特定的结果 2. 同样，根据DNN，很容易产生一张在人眼下毫无意义的image，但是在DNN中能够获得高confidence的label。 两种EA算法生成的样本，这些样本人类完全无法识别，但深度学习模型会以高置信度对它们进行分类，例如将噪声识别为狮子。 2.2.1： Adversarial examples for API sequences(生成API序列对抗样本与生成图像对抗样本并不同): API序列由长度可变的离散符号组成，但图像可以用固定维度的矩阵表示为矩阵，且矩阵的值是连续的。 对于对抗API序列，其必须验证原始的恶意功能是完整的。 对抗样本的迁移性：针对一种模型的对抗样本通常对另一种模型也奏效，即使这两个模型不是用同一数据集训练的。 2.3 几种攻击方法： White-box attack：白盒攻击，对模型和训练集完全了解。 Black-box attack：黑盒攻击：对模型不了解，对训练集不了解或了解很少。 Real-word attack：在真实世界攻击。如将对抗样本打印出来，用手机拍照识别。 targeted attack：使得图像都被错分到给定类别上。 non-target attack：事先不知道需要攻击的网络细节，也不指定预测的类别，生成对抗样本来欺骗防守方的网络。 mimicry attack: 编写恶意的exploit，该exp模拟良性代码系统调用的痕迹，因为能够逃逸检测。 disguise attack: 仅修改系统调用的参数使良性系统调用生成恶意行为 。 No-op attack: 添加语义的no-ops-系统调用，其没有影响，或者是不相干的影响，即，打开一个不存在的文件。 Equivalence attack: 使用一个不同的系统调用来达到恶意的目的. 2.4 decision boundary(决策界限) 2.5 end-to-end: 2.6 结果分类： 虑一个二分问题，即将实例分成正类（positive）或负类（negative）。对一个二分问题来说，会出现四种情况。如果一个实例是正类并且也被 预测成正类，即为真正类（True positive）,如果实例是负类被预测成正类，称之为假正类（False positive）。相应地，如果实例是负类被预测成负类，称之为真负类（True negative）,正类被预测成负类则为假负类（false negative）。 列联表如下表所示，1代表正类，0代表负类。（预测正确：true，预测是正类：positive） 预测 1 0 合计 实际 1 True Positive（TP） False Negative（FN） Actual Positive(TP+FN) 0 False Positive（FP) True Negative(TN) Actual Negative(FP+TN) 合计 Predicted Positive(TP+FP) Predicted Negative(FN+TN) TP+FP+FN+TN 从列联表引入两个新名词。 其一是真正类率(true positive rate ,TPR), 计算公式为 TPR=TP/ ( TP+ FN)，刻画的是分类器所识别出的 正实例占所有正实例的比例。 另外一个是负正类率(false positive rate, FPR),计算公式为 FPR= FP / (FP + TN)，计算的是分类器错认为负类的正实例占所有负实例的比例。 还有一个真负类率（True Negative Rate，TNR），也称为specificity,计算公式为TNR= TN/ ( FP+ TN) = 1 - FPR。 3. 如何实现？ 一些问题：程序调用API的过程；","categories":[{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"}],"tags":[{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"}],"author":"Shaw"},{"title":"Shaw's blog","slug":"Welcome","date":"2021-09-03T05:50:00.118Z","updated":"2023-02-20T05:03:18.440Z","comments":true,"path":"2021/09/03/Welcome/","link":"","permalink":"http://example.com/2021/09/03/Welcome/","excerpt":"","text":"论文阅读/技术学习/比赛积累","categories":[],"tags":[],"author":"Shaw"}],"categories":[{"name":"Code","slug":"Code","permalink":"http://example.com/categories/Code/"},{"name":"Paper","slug":"Paper","permalink":"http://example.com/categories/Paper/"},{"name":"paper","slug":"paper","permalink":"http://example.com/categories/paper/"},{"name":"Reproduce","slug":"Reproduce","permalink":"http://example.com/categories/Reproduce/"},{"name":"Kaggle","slug":"Kaggle","permalink":"http://example.com/categories/Kaggle/"},{"name":"Something","slug":"Something","permalink":"http://example.com/categories/Something/"},{"name":"Book","slug":"Book","permalink":"http://example.com/categories/Book/"}],"tags":[{"name":"Vulnerability","slug":"Vulnerability","permalink":"http://example.com/tags/Vulnerability/"},{"name":"AEG","slug":"AEG","permalink":"http://example.com/tags/AEG/"},{"name":"LLVM","slug":"LLVM","permalink":"http://example.com/tags/LLVM/"},{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/tags/Cloud/"},{"name":"Windows","slug":"Windows","permalink":"http://example.com/tags/Windows/"},{"name":"Symbolic Execution","slug":"Symbolic-Execution","permalink":"http://example.com/tags/Symbolic-Execution/"},{"name":"Binary","slug":"Binary","permalink":"http://example.com/tags/Binary/"},{"name":"ML","slug":"ML","permalink":"http://example.com/tags/ML/"},{"name":"Pytorch","slug":"Pytorch","permalink":"http://example.com/tags/Pytorch/"},{"name":"RL","slug":"RL","permalink":"http://example.com/tags/RL/"},{"name":"CV","slug":"CV","permalink":"http://example.com/tags/CV/"},{"name":"Malware Classifiers","slug":"Malware-Classifiers","permalink":"http://example.com/tags/Malware-Classifiers/"},{"name":"BCSA","slug":"BCSA","permalink":"http://example.com/tags/BCSA/"},{"name":"obfuscation","slug":"obfuscation","permalink":"http://example.com/tags/obfuscation/"},{"name":"IDS","slug":"IDS","permalink":"http://example.com/tags/IDS/"},{"name":"DDoS","slug":"DDoS","permalink":"http://example.com/tags/DDoS/"},{"name":"AD","slug":"AD","permalink":"http://example.com/tags/AD/"},{"name":"Botnet","slug":"Botnet","permalink":"http://example.com/tags/Botnet/"},{"name":"Mutiagent","slug":"Mutiagent","permalink":"http://example.com/tags/Mutiagent/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://example.com/tags/Machine-Learning/"},{"name":"Math","slug":"Math","permalink":"http://example.com/tags/Math/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"AD training","slug":"AD-training","permalink":"http://example.com/tags/AD-training/"},{"name":"DNN","slug":"DNN","permalink":"http://example.com/tags/DNN/"},{"name":"RNN","slug":"RNN","permalink":"http://example.com/tags/RNN/"},{"name":"PDF","slug":"PDF","permalink":"http://example.com/tags/PDF/"},{"name":"Android","slug":"Android","permalink":"http://example.com/tags/Android/"}]}