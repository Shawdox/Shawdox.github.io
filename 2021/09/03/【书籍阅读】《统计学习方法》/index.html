<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <title>《统计学习方法》 | Shaw</title>
  <meta name="description" content="@hust" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <link rel="stylesheet" type="text/css" href="/css/screen.css" />
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Noto+Serif:400,700,400italic|Open+Sans:700,400" />

  <meta name="generator" content="Shaw">

  
  
  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>


<body class="post-template">

  <header class="site-head"  style="background-image: url(//blog.ghost.org/content/images/2013/Nov/cover.png)" >
    <div class="vertical">
        <div class="site-head-content inner">
             <a class="blog-logo" href="/"><img src="//blog.ghost.org/content/images/2013/Nov/bloglogo_1-1.png" alt="Blog Logo"/></a> 
            <h1 class="blog-title">Shaw</h1>
            <h2 class="blog-description">@hust</h2>
        </div>
    </div>
</header>
  

<main class="content" role="main">
  <article class="post">
    <span class="post-meta">
      <time datetime="2021-09-03T06:17:39.403Z" itemprop="datePublished">
          2021-09-03
      </time>
    
    
    | 
    <a href='/tags/Machine-Learning/'>Machine Learning</a>
    
    
</span>
    <h1 class="post-title">《统计学习方法》</h1>
    <section class="post-content">
      <h1 id="【书籍阅读】《统计学习方法》"><a href="#【书籍阅读】《统计学习方法》" class="headerlink" title="【书籍阅读】《统计学习方法》"></a>【书籍阅读】《统计学习方法》</h1><h3 id="一-统计学习方法概论："><a href="#一-统计学习方法概论：" class="headerlink" title="一. 统计学习方法概论："></a>一. 统计学习方法概论：</h3><p>​    首先，要明确计算机科学中存在三个维度：系统，计算，与信息。统计学习方法（机器学习）主要属于信息这一维度，并在其中扮演者核心角色。</p>
<h4 id="1-监督学习概念："><a href="#1-监督学习概念：" class="headerlink" title="1. 监督学习概念："></a>1. 监督学习概念：</h4><p>​    监督学习，Supervised learning，指在已经做好标注的训练集上学习，为了叙述方便，定义以下基本概念：</p>
<blockquote>
<ol>
<li><strong>输入空间（X），输出空间（Y）：</strong>输入所有可能取值，输出所有可能取值；</li>
<li><strong>特征空间：</strong>输入一般由特征向量表示，所有特征向量存在的空间称为特征空间，输入空间与特征空间并不完全等价，有时需要映射；</li>
<li><strong>上标  x^i^</strong> :表示一个输入的第  i 个特征；</li>
<li><strong>下标  x~j~：</strong>表示第 j 个输入。</li>
<li><strong>回归问题：</strong>输入输出都为连续型变量；</li>
<li><strong>分类问题：</strong>输出变量为有限个离散型变量；</li>
<li><strong>标注问题：</strong>输入与输出变量都为变量序列。</li>
<li><strong>假设空间：</strong>所有可能的模型的集合，也就是学习的范围。</li>
</ol>
</blockquote>
<p>​    使用训练集学习——&gt;对未知数据进行预测</p>
<p>​    </p>
<h4 id="2-统计学习三要素："><a href="#2-统计学习三要素：" class="headerlink" title="2. 统计学习三要素："></a>2. 统计学习三要素：</h4><p>​    统计学习三要素为：<strong>模型，策略，算法</strong>；</p>
<p>​    模型是决定学习的预测函数的类型；</p>
<p>​    策略是判定什么样的模型是好的，用于度量当前的模型好坏；</p>
<p>​    算法是训练过程中的具体做法，例如如何回归，如何计算，如何调整等。</p>
<h4 id="3-模型的衡量方法："><a href="#3-模型的衡量方法：" class="headerlink" title="3. 模型的衡量方法："></a>3. 模型的衡量方法：</h4><ul>
<li><p><strong>损失函数与风险函数：</strong></p>
<p>​    损失函数，Loss Function，用于模型一次预测的错误程度，例如：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210902154618.png" alt=""></p>
<p>​    损失函数的数值越小，模型就越好。如果计算损失函数的期望，得到的就是风险函数，Risk Function:</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210902154745.png" alt=""></p>
<p>​    可以看出，损失函数用于某次预测的估计，风险函数用于总体平均估计。我们当然希望训练出的模型的风险函数越小越好。</p>
<p>​    <strong>但是，观察上式，理想化的概率分布P(x，y)是未知的，我们进行学习就是要通过模型来模拟它，故这个式子理论存在，实际不能计算，不能用作评估模型的直接方法。</strong></p>
</li>
</ul>
<ul>
<li><p><strong>经验风险与结构风险：</strong></p>
<p>​    为了解决上述问题，我们引入经验风险：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210902155212.png" alt=""></p>
<p>​    可以看到，经验风险将每个样本视作等概率出现，是模型对于训练集的平均损失，那么其与风险函数的误差在哪？</p>
<p>​    根据大数定律，当训练集足够大时，二者是近似相等的。但实际情况下，很多时候训练样本数目有限，甚至很小，故用经验风险效估计风险函数并不理想，故需要进行修正，这就是监督学习中的<strong>两个基本策略：</strong>经验风险最小化和结构风险最小化。</p>
<p>​    如果训练样本容量较大，使用经验风险最小化没什么问题。</p>
<p>​    当样本容量很小时，仅仅使用经验风险最小化容易导致过拟合，故这里使用<strong>结构风险（就是正则化）</strong>最小化方法，对模型复杂度进行惩罚，后续介绍。</p>
</li>
</ul>
<ul>
<li><p><strong>训练误差与测试误差：</strong></p>
<p>​    训练误差本质上不重要，它可以反应一个问题是不是容易学习，但要衡量模型的预测能力，主要是看测试误差。</p>
</li>
</ul>
<ul>
<li><p><strong>正则化与交叉验证：</strong></p>
<p>​    正则化是在经验风险项后再增加一个正则化项（Regularizer），其与模型的复杂度成正相关，一般使用模型参数向量的范数：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210902161201.png" alt=""></p>
<p>​    交叉验证的基本思想是重复使用数据：</p>
<ol>
<li><p><strong>简单交叉验证：</strong></p>
<p>将训练集随机分为两部分，一部分训练，一部分测试，然后在各种条件下训练出不同的模型，用测试集进行横向对比，选出最好的。</p>
<ol>
<li><strong>S折交叉验证：</strong></li>
</ol>
<p>S-fold cross validation，随机地将已给数据切分为S个互不相交的大小相同的子集，选取S-1个用于训练，剩下一个用于测试。</p>
<p>这样总共测试集有S种选法，将这S种全部试一遍，评选S次测评中平均误差最小的模型。</p>
<ol>
<li><strong>留一交叉验证：</strong></li>
</ol>
<p>令S=N（训练集大小）即可，这种方法往往是在数据集特别缺乏的情况下使用。</p>
</li>
</ol>
</li>
<li><p><strong>泛化误差与泛化上界：</strong></p>
<p>​    泛化能力指模型对位置数据的预测能力，就是模型的好坏。如何量化这个能力？</p>
<p>​    根据定义，其就是模型在测试集上的测试表现：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210902162814.png" alt=""></p>
<p>​    同时可以用以下式子衡量泛化误差的上界：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210902162851.png" alt=""></p>
</li>
<li><p><strong>生成模型与判别模型：</strong></p>
<p>​    监督学习方法又可以分为两种方法：生成方法（Generatice Approach）和判别方法（Discriminative Approach）。</p>
<p>​    如果以概率论的角度来看待，模型的作用是根据P（x）来求P（y | x），故下面有两种方法求</p>
<p>P（y | x），直接模拟P（y | x）和通过求  $P(\frac{y}{x}) = \frac{P(x,y)}{P(x)}$  来求P（y | x）。</p>
<p>​    前者就是判别模型，后者是生成模型。</p>
<p>​    生成模型可以还原出联合概率分布P（x , y），学习收敛速度更快，可以适应存在隐含变量的情况；</p>
<p>​    判别模型直接学习条件概率,直接面对预测，准确率更高，并且简化了学习问题。</p>
</li>
</ul>
<hr>
<h3 id="二-感知机"><a href="#二-感知机" class="headerlink" title="二. 感知机"></a>二. 感知机</h3><p>​    感知机，perceptron，是二分类的线性分类模型，输入为特征向量，输出为类别，取1和-1两种。</p>
<p>​    感知机属于判别模型。</p>
<p>​    </p>
<p>​    对于一个给定数据集，T = {（x~1~，y~1~）……（x~n~，y~n~）}，如果存在某个超平面S，w·x + b = 0（这里w是超平面的法向量，b是截距），使得所有 y~i~ = 1 的实例i，有 w·x~i~ + b &gt; 0，y~i~ = -1则相反，则称数据集T为<strong>线性可分数据集（<em>Linealy separable data set</em>）</strong>，否则，称数据集T为线性不可分数据集。</p>
<h4 id="2-1-感知机损失函数："><a href="#2-1-感知机损失函数：" class="headerlink" title="2.1 感知机损失函数："></a>2.1 感知机损失函数：</h4><p>​    感知机的目的就是对于一个线性可分的数据集，通过找出w和b，来确定一个超平面用于分类。</p>
<p>​    这里，我们选取某错误分类点到超平面S的<strong>总距离</strong>来当做损失函数，某一点到超平面S的距离如下：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210904182025.png" alt=""></p>
<p>​    ‖w‖是w的L~2~范数。</p>
<p>​    故，某个误分类点到超平面S的距离是：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210904182154.png" alt=""></p>
<p>​    将所有误分类点求和，忽略L~2~范数，即可得到<strong>感知机的损失函数</strong>（M为误分类点集合）：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210904182339.png" alt=""></p>
<p>​    对于一个特定样本点的损失函数，在误分类时是参数w,b的线性函数，在正确分类时是0，故给定训练数据集T，损失函数L是w，b的连续可导函数。</p>
<h4 id="2-2-训练过程："><a href="#2-2-训练过程：" class="headerlink" title="2.2 训练过程："></a>2.2 训练过程：</h4><p>​    感知机训练采用随机梯度下降的方法：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210904183233.png" alt=""></p>
<p>​    当找到一个误分类点时，不断梯度下降直至该点被正确分类为止。</p>
<p>​    <strong>数学证明其收敛性：</strong></p>
<p>​        具体见书本，这里略过。</p>
<h4 id="2-3-感知机的对偶形式："><a href="#2-3-感知机的对偶形式：" class="headerlink" title="2.3 感知机的对偶形式："></a>2.3 感知机的对偶形式：</h4><p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905112734.png" alt=""></p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905112750.png" alt=""></p>
<p>​    由图可以看到，对于每个测试集中的x~i~，都有一个与之对应的α~i~，对偶形式中就是调整其对应的α。</p>
<p>​    关于gram矩阵的作用，如果手算一遍简单的训练过程，就可以得到答案。</p>
<hr>
<h3 id="三-k近邻法"><a href="#三-k近邻法" class="headerlink" title="三. k近邻法"></a>三. k近邻法</h3><p>​    k近邻法是一种基本的分类与回归方法，这里只讨论分类方法。</p>
<p>​    其输入为特征向量，输出为实例的类别，可以取<strong>多类</strong>。</p>
<h4 id="3-1-算法描述："><a href="#3-1-算法描述：" class="headerlink" title="3.1 算法描述："></a>3.1 算法描述：</h4><p>​    给定一个训练集，对于新的数据实例，在训练数据集中找到与其最邻近的k个实例，这k个实例多数属于某个类，就把该输入实例分为这个类。</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905113440.png" alt=""></p>
<p>​        <u>k近邻法没有显式的学习过程。</u>可以理解为，k近邻算法将特征空间划分为了一些子空间，每个点所属的空间是确定的。</p>
<p>​        </p>
<ul>
<li><strong>如何度量两个特征之间的距离？</strong></li>
</ul>
<p>​        k邻近模型的特征空间一般是n维实空间R^n^，使用欧氏距离或者L~p~距离（<em>L~p~ distance</em>），Minkowski距离（<em>Minkowski distance</em>）；</p>
<p>​        <strong>L~p~距离：</strong></p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905143250.png" alt=""></p>
<p>​        <strong>欧氏距离：</strong></p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905143332.png" alt=""></p>
<p>​        <strong>曼哈顿距离：</strong></p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905143356.png" alt=""></p>
<p>​        <strong>无穷距离：</strong></p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905143648.png" alt=""></p>
<p>​    由下图可以看出，p取值不同时到原点距离为1的图形是不同的：</p>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905143741.png" alt=""></p>
<ul>
<li><p><strong>如何选择k的值？</strong></p>
<p>​    k值越小，模型学习时的近似误差越小，估计误差越大，模型会越复杂，抗干扰性越小（例如，最邻近的点是噪声），模型会非常敏感，容易过拟合；</p>
<p>​    k值越大，估计误差会很小，近似误差会很大，整体模型变得简单。</p>
<p>​    k一般的取值并不大，使用交叉验证的方法来选取最佳的k值。</p>
</li>
<li><p><strong>如何决策？</strong></p>
<p>​    在得到k个最相似的实例后，采用何种规则判断测试样本属于哪一类呢？</p>
<p>​    k邻近算法使用多数表决的方法：</p>
</li>
</ul>
<p><img src="https://shaw-typora.oss-cn-beijing.aliyuncs.com/20210905154943.png" alt=""></p>
<blockquote>
<p>ps: ci表示某种决策规则下一组测试用例的表决结果。经由以上推导可以得出，多数表决规则是合理的。</p>
</blockquote>
<p>​        </p>
<ul>
<li><p><strong>如何快速找到某个用例的K近邻点？</strong></p>
<p><strong>KD树：</strong> 具体算法见书。</p>
</li>
</ul>
<hr>
<h3 id="四-朴素贝叶斯法"><a href="#四-朴素贝叶斯法" class="headerlink" title="四. 朴素贝叶斯法"></a>四. 朴素贝叶斯法</h3><p>  ​    </p>

    </section>
    <footer class="post-footer">
      <section class="author">
    <h4>Shaw</h4>
    <p>A designer, developer and entrepreneur. Spends his time travelling the world with a bag of kites. Likes journalism and publishing platforms.</p>
</section>
      <section class="share">
    <h4>Share this post</h4>
    <a class="icon-twitter" target="_blank" rel="noopener" href="http://twitter.com/share?url=http://example.com/2021/09/03/【书籍阅读】《统计学习方法》/"
       onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
        <span class="hidden">Twitter</span>
    </a>
    <a class="icon-facebook" target="_blank" rel="noopener" href="https://www.facebook.com/sharer/sharer.php?u=http://example.com/2021/09/03/【书籍阅读】《统计学习方法》/"
       onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
        <span class="hidden">Facebook</span>
    </a>
    <a class="icon-google-plus" target="_blank" rel="noopener" href="https://plus.google.com/share?url=http://example.com/2021/09/03/【书籍阅读】《统计学习方法》/"
       onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
        <span class="hidden">Google+</span>
    </a>
</section>
    </footer>
  </article>
  <nav class="pagination" role="pagination">
    
    <a class="newer-posts" href="/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Adversarial%20Training%20with%20Fast%20Gradient%20Projection%20Method%20against%20Synonym%20Substitution%20Based%20Text%20Attacks/">
        ← Adversarial Training with Fast Gradient Projection Method against Synonym Substitution Based Text Attacks
    </a>
    
    <span class="page-number">•</span>
    
    <a class="older-posts" href="/2021/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Deep%20Text%20Classifification%20Can%20be%20Fooled/">
        Deep Text Classifification Can be Fooled →
    </a>
    
</nav>
  <div id="comment" class="comments-area">
    <h1 class="title"><a href="#disqus_comments" name="disqus_comments">Comments</a></h1>

    
</div>
</main>


  
<footer class="site-footer">
  
  <div class="inner">
     <section class="copyright">All content copyright <a href="/">Shaw</a> &copy; 2014 &bull; All rights reserved.</section>
     <section class="poweredby">Proudly published with <a class="icon-ghost" target="_blank" rel="noopener" href="http://zespia.tw/hexo/">Hexo</a></section>
  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script type="text/javascript" src="/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/js/index.js"></script>






<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
